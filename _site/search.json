[
  {
    "objectID": "ECD.html#the-interplay-of-design-and-statistical-modeling",
    "href": "ECD.html#the-interplay-of-design-and-statistical-modeling",
    "title": "ECD Intro",
    "section": "The Interplay of Design and Statistical Modeling",
    "text": "The Interplay of Design and Statistical Modeling\nStatistical models must be selected/tailored according to the needs of the assessment\nSuch selection and adaptation is only meaningful in the larger context of the assessment design\nUnderstanding the discipline of assessment design is a necessary prerequisite for statistical modeling\nEvidence Centered Design is an assessment design framework with general applicability and utility"
  },
  {
    "objectID": "ECD.html#test-design-considerations",
    "href": "ECD.html#test-design-considerations",
    "title": "ECD Intro",
    "section": "Test Design Considerations",
    "text": "Test Design Considerations\n\nStakeholders\nRequirements\n\nPurpose of the test\nIntended population\n\nProspective Score Report\nEvidence-Centered Design\n\nClaims\nValidity\n\nSpecifications"
  },
  {
    "objectID": "ECD.html#evidence-centered-designadvantages",
    "href": "ECD.html#evidence-centered-designadvantages",
    "title": "ECD Intro",
    "section": "Evidence-Centered Design—Advantages",
    "text": "Evidence-Centered Design—Advantages\n\nEvidence Centered Design (ECD) provides a mechanism for\n\nCapturing and documenting information about the structure and strength of evidentiary relationships.\nCoordinating the work of test developers in authoring tasks and psychometricians in calibrating the measurement model.\nDocumenting the scientific information that provides the foundation for the assessment and its validity."
  },
  {
    "objectID": "ECD.html#evidence-centered-designcentral-question",
    "href": "ECD.html#evidence-centered-designcentral-question",
    "title": "ECD Intro",
    "section": "Evidence-Centered Design–Central Question",
    "text": "Evidence-Centered Design–Central Question\n\nEvidence-centered design centers around the questions:\n\n“What can we observe about an examinee’s performance which will provide evidence that the examinee has or does not have the knowledge, skills and abilities we wish to make claims about?”\n\n\n“How can we structure situations to be able to make those observations?”\n\n\nThis process results in the Conceptual Assessment Framework (CAF)"
  },
  {
    "objectID": "ECD.html#the-initial-frame",
    "href": "ECD.html#the-initial-frame",
    "title": "ECD Intro",
    "section": "The Initial Frame",
    "text": "The Initial Frame\n\nWhy are we measuring?\n\nWhat are the goals and the desires for use of this assessment?\nProspective Score Report\n\nWho are we measuring?\n\nWho would take the assessment?\nWho would view results and for what purpose?\n\nGoals of the assessment that represent the targets around which the rest of the design process is oriented"
  },
  {
    "objectID": "ECD.html#activity-1-cont",
    "href": "ECD.html#activity-1-cont",
    "title": "ECD Intro",
    "section": "Activity 1 (cont)",
    "text": "Activity 1 (cont)\nList a bunch of activities that you may want prospective drivers to do in their exam\nWhat is environment of the task\nWhat are manipulable features of the task?\nPick one of the tasks you created and build an evidence model for it.\nWhat are some observable outcomes? their possible values?\nWhich proficiencies do they measure?\nThink a bit about putting this driver’s test together\nHow many tasks do we need of what types?\nHow much time will be spent in written tests? On the road? In simulators?\nHow do we verify the identity of applicants?"
  },
  {
    "objectID": "ECD.html#cup-and-cap-notation",
    "href": "ECD.html#cup-and-cap-notation",
    "title": "ECD Intro",
    "section": "Cup and Cap notation",
    "text": "Cup and Cap notation\nIn probability theory, events are sets (sets of balls in the urn).\nLet \\(A\\) and \\(B\\) be two events\n\nUnion: Either \\(A\\) or \\(B\\) occurs \\[ A \\cup B \\qquad A \\vee B \\]\nIntersection: Both \\(A\\) and \\(B\\) occur \\[ A \\cap B \\qquad A \\wedge B \\]\n\nSometimes also just \\(\\Pr(A,B)\\)\n\nComplement: Not \\(A\\) \\[ \\neg A \\qquad \\overline{A} \\]\n\n\\(\\Pr(\\overline{A}) = 1- \\Pr(A)\\)$"
  },
  {
    "objectID": "ECD.html#law-of-total-probability",
    "href": "ECD.html#law-of-total-probability",
    "title": "ECD Intro",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\\[ \\Pr(E) = \\Pr(E|H) \\Pr(H) + \\Pr(E|\\overline{H})\\Pr(\\overline{H}) \\]\n\\[ \\Pr(B) = \\sum_i \\Pr(B|A_i)\\Pr(A_i) \\]\nWhere $ A_i A_j =$ and \\(\\bigcup_i A_i = \\Omega\\)"
  },
  {
    "objectID": "ECD.html#independence",
    "href": "ECD.html#independence",
    "title": "ECD Intro",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\\[ \\Pr(B) = \\Pr(B|A) = \\Pr(B|\\overline{A}) \\] \\[ \\Pr(A) = \\Pr(A|B) = \\Pr(A|\\overline{B}) \\] \\[ \\Pr(A \\cap B) = \\Pr(A|B)\\Pr(B) = \\Pr(A)\\Pr(B) \\]\n\nKnowing \\(A\\) provides no information about \\(B\\) and vise versa."
  },
  {
    "objectID": "ECD.html#accident-proneness-feller-1968",
    "href": "ECD.html#accident-proneness-feller-1968",
    "title": "ECD Intro",
    "section": "Accident Proneness (Feller, 1968)",
    "text": "Accident Proneness (Feller, 1968)\n\nDriving Skill: 5/6 Normal, 1/6 Accident Prone\nProbability of an accident in a given year\n\n1/100 for Normal drivers\n1/10 for Accident prone drivers\n\nAccidents happen independently in each year\nWhat is the probability a randomly chosen driver will have an accident in Year 1?\nGiven a driver had an accident in Year 1, what is probability of accident in Year 2?"
  },
  {
    "objectID": "ECD.html#accident-proneness-year-1",
    "href": "ECD.html#accident-proneness-year-1",
    "title": "ECD Intro",
    "section": "Accident Proneness (Year 1)",
    "text": "Accident Proneness (Year 1)\nWhat is the probability a randomly chosen driver will have an accident in Year 1? Year 2?\n\\(\\Pr(Y_i)\\). – Prob of accident in a given year.\n\\[ \\Pr(A_i) = \\Pr(A_i|N)\\Pr(N) +\n\\Pr(A_i|\\overline{N})\\Pr(\\overline{N}) \\]\n\nDrivingSkill <- c(N=5/6,A=1/6)\nAccLike <- cbind(Yes=c(N=1/100,A=1/10),No=c(N=99/100,A=9/10))\nYear1 <- sweep(AccLike,1,DrivingSkill,\"*\")\nYear1\n\n          Yes    No\nN 0.008333333 0.825\nA 0.016666667 0.150\n\nsum(Year1[,\"Yes\"])\n\n[1] 0.025"
  },
  {
    "objectID": "ECD.html#accident-proneness-year-ii",
    "href": "ECD.html#accident-proneness-year-ii",
    "title": "ECD Intro",
    "section": "Accident Proneness (Year II)",
    "text": "Accident Proneness (Year II)\nGiven a driver had an accident in Year 1, what is probability of accident in Year 2?\n\\[ \\begin{array}{rcl}\n\\Pr(A_1 \\cap A_2) &=& \\Pr(A_1 \\cap A_2|N)\\Pr(N) + \\Pr(A_1 \\cap\nA_2|\\overline{N}) \\Pr(\\overline{N}) \\\\\n  &=& \\Pr(A_1|N)\\Pr(A_2|N)\\Pr(N) + \\Pr(A_1|\\overline{N})\n\\Pr(A_2|\\overline{N}) \\Pr(\\overline{N})\n   \\end{array} \\]\n\nAcc2Like <- AccLike\nAcc2Like[,\"Yes\"] <- AccLike[,\"Yes\"]^2\nAcc2Like[,\"No\"] <- 1 -Acc2Like[,\"Yes\"]\nYear12 <- sweep(Acc2Like,1,DrivingSkill,\"*\")\nYear12\n\n           Yes      No\nN 8.333333e-05 0.83325\nA 1.666667e-03 0.16500\n\nsum(Year12[,\"Yes\"])\n\n[1] 0.00175"
  },
  {
    "objectID": "ECD.html#accident-prone-chain",
    "href": "ECD.html#accident-prone-chain",
    "title": "ECD Intro",
    "section": "Accident Prone (Chain)",
    "text": "Accident Prone (Chain)\n\\(\\Pr(Y_2 | Y_1)\\) – Accident in 2nd year given accident in first year.\n\nsum(Year12[,\"Yes\"])/sum(Year1[,\"Yes\"])\n\n[1] 0.07"
  },
  {
    "objectID": "ECD.html#explanation",
    "href": "ECD.html#explanation",
    "title": "ECD Intro",
    "section": "Explanation",
    "text": "Explanation\n\\(\\Pr(S=\\text{normal}|A_i)\\) – Probability in normal category given accident.\n\nDiagrammeR::grViz(\"\ndigraph AP {\n  Driving -> Year1;\n  Driving -> Year2;\n}\")"
  },
  {
    "objectID": "ECD.html#conditional-independence",
    "href": "ECD.html#conditional-independence",
    "title": "ECD Intro",
    "section": "Conditional Independence",
    "text": "Conditional Independence\n\nConditional Independence: $(Y_1,Y_2|S) = (Y_1|S) (Y_2|S) $\nYears are marginally dependent.\nSeparation in graph tells the story.\nInformation flows from from Year1 to Driving Skill to Year2"
  },
  {
    "objectID": "ECD.html#another-example",
    "href": "ECD.html#another-example",
    "title": "ECD Intro",
    "section": "Another Example",
    "text": "Another Example\n\nDiagrammeR::grViz(\"\ndigraph Autotrain {\n  Train -> COVID;\n  Train -> MaskOnTrain;\n  MaskOnTrain -> COVID;\n  Vaccine -> COVID;\n  COVID -> fever;\n  COVID -> congestion;\n  COVID -> pcrTest;\n}\")"
  },
  {
    "objectID": "ECD.html#competing-explanations",
    "href": "ECD.html#competing-explanations",
    "title": "ECD Intro",
    "section": "Competing Explanations",
    "text": "Competing Explanations\n\nDiagrammeR::grViz(\"\ndigraph CompExpl {\n  Skill1 -> X\n  Skill2 -> X\n}\")\n\n\n\n\n\n\nSkill1 and Skill2 are (a priori) independent in population\nTask X requires both skills (conjunctive model)\nAnswer the following questions:\n\nWhat is posterior of Skill2 after learning X=False, and Skill1=High?\nWhat is posterior of Skill1 after learning X=False, and Skill2=High?\nWhat is true of joint posterior of Skill1 and Skill2 after learning X=False?"
  },
  {
    "objectID": "ECD.html#d-separation-example",
    "href": "ECD.html#d-separation-example",
    "title": "ECD Intro",
    "section": "D-separation Example",
    "text": "D-separation Example\n\nDiagrammeR::grViz('\ndigraph DSep {\n  rankdir = \"LR\";\n  A -> B; A -> C;\n  B -> D; C -> D;\n  D -> E -> F;\n}')\n\n\n\n\n\n\\(B\\) and \\(C\\) are independent if \\(A\\) is known and all of \\(D\\), \\(E\\) or \\(F\\) are not known.\n\\(D\\) is independent of \\(F\\) if \\(E\\) is known."
  },
  {
    "objectID": "ECD.html#d-separation-exercise",
    "href": "ECD.html#d-separation-exercise",
    "title": "ECD Intro",
    "section": "D-Separation Exercise",
    "text": "D-Separation Exercise\n\nDiagrammeR::grViz('\ndigraph DSepEx {\n  rankdir = \"TB\";\n  A -> D; B -> D; B -> E; C -> E;\n  D -> F; D -> G; E -> G; E -> H;\n}')\n\n\n\n\n\n\nAre A and C independent if\n\nWe have observed no other variables?\n\nWhat could we condition on to make A and C independent?\n\nWe have observed F and H?\n\nWhat else could we condition on to make A and C independent?\n\nWe have observed G ?\n\nWhat else could we condition on to make A and C independent?"
  },
  {
    "objectID": "ECD.html#building-up-complex-networks-irt",
    "href": "ECD.html#building-up-complex-networks-irt",
    "title": "ECD Intro",
    "section": "Building Up Complex Networks: IRT",
    "text": "Building Up Complex Networks: IRT\n\nDiagrammeR::grViz('\ndigraph IRT {\n  subgraph{ Q[label=\"θ\"] }\n  subgraph {\n  X1; X2; andC[shape=none,label=\"...\"]; XJ\n  }\n  Q -> X1; Q-> X2; Q->andC [style=\"invis\"]; Q->XJ\n}')\n\n\n\n\n\nFor example, in IRT, item responses are conditionally independent given \\(\\theta\\):\n\\[ p(X_1,\\ldots,X_J,\\theta) = p(\\theta) \\prod_{j=1}^{J} p(X_j|\\theta)\\]"
  },
  {
    "objectID": "ECD.html#bayes-net",
    "href": "ECD.html#bayes-net",
    "title": "ECD Intro",
    "section": "Bayes net",
    "text": "Bayes net\n\nDiagrammeR::grViz(\"\ndigraph ABN {\n  A -> C; B -> C;\n  C -> D; C -> E;\n  D -> F; E -> F\n}\")\n\n\n\n\n\nOne factor for each node in graph\nThis factor is conditioned on parents in graph\n“Prior” nodes have no parents\n\\[p(A)p(B)p(C|A\\,B)p(D|C)p(E|C)p(F|D\\,E) = p(A\\,B\\,C\\,D\\,E\\,F)\\]\nDigraph must be acyclic"
  },
  {
    "objectID": "ECD.html#activity-2-build-a-bayes-net",
    "href": "ECD.html#activity-2-build-a-bayes-net",
    "title": "ECD Intro",
    "section": "Activity 2: Build a Bayes Net",
    "text": "Activity 2: Build a Bayes Net\nPick one of the tasks you created and build an a Bayes net in Netica:\nProficiency variables, their possible values\nObservable variables, their possible values\nConditional probabilities between Proficiency variables and Observable variables\nAdd your observables to the proficiency model you made in Netica"
  },
  {
    "objectID": "Scoring.html",
    "href": "Scoring.html",
    "title": "Scoring Individual Students",
    "section": "",
    "text": "The RNetica suite consists of a number of packages:\n\n\n\nRNetica Package Suite\n\n\n\nCPTtools is a collection of tools for building conditional probability tables (particularly, the DiBello models). It stands alone.\nRNetica links R to the Netica Bayes net engine. Note: non-trivial uses of RNetica require a Netica API (not GUI) license from Norsys.\n\n\n\n\nThe Peanut object oriented framework rests on top of CPTtools and RNetica.\n\n\n\nRNetica Package Suite\n\n\n\nCPTtools is a collection of tools for building conditional probability tables (particularly, the DiBello models). It stands alone.\nRNetica links R to the Netica Bayes net engine. _Note: non-trivial uses of RNetica require a Netica API (not GUI) license from Norsys.\n\n\n\n\nThe Peanut object oriented framework rests on top of CPTtools and RNetica.\n\n\n\nRNetica Package Suite\n\n\n\nPeanut (a corrupt reading of Pnet, or parameterized network) is an object oriented frame work on top of CPTtools.\nPNetica is an implementation of the Peanut framework using RNetica.\n\nPackages can be installed through R-Universe\n\ninstall.packages(c('CPTtootls','RNetica','Peanut','PNetica'),\n                 repos = c('https://ralmond.r-universe.dev', 'https://cloud.r-project.org'))\n\nSource code is on github https://github.com/ralmond/.\n\n\n\n\nR – GPL-3 (Free and open source)\nRNetica – Artistic (Free and open source)\nNetica.dll/libNetica.so– Commercial (open API, but not open source)\n\nFree Student/Demo version\n\nLimited number of nodes\nLimited usage (education, evaluation of Netica)\n\nPaid version (seehttp://www.norsys.com/for price information)\n\nNeed to purchase API not GUI version ofNetica\nMay want both (use GUI to visualize networks built in RNetica)\n\n\nCPTtools, Peanut – Artistic (Free and open source), does not depend on Netica\nRNetica – Artistic, but depends on RNetica.\n\n\n\n\n\nWhen you purchase a license, Norsys will send you a license key. Something that looks like:\n“+User/Organization/120,310-details/XXXXX”\n(Where I’ve obscured the last 5 security digits) 120 – GUI, 310 – API (I think)\n\nThree ways of installing the license key:\n\nUse as argument to NeticaSession(...,LicenseKey=XXX)\nSet options(NeticaLicenseKey=XXX)\n\n\nThis can be set in .Rprofile in your home directory.\n\n\nSet an environmental variable NeticaLicenseKey before launching R.\n\n\nThis can be set in .Renviron in your home directory.\n\n\n\n\nAfter you load RNetica you need to start the session. This is when you pass the license key.\n\nlibrary(RNetica)\nsess <- NeticaSession(LicenseKey=NeticaLicenseKey)\nstartSession(sess)\n\n\nlibrary(RNetica)\n\nLoading required package: CPTtools\n\n\nLoading required package: futile.logger\n\nsess <- NeticaSession(LicenseKey=\"\")\nstartSession(sess)\n\nNetica 5.04 Linux (AF), (C) 1992-2012 Norsys Software Corp.\n\nNetica operating without a password; there are some limitations.\n\n\nEverything in this tutorial should run without the license.\n\n\n\n\nWhen starting/restarting Netica\nWhen creating a network, or reading one from a file.\nWhen searching for networks.\nCertain global properties\n\nNeticaBN objects have a $session proprty which points back to the session.\nNeticaNode objects have a $node property which points back to the network (which points to the session).\n\n\n\nR and Netica have two different workspaces (memory heaps)\nR workspace is saved and restored automatically when you quick and restart R.\nNeticaheap must be reconnected manually.\n\n\n\nR and Netica Heaps\n\n\n\n\n\nWhen RNetica creates/finds aNeticaobject it creates a corresponding R object\nIf the R object is active then it points to the Netica object, and the Netica object points back at it.\nIf the pointer gets broken (saving and restarting R, deleting the network/node then the R object becomes inactive.\nThe function is.active(nodeOrNet) test to see if the node/net is active.\n\n\n\nR and Netica Heap"
  },
  {
    "objectID": "Scoring.html#mini-aced-proficiency-model",
    "href": "Scoring.html#mini-aced-proficiency-model",
    "title": "Scoring Individual Students",
    "section": "Mini-ACED Proficiency model",
    "text": "Mini-ACED Proficiency model\nSubset of ACED network: Shute, Hansen & Almond (2008); http://ecd.ralmond.net/ecdwiki/ACED\nDownload the mini-ACED file Unzip it in miniACED and that this folder is in the same directory as this qmd file.\nNext, change directory into the directory that contains miniACED\n\nif (\"miniACED-Geometric.csv\" %in% list.files(\"miniACED\")) {\n  print(\"You're good to go.\")\n} else {\n  stop(\"You need to unpack 'miniACED.zip' or go to the directory where it is.\")\n}\n\n[1] \"You're good to go.\"\n\n\n\n\n\nProficiency Model"
  },
  {
    "objectID": "Scoring.html#mini-aced-em-fragments",
    "href": "Scoring.html#mini-aced-em-fragments",
    "title": "Scoring Individual Students",
    "section": "Mini-ACED EM Fragments",
    "text": "Mini-ACED EM Fragments\nAll ACED tasks were scored correct/incorrect\nEach evidence model is represented by a fragment consisting of observables with stub edges indicating where it should be adjoined with the network.\n\n\n\nCommon Ratio Easy\n\n\n\n\n\nModel Table Extend Hard"
  },
  {
    "objectID": "Scoring.html#preliminaries",
    "href": "Scoring.html#preliminaries",
    "title": "Scoring Individual Students",
    "section": "Preliminaries",
    "text": "Preliminaries"
  },
  {
    "objectID": "Scoring.html#task-to-em-map",
    "href": "Scoring.html#task-to-em-map",
    "title": "Scoring Individual Students",
    "section": "Task to EM map",
    "text": "Task to EM map\nNeed a table to tell us which EM to use with which task\n\n## Read in task->evidence model mapping\nEMtable <- read.csv(file.path(\"miniACED\",\"MiniACEDEMTable.csv\"),\n                    row.names=1,\n                    as.is=2) #Keep EM names as strings\nEMtable\n\n                                                 EM    X   Y\ntCommonRatio1a                    CommonRatioEasyEM  108 294\ntCommonRatio1b                    CommonRatioEasyEM  108 414\ntCommonRatio2a                     CommonRatioMedEM  108 534\ntCommonRatio2b                     CommonRatioMedEM  108 654\ntCommonRatio3a                    CommonRatioHardEM  108 774\ntCommonRatio3b                    CommonRatioHardEM  108 894\ntExamplesGeometric1a                 ExamplesEasyEM  342 294\ntExamplesGeometric1b                 ExamplesEasyEM  342 414\ntExamplesGeometric2a                  ExamplesMedEM  342 534\ntExamplesGeometric2b                  ExamplesMedEM  342 654\ntExamplesGeometric3a                 ExamplesHardEM  342 774\ntExamplesGeometric3b                 ExamplesHardEM  342 894\ntExtendGeometric1a                     ExtendEasyEM  588 294\ntExtendGeometric1b                     ExtendEasyEM  588 414\ntExtendGeometric2a                      ExtendMedEM  588 534\ntExtendGeometric2b                      ExtendMedEM  588 654\ntExtendGeometric3a                     ExtendHardEM  588 774\ntExtendGeometric3b                     ExtendHardEM  588 894\ntTableExtendGeometric1a           TableExtendEasyEM 1134 282\ntTableExtendGeometric1b           TableExtendEasyEM 1134 402\ntTableExtendGeometric2a            TableExtendMedEM 1134 522\ntTableExtendGeometric2b            TableExtendMedEM 1134 642\ntTableExtendGeometric3a           TableExtendHardEM 1134 762\ntTableExtendGeometric3b           TableExtendHardEM 1134 882\ntModelExtendTableGeometric1a ModelTableExtendEasyEM  858 288\ntModelExtendTableGeometric1b ModelTableExtendEasyEM  858 408\ntModelExtendTableGeometric2a  ModelTableExtendMedEM  858 528\ntModelExtendTableGeometric2b  ModelTableExtendMedEM  858 648\ntModelExtendTableGeometric3a ModelTableExtendHardEM  858 768\ntModelExtendTableGeometric3b ModelTableExtendHardEM  858 888\n\n\n\n## Scoring Script\n## Preliminaries\n#| eval=FALSE\nlibrary(RNetica)\nsess <- NeticaSession()\nstartSession(sess)\n\nNetica environment already initialized"
  },
  {
    "objectID": "Scoring.html#read-in-the-network.",
    "href": "Scoring.html#read-in-the-network.",
    "title": "Scoring Individual Students",
    "section": "Read in the Network.",
    "text": "Read in the Network.\n\n## Read in network -- Do this every time R is restarted\nprofModel <- ReadNetworks(file.path(\"miniACED\",\"miniACEDPnet.dne\"),session = sess)\n## If profModels already exists could also use\n\n## Reconnect nodes -- Do this every time R is restarted\nallNodes <- NetworkAllNodes(profModel)\nsgp <- allNodes$SolveGeometricProblems\nsgp\n\nDiscrete  Netica Node named  SolveGeometricProblems in network  MiniACEDPM \n  Node is currently active.\nStates are:  High, Medium, Low"
  },
  {
    "objectID": "Scoring.html#aside-1-node-sets",
    "href": "Scoring.html#aside-1-node-sets",
    "title": "Scoring Individual Students",
    "section": "Aside 1 – Node Sets",
    "text": "Aside 1 – Node Sets\nNode sets can be viewed as either\nA. a set of labels assigned to each node.\nB. a set of nodes which have a particular label.\nIn RNetica, these are very useful as they define collections of nodes that might be interesting in some way (e.g., Proficiency variables, Observable variable, background variables)\nNode set operations yeild a list of nodes; iterating over that set is often very useful."
  },
  {
    "objectID": "Scoring.html#node-set-examples",
    "href": "Scoring.html#node-set-examples",
    "title": "Scoring Individual Students",
    "section": "Node Set Examples",
    "text": "Node Set Examples\n\n## Node Sets\nNetworkNodeSets(profModel)\n\n[1] \"pnodes\"        \"Proficiencies\"\n\nNetworkNodesInSet(profModel,\"pnodes\")\n\n$TableGeometric\nDiscrete  Netica Node named  TableGeometric in network  MiniACEDPM \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$ModelGeometric\nDiscrete  Netica Node named  ModelGeometric in network  MiniACEDPM \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$ExtendGeometric\nDiscrete  Netica Node named  ExtendGeometric in network  MiniACEDPM \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$ExamplesGeometric\nDiscrete  Netica Node named  ExamplesGeometric in network  MiniACEDPM \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$CommonRatio\nDiscrete  Netica Node named  CommonRatio in network  MiniACEDPM \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$SolveGeometricProblems\nDiscrete  Netica Node named  SolveGeometricProblems in network  MiniACEDPM \n  Node is currently active.\nStates are:  High, Medium, Low"
  },
  {
    "objectID": "Scoring.html#more-node-set-examples",
    "href": "Scoring.html#more-node-set-examples",
    "title": "Scoring Individual Students",
    "section": "More Node Set Examples",
    "text": "More Node Set Examples\n\nprofNodes <- NetworkNodesInSet(profModel,\"Proficiencies\")\nNodeSets(sgp)\n\n[1] \"pnodes\"        \"Proficiencies\"\n\n\nAdding a node to a set.\n\n## These are all settable\nNodeSets(sgp) <- c(NodeSets(sgp),\"HighLevel\")\nNodeSets(sgp)\n\n[1] \"HighLevel\"     \"pnodes\"        \"Proficiencies\""
  },
  {
    "objectID": "Scoring.html#aside-2-common-net-operations",
    "href": "Scoring.html#aside-2-common-net-operations",
    "title": "Scoring Individual Students",
    "section": "Aside 2: Common Net operations",
    "text": "Aside 2: Common Net operations\nJust about everything that can be done through the Netica GUI, can be done through the Netica API, and hence through R Netica. [In practice, the API version has lagged the GUI version, and my RNetica release lag Norsys’s API updates.] Many more examples are in the RNetica help.\n\n## Querying Nodes\nNodeStates(sgp)   #List states\n\n    High   Medium      Low \n  \"High\" \"Medium\"    \"Low\" \n\nNodeParents(sgp)  #List parents\n\nnamed list()"
  },
  {
    "objectID": "Scoring.html#more-rnetica-queries",
    "href": "Scoring.html#more-rnetica-queries",
    "title": "Scoring Individual Students",
    "section": "More RNetica Queries",
    "text": "More RNetica Queries\n\nNodeLevels(sgp)   #List numeric values associated with states\n\n      High     Medium        Low \n 0.9674216  0.0000000 -0.9674216 \n\nNodeProbs(sgp) # Conditional Probability Table (as array)\n\nSolveGeometricProblems\n  High Medium    Low \n0.1532 0.2784 0.5684 \nattr(,\"class\")\n[1] \"CPA\"   \"array\"\n\n## These are all settable (can be used on RHS of <-) for model\n## construction"
  },
  {
    "objectID": "Scoring.html#conditional-probability-tables-as-data-frame",
    "href": "Scoring.html#conditional-probability-tables-as-data-frame",
    "title": "Scoring Individual Students",
    "section": "Conditional Probability Tables (as Data Frame)",
    "text": "Conditional Probability Tables (as Data Frame)\n\nsgp[] # Conditional Probability Table (as data frame)\n\n  SolveGeometricProblems.High SolveGeometricProblems.Medium \n                       0.1532                        0.2784 \n   SolveGeometricProblems.Low \n                       0.5684 \n\n\nCan use [] operator to select rows or elements\nCan set table or (row or cell).\nCPTtools package has tools for building tables.\n\nhelp(package=\"CPTtools\")"
  },
  {
    "objectID": "Scoring.html#inference",
    "href": "Scoring.html#inference",
    "title": "Scoring Individual Students",
    "section": "Inference",
    "text": "Inference\nNetworks must be compiled before they are used for inference.\n\n## Inference\nCompileNetwork(profModel) #Lightning bolt on GUI \n## Must do this before inference\n## Recompiling an already compiled network is harmless\n\n\n## Enter Evidence by setting values for these functions\nNodeValue(sgp) #View or set the value\n\n[1] NA\n\nNodeLikelihood(sgp) #Virtual evidence\n\n  High Medium    Low \n     1      1      1"
  },
  {
    "objectID": "Scoring.html#beliefs-marginal-probabilities",
    "href": "Scoring.html#beliefs-marginal-probabilities",
    "title": "Scoring Individual Students",
    "section": "Beliefs (Marginal Probabilities)",
    "text": "Beliefs (Marginal Probabilities)\n\n## Query beliefs\nNodeBeliefs(sgp) #Current probability (given entered evidence)\n\n  High Medium    Low \n0.1532 0.2784 0.5684 \n\nNodeExpectedValue(sgp) #If node has values, EAP\n\n[1] -0.4016734\nattr(,\"std_dev\")\n[1] 0.7169429\n\n## These aren't settable\n\n\n## Retract Evidence\nRetractNodeFinding(profNodes$ExamplesGeometric)\nRetractNetFindings(profModel)"
  },
  {
    "objectID": "Scoring.html#example-enter-evidence",
    "href": "Scoring.html#example-enter-evidence",
    "title": "Scoring Individual Students",
    "section": "Example: Enter Evidence",
    "text": "Example: Enter Evidence\n\n## Enter Evidence\nNodeFinding(profNodes$CommonRatio) <- \"Medium\"\n## Enter Evidence \"Not Low\" (\"High or Medium\")\nNodeLikelihood(profNodes$ExamplesGeometric) <- c(1,1,0)\n\nNodeBeliefs(sgp) #Current probability (given entered evidence)\n\n     High    Medium       Low \n0.0000000 0.1811515 0.8188485 \n\nNodeExpectedValue(sgp) #If node has values, EAP\n\n[1] -0.7921717\nattr(,\"std_dev\")\n[1] 0.3725963"
  },
  {
    "objectID": "Scoring.html#example-retract-evidence",
    "href": "Scoring.html#example-retract-evidence",
    "title": "Scoring Individual Students",
    "section": "Example: Retract Evidence",
    "text": "Example: Retract Evidence\n\n## Retract Evidence\nRetractNetFindings(profModel)\n\nMany more examples:\n\nhelp(RNetica)"
  },
  {
    "objectID": "Scoring.html#back-to-work",
    "href": "Scoring.html#back-to-work",
    "title": "Scoring Individual Students",
    "section": "Back to work",
    "text": "Back to work\nSimple Scoring Example\nStart New Student Copy the proficiency model to make student model.\n\nFred.SM <- CopyNetworks(profModel,\"Fred\")\nFred.SMvars <- NetworkAllNodes(Fred.SM)\nCompileNetwork(Fred.SM)"
  },
  {
    "objectID": "Scoring.html#setup-score-history.",
    "href": "Scoring.html#setup-score-history.",
    "title": "Scoring Individual Students",
    "section": "Setup score history.",
    "text": "Setup score history.\n\nprior <- NodeBeliefs(Fred.SMvars$SolveGeometricProblems)\nFred.History <- matrix(prior,1,3)\nrow.names(Fred.History) <- \"*Baseline*\"\ncolnames(Fred.History) <- names(prior)\nFred.History\n\n             High Medium    Low\n*Baseline* 0.1532 0.2784 0.5684"
  },
  {
    "objectID": "Scoring.html#fred-does-a-task",
    "href": "Scoring.html#fred-does-a-task",
    "title": "Scoring Individual Students",
    "section": "Fred does a task",
    "text": "Fred does a task\nTask name and data.\n\nt.name <- \"tCommonRatio1a\"\nt.isCorrect <- \"Yes\"\n\nAdjoin SM and EM\n\nEMnet <- ReadNetworks(file.path(\"miniACED\",\n                                paste(EMtable[t.name,\"EM\"],\"dne\",sep=\".\")),\n                      session = sess)\nobs <- AdjoinNetwork(Fred.SM,EMnet)\nnames(NetworkAllNodes(Fred.SM)) \n\n[1] \"SolveGeometricProblems\" \"CommonRatio\"            \"ExamplesGeometric\"     \n[4] \"ExtendGeometric\"        \"ModelGeometric\"         \"TableGeometric\"        \n[7] \"isCorrect\"             \n\n## Fred.SM is now the Motif for the current task.\nCompileNetwork(Fred.SM)"
  },
  {
    "objectID": "Scoring.html#absorb-evidence",
    "href": "Scoring.html#absorb-evidence",
    "title": "Scoring Individual Students",
    "section": "Absorb Evidence",
    "text": "Absorb Evidence\nEnter finding\n\nNodeFinding(obs$isCorrect) <- t.isCorrect\n\nCalculate statistics of interest\n\npost <- NodeBeliefs(Fred.SMvars$SolveGeometricProblems)\nFred.History <- rbind(Fred.History,new=post)\nrownames(Fred.History)[nrow(Fred.History)] <- paste(t.name,t.isCorrect,sep=\"=\")\nFred.History\n\n                       High    Medium       Low\n*Baseline*         0.153200 0.2784000 0.5684000\ntCommonRatio1a=Yes 0.160016 0.2893454 0.5506387"
  },
  {
    "objectID": "Scoring.html#cleanup",
    "href": "Scoring.html#cleanup",
    "title": "Scoring Individual Students",
    "section": "Cleanup",
    "text": "Cleanup\nNetwork and Observable no longer needed, so absorb it:\n\nDeleteNetwork(EMnet) ## Delete EM\n#try(AbsorbNodes(obs))\n## Currently, there is a Netica bug with Absorb Nodes, we will\n## leave this node in place, as that is mostly harmless."
  },
  {
    "objectID": "Scoring.html#nd-task",
    "href": "Scoring.html#nd-task",
    "title": "Scoring Individual Students",
    "section": "2nd Task",
    "text": "2nd Task\nWrite a script for scoring the second task.\nThis time Fred attempts the task tCommonRatio2a and gets it incorrect.\n\n### Fred does another task\nt.name <- \"tCommonRatio2a\"\nt.isCorrect <- \"No\"\n\n## Load Evidence Model and adjoin\n\n## Recompile\n\n## Add Evidence\n\n## Check Finding and add to history\n\n## Clean up"
  },
  {
    "objectID": "Scoring.html#answer-for-2nd-task",
    "href": "Scoring.html#answer-for-2nd-task",
    "title": "Scoring Individual Students",
    "section": "Answer for 2nd Task",
    "text": "Answer for 2nd Task\n\n### Fred does another task\nt.name <- \"tCommonRatio2a\"\nt.isCorrect <- \"No\"\n\nEMnet <- ReadNetworks(file.path(\"miniACED\",\n                                paste(EMtable[t.name,\"EM\"],\"dne\", sep=\".\")),\n                      session=sess)\nobs <- AdjoinNetwork(Fred.SM,EMnet)\n#NodeVisPos(obs$isCorrect) <- EMtable[t.name,c(\"X\",\"Y\")]\n## Fred.SM is now the Motif for the current task.\nCompileNetwork(Fred.SM)\n\nNodeFinding(obs[[1]]) <- t.isCorrect\npost <- NodeBeliefs(Fred.SMvars$SolveGeometricProblems)\nFred.History <- rbind(Fred.History,new=post)\nrownames(Fred.History)[nrow(Fred.History)] <- \n      paste(t.name,t.isCorrect,sep=\"=\")\nFred.History\n\n                        High    Medium       Low\n*Baseline*         0.1532000 0.2784000 0.5684000\ntCommonRatio1a=Yes 0.1600160 0.2893454 0.5506387\ntCommonRatio2a=No  0.1064912 0.2057332 0.6877756\n\n## Cleanup:  Delete EM and Absorb observables\nDeleteNetwork(EMnet) ## Delete EM\n#AbsorbNodes(obs)"
  },
  {
    "objectID": "Scoring.html#fred-does-another-task",
    "href": "Scoring.html#fred-does-another-task",
    "title": "Scoring Individual Students",
    "section": "Fred does another task",
    "text": "Fred does another task\n\nt.name <- \"tCommonRatio2a\"\nt.isCorrect <- \"No\"\n\n\nEMnet <- ReadNetworks(file.path(\"miniACED\",\n                paste(EMtable[t.name,\"EM\"],\"dne\",sep=\".\")),\n                session=sess)\nobs <- AdjoinNetwork(Fred.SM,EMnet)\n(NetworkAllNodes(Fred.SM)) ## Fred.SM is now the Motif for the current task.\n\n$SolveGeometricProblems\nDiscrete  Netica Node named  SolveGeometricProblems in network  Fred \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$CommonRatio\nDiscrete  Netica Node named  CommonRatio in network  Fred \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$ExamplesGeometric\nDiscrete  Netica Node named  ExamplesGeometric in network  Fred \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$ExtendGeometric\nDiscrete  Netica Node named  ExtendGeometric in network  Fred \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$ModelGeometric\nDiscrete  Netica Node named  ModelGeometric in network  Fred \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$TableGeometric\nDiscrete  Netica Node named  TableGeometric in network  Fred \n  Node is currently active.\nStates are:  High, Medium, Low \n\n$isCorrect\nDiscrete  Netica Node named  isCorrect in network  Fred \n  Node is currently active.\nStates are:  Yes, No \n\n$isCorrect1\nDiscrete  Netica Node named  isCorrect1 in network  Fred \n  Node is currently active.\nStates are:  Yes, No \n\n$isCorrect2\nDiscrete  Netica Node named  isCorrect2 in network  Fred \n  Node is currently active.\nStates are:  Yes, No \n\nCompileNetwork(Fred.SM)"
  },
  {
    "objectID": "Scoring.html#task-2-continued",
    "href": "Scoring.html#task-2-continued",
    "title": "Scoring Individual Students",
    "section": "Task 2 continued",
    "text": "Task 2 continued\n\nNodeFinding(obs[[1]]) <- t.isCorrect\npost <- NodeBeliefs(Fred.SMvars$SolveGeometricProblems)\nFred.History <- rbind(Fred.History,new=post)\nrownames(Fred.History)[nrow(Fred.History)] <- paste(t.name,t.isCorrect,sep=\"=\")\nFred.History\n\n                         High    Medium       Low\n*Baseline*         0.15320002 0.2784000 0.5684000\ntCommonRatio1a=Yes 0.16001597 0.2893454 0.5506387\ntCommonRatio2a=No  0.10649122 0.2057332 0.6877756\ntCommonRatio2a=No  0.04991532 0.1159301 0.8341545\n\n\nCleanup: Delete EM and Absorb observables\n\nDeleteNetwork(EMnet) ## Delete EM\n#try(AbsorbNodes(obs))\n## Currently, there is a Netica bug with Absorb Nodes, we will leave\n##this the node in place as that is mostly harmless."
  },
  {
    "objectID": "Scoring.html#fred-logs-out",
    "href": "Scoring.html#fred-logs-out",
    "title": "Scoring Individual Students",
    "section": "Fred logs out",
    "text": "Fred logs out\nSave network to a file.\n\nWriteNetworks(Fred.SM,\"FredSM.dne\")\nDeleteNetwork(Fred.SM)\nis.active(Fred.SM)  ## No longer active in Netica space\n\n[1] FALSE\n\n\nFred logs back in\n\nFred.SM <- ReadNetworks(\"FredSM.dne\",session=sess)\nis.active(Fred.SM)\n\n[1] TRUE"
  },
  {
    "objectID": "Scoring.html#read-in-the-scores.",
    "href": "Scoring.html#read-in-the-scores.",
    "title": "Scoring Individual Students",
    "section": "Read in the scores.",
    "text": "Read in the scores.\n\nminiACED.data <- read.csv(file.path(\"miniACED\",\"miniACED-Geometric.csv\"),row.names=1)\nhead(miniACED.data)\n\n     Class Treatment Sequencing Feedback Total.Items Correct Incorrect\nS055     1         1          2        2          63       8        55\nS058     1         1          2        2          63      33        30\nS053     1         2          2        1          63      12        51\nS061     1         1          2        2          63      21        42\nS063     1         1          2        2          63      21        42\nS066     1         1          2        2          63      19        44\n     Remaining tCommonRatio1a tCommonRatio1b tCommonRatio3a tCommonRatio3b\nS055         0              1              1              2              1\nS058         0              2              2              2              1\nS053         0              1              1              1              1\nS061         0              1              1              1              1\nS063         0              1              2              2              1\nS066         0              1              2              1              1\n     tCommonRatio2a tCommonRatio2b tExamplesGeometric1a tExamplesGeometric1b\nS055              1              1                    1                    1\nS058              1              2                    2                    1\nS053              1              1                    1                    1\nS061              1              1                    1                    1\nS063              1              1                    2                    1\nS066              1              1                    2                    1\n     tExamplesGeometric3a tExamplesGeometric3b tExamplesGeometric2a\nS055                    1                    1                    1\nS058                    1                    1                    1\nS053                    1                    1                    1\nS061                    1                    1                    1\nS063                    1                    1                    1\nS066                    1                    1                    1\n     tExamplesGeometric2b tExtendGeometric1a tExtendGeometric1b\nS055                    1                  1                  1\nS058                    1                  1                  2\nS053                    1                  1                  1\nS061                    1                  2                  2\nS063                    1                  1                  2\nS066                    1                  1                  2\n     tExtendGeometric3a tExtendGeometric3b tExtendGeometric2a\nS055                  2                  1                  1\nS058                  2                  1                  2\nS053                  1                  1                  1\nS061                  1                  1                  1\nS063                  2                  2                  2\nS066                  2                  2                  1\n     tExtendGeometric2b tModelExtendTableGeometric1a\nS055                  1                            1\nS058                  1                            2\nS053                  1                            1\nS061                  2                            1\nS063                  1                            1\nS066                  1                            2\n     tModelExtendTableGeometric1b tModelExtendTableGeometric3a\nS055                            1                            1\nS058                            2                            2\nS053                            1                            2\nS061                            2                            2\nS063                            2                            1\nS066                            1                            2\n     tModelExtendTableGeometric3b tModelExtendTableGeometric2a\nS055                            1                            1\nS058                            2                            1\nS053                            1                            1\nS061                            1                            1\nS063                            2                            1\nS066                            2                            1\n     tModelExtendTableGeometric2b tTableExtendGeometric1a\nS055                            1                       1\nS058                            2                       1\nS053                            2                       1\nS061                            1                       1\nS063                            1                       2\nS066                            1                       2\n     tTableExtendGeometric1b tTableExtendGeometric3a tTableExtendGeometric3b\nS055                       1                       1                       1\nS058                       1                       1                       2\nS053                       1                       1                       2\nS061                       1                       1                       2\nS063                       1                       1                       1\nS066                       1                       1                       2\n     tTableExtendGeometric2a tTableExtendGeometric2b\nS055                       1                       2\nS058                       1                       1\nS053                       1                       1\nS061                       1                       1\nS063                       1                       1\nS066                       2                       2\n\n\nSome meta-data\n\n## Mark columns of table corresponding to tasks\nfirst.task <- 9\nlast.task <- 20 #ncol(miniACED.data)\n## Code key for numeric values\nt.vals <- c(\"No\",\"Yes\")\n\n## Pick a student, we might normally iterate over this.\nStudent.row <- 1"
  },
  {
    "objectID": "Scoring.html#setup-for-student-in-sample",
    "href": "Scoring.html#setup-for-student-in-sample",
    "title": "Scoring Individual Students",
    "section": "Setup for student in sample",
    "text": "Setup for student in sample\nCreate Student Model from Proficiency Model\n\nStudent.SM <- CopyNetworks(profModel,\"Student\")\nStudent.SMvars <- NetworkAllNodes(Student.SM)\nCompileNetwork(Student.SM)\n\nInitialize history list\n\nprior <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\nStudent.History <- matrix(prior,1,3)\nrow.names(Student.History) <- \"*Baseline*\"\ncolnames(Student.History) <- names(prior)"
  },
  {
    "objectID": "Scoring.html#now-loop-over-tasks",
    "href": "Scoring.html#now-loop-over-tasks",
    "title": "Scoring Individual Students",
    "section": "Now loop over tasks",
    "text": "Now loop over tasks\n\nfor (itask in first.task:last.task) {\n  \n  ## Look up the EM for the task, and adjoin it.\n  tid <- names(miniACED.data)[itask]\n  print(tid)\n  EMnet <- ReadNetworks(file.path(\"miniACED\",\n                                  paste(EMtable[tid,\"EM\"],\"dne\",sep=\".\")),\n                        session=sess)\n  print(sapply(NetworkAllNodes(EMnet),NodeVisPos))\n  browser()\n  obs <- AdjoinNetwork(Student.SM,EMnet)\n  CompileNetwork(Student.SM)\n\n  ## Add the evidence\n  t.val <- t.vals[miniACED.data[Student.row,itask]] #Decode integer\n  NodeFinding(obs[[1]]) <- t.val\n  \n  ## Update the history\n  post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\n  Student.History <- rbind(Student.History,new=post)\n  rownames(Student.History)[nrow(Student.History)] <- paste(tid,t.val,sep=\"=\")\n\n  ## Cleanup, Delete EM and Absob Observables\n  DeleteNetwork(EMnet)\n  #try(AbsorbNodes(obs)) # Still broken\n}\n\n[1] \"tCommonRatio1a\"\n  isCorrect\nx         0\ny         0\nCalled from: eval(expr, envir, enclos)\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tCommonRatio1b\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tCommonRatio3a\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tCommonRatio3b\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tCommonRatio2a\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tCommonRatio2b\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tExamplesGeometric1a\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tExamplesGeometric1b\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tExamplesGeometric3a\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tExamplesGeometric3b\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tExamplesGeometric2a\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)\ndebug at <text>#4: tid <- names(miniACED.data)[itask]\ndebug at <text>#5: print(tid)\n[1] \"tExamplesGeometric2b\"\ndebug at <text>#6: EMnet <- ReadNetworks(file.path(\"miniACED\", paste(EMtable[tid, \n    \"EM\"], \"dne\", sep = \".\")), session = sess)\ndebug at <text>#9: print(sapply(NetworkAllNodes(EMnet), NodeVisPos))\n  isCorrect\nx         0\ny         0\ndebug at <text>#10: browser()\ndebug at <text>#11: obs <- AdjoinNetwork(Student.SM, EMnet)\ndebug at <text>#12: CompileNetwork(Student.SM)\ndebug at <text>#15: t.val <- t.vals[miniACED.data[Student.row, itask]]\ndebug at <text>#16: NodeFinding(obs[[1]]) <- t.val\ndebug at <text>#19: post <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\ndebug at <text>#20: Student.History <- rbind(Student.History, new = post)\ndebug at <text>#21: rownames(Student.History)[nrow(Student.History)] <- paste(tid, \n    t.val, sep = \"=\")\ndebug at <text>#24: DeleteNetwork(EMnet)"
  },
  {
    "objectID": "Scoring.html#now-look-at-the-scoring-history.",
    "href": "Scoring.html#now-look-at-the-scoring-history.",
    "title": "Scoring Individual Students",
    "section": "Now look at the scoring history.",
    "text": "Now look at the scoring history.\n\nStudent.History\n\n                                High     Medium       Low\n*Baseline*              0.1532000154 0.27840000 0.5684000\ntCommonRatio1a=No       0.0693975762 0.14382692 0.7867755\ntCommonRatio1b=No       0.0131133273 0.04543132 0.9414554\ntCommonRatio3a=Yes      0.0576437376 0.12699647 0.8153598\ntCommonRatio3b=No       0.0383302718 0.09589107 0.8657787\ntCommonRatio2a=No       0.0122262547 0.05103555 0.9367382\ntCommonRatio2b=No       0.0033196749 0.03242403 0.9642563\ntExamplesGeometric1a=No 0.0010064315 0.02298103 0.9760125\ntExamplesGeometric1b=No 0.0006250255 0.02056840 0.9788066\ntExamplesGeometric3a=No 0.0006051398 0.02043075 0.9789641\ntExamplesGeometric3b=No 0.0005891956 0.02031317 0.9790976\ntExamplesGeometric2a=No 0.0005483013 0.01998808 0.9794636\ntExamplesGeometric2b=No 0.0005246418 0.01979495 0.9796804"
  },
  {
    "objectID": "Scoring.html#weight-of-evidence-1",
    "href": "Scoring.html#weight-of-evidence-1",
    "title": "Scoring Individual Students",
    "section": "Weight of Evidence",
    "text": "Weight of Evidence\nGood (1985)\nH is binary hypothesis, e.g., Proficiency > Medium\nE is evidence for hypothesis\nWeight of Evidence (WOE) is\n\\[ W(H:E) = \\log \\frac{P(E|H)}{P(E|\\overline{H})} =\n\\log  \\frac{P(H|E)}{P(\\overline{H}|E)} -\n\\log  \\frac{P(H)}{P(\\overline{H})} \\]"
  },
  {
    "objectID": "Scoring.html#conditional-weight-of-evidence",
    "href": "Scoring.html#conditional-weight-of-evidence",
    "title": "Scoring Individual Students",
    "section": "Conditional Weight of Evidence",
    "text": "Conditional Weight of Evidence\n\\[ W(H: E_2 | E_1) = \\log  \\frac{P(E_2|H,E_1)}{P(E_2|\\overline{H},E_1)}\n\\] Additive properties\n\\[ W(H: E_1, E2) = W(H: E_1) + W(H: E_2|E_1) \\]\nOrder senstive (evidence seen earlier is worth more)\nWOE Balance Sheet:"
  },
  {
    "objectID": "Scoring.html#now-examine-scoring-history",
    "href": "Scoring.html#now-examine-scoring-history",
    "title": "Scoring Individual Students",
    "section": "Now examine scoring history",
    "text": "Now examine scoring history\n\nwoeBal(Student.History,c(\"High\",\"Medium\"),\"Low\",\n       title=paste(\"Evidence Balance Sheet for \",\n                   rownames(miniACED.data)[Student.row]))"
  },
  {
    "objectID": "Scoring.html#for-more-information",
    "href": "Scoring.html#for-more-information",
    "title": "Scoring Individual Students",
    "section": "For More information",
    "text": "For More information\n\nhelp(RNetica)\nhelp(package=\"RNetica\")\nhelp(CPTtools)\nhelp(package=\"CPTtools\")"
  },
  {
    "objectID": "Session3.html",
    "href": "Session3.html",
    "title": "Peanut Tutorial",
    "section": "",
    "text": "Bayesian Networks in Educational Assessment\nTutorial\nSession III: __ __ Bayes Net with R\nDuanli Yan, Diego Zapata, ETS\nRussell Almond, FSU\n2021 NCME Tutorial: Bayesian Networks in Educational Assessment\nSESSION __ __ TOPIC __ __ PRESENTERS\nSession 1 : Evidence Centered Design Diego Zapata Bayesian Networks\nSession 2 : Bayes Net Applications Duanli Yan & ACED: ECD in Action Russell Almond\nSession 3 : Bayes Nets with R Russell Almond & Duanli Yan\nSession 4 : Refining Bayes Nets with Duanli Yan & Data Russell Almond\n\nRNetica\n\n\nQuick Start Guide\nScoring A Student\nRNetica Quick Start\n\n\nDownloading\n\nhttp://pluto.coe.fsu.edu/RNetica/\nFour Packages:\n\nRNetica – R to Netica link\nCPTtools – Design patterns for CPTs\nPeanut/PNetica -- Object-Oriented Parameterized Network\n\nSource & binary version (Win 64, Mac OS X)\n\nBinary versions include Netica.dll/libNetica.so\n\nIn RStudio select “Package Archive” rather than CRAN\n\nSource version need to download from http://www.norsys.com/ first\n\nSee INSTALLATION\n\n\n\nRNetica Quick Start\n\n\nLicense\n\nR – GPL-3 (Free and open source)\nRNetica – Artistic (Free and open source)\nNetica.dll/libNetica.so – Commercial (open API, but not open source)\n\nFree Student/Demo version\n\nLimited number of nodes\nLimited usage (education, evaluation of Netica)\n\nPaid version (see http://www.norsys.com/ for price information)\n\nNeed to purchase API not GUI version of Netica\nMay want both (use GUI to visualize networks build in RNetica)\n\n\nCPTtools – Artistic (Free and open source), does not depend on Netica\n\nRNetica Quick Start\n\n\nInstalling the License Key\n\nWhen you purchase a license, Norsys will send you a license key. Something that looks like: “+Course/FloridaSU/Ex15-05-30,120,310/XXXXX” (Where I’ve obscured the last 5 security digits)\nTo install the license key, start R in your project directory and type:\n\n\nNeticaLicenseKey <- “+Course/FloridaSU/Ex15-05-30,120,310/XXXXX”\n\n\nq(“yes”)\n\n\nRestart R and type\n\n\nlibrary(RNetica)\n\n\nIf license key is not installed, then you will get the limited/student mode. Most of these examples will run\n\nRNetica Quick Start\n\n\nThe R heap and the Netica heap\nR and Netica have two different workspaces (memory heaps)\nR workspace is saved and restored automatically when you quick and restart R.\nNetica heap must be reconnected manually.\nRNetica Quick Start\n\n\nActive and Inactive pointers\nWhen RNetica creates/finds a Netica object it creates a corresponding R object\nIf the R object is active then it points to the Netica object, and the Netica object points back at it\nIf the pointer gets broken (saving & restarting R, deleting the network/node) then the R object becomes inactive.\nThe function is.active(nodeOrNet) test to see if the node/net is active\nRNetica Quick Start\n\n\nMini-ACED Proficiency model\nSubset of ACED network (Shute, Hansen & Almond (2008); http://ecd.ralmond.net/ecdwiki/ACED )\nProficiency Model subset:\n\nRNetica Quick Start\n\n\nMini-ACED EM Fragments\nAll ACED tasks were scored correct/incorrect\nEach evidence model is represented by a fragment consisting of observables with stub  edges indicating where it should be adjoined with the network.\n\n\nCommon Ratio Easy\nModel Extend Table Hard\nRNetica Quick Start\n\n\nTask to EM map\nNeed a table to tell us which EM to use with which task\n\n\n\nTask ID\nEM Filename\nX\nY\n\n\n\n\ntCommonRatio1b\nCommonRatioEasyEM\n108\n414\n\n\ntCommonRatio2a\nCommonRatioMedEM\n108\n534\n\n\ntCommonRatio2b\nCommonRatioMedEM\n108\n654\n\n\ntCommonRatio3a\nCommonRatioHardEM\n108\n774\n\n\ntCommonRatio3b\nCommonRatioHardEM\n108\n894\n\n\ntExamplesGeometric1a\nExamplesEasyEM\n342\n294\n\n\ntExamplesGeometric1b\nExamplesEasyEM\n342\n414\n\n\n\n\n\n\n\n\n\nRNetica Quick Start\n\n\nScoring Script\nFollow along using the script found in ScoringScript.R in the miniACED folder.\nDon’t forget to setwd() to the miniACED folder (as it needs to find its networks).\nDon’t forget to set the license key before issuing library(RNetica) command.\nRNetica Quick Start\n\n\nReloading Nets and Nodes\n## Scoring Script\n## Preliminaries\nlibrary(RNetica)\nlibrary(CPTtools)\n## Read in network – Do this every time R is restarted\nprofModel <- ReadNetworks(“miniACEDPnet.dne”)\n## If  profModels  already exists could also use\n## Reconnect nodes – Do this every time R is restarted\nallNodes <- NetworkAllNodes(profModel)\nsgp <- allNodes$SolveGeometricProblems\nprofNodes <- NetworkNodesInSet(profModel,“Proficiencies”)\nRNetica Quick Start\n\n\nAside 1: Node Sets\n\nNetica defines a node set functionality which\n\nAdds a collection of labels (sets) to each node\nDefines a collection of nodes with that label\n\nNetica GUI really only offers the opportunity to color nodes by set\nRNetica can loop over node sets (lists of nodes)\n## Node Sets\nNetworkNodeSets(profModel)\nNetworkNodesInSet(profModel,“pnodes”)\nNodeSets(sgp)\n## These are all settable\nNodeSets(sgp) <- c(NodeSets(sgp),“HighLevel”)\nNodeSets(sgp)\n\nRNetica Quick Start\n\n\nAside 2: RNetica Functions\n## Querying Nodes\nNodeStates(sgp) #List states\nNodeParents(sgp) #List parents\nNodeLevels(sgp) #List numeric values associated with states\nNodeProbs(sgp) # Conditional Probability Table (as array)\nsgp[] # Conditional Probability Table (as data frame)\n## These are all settable (can be used on RHS of <-) for \n## model construction\n## Inference\nCompileNetwork(profModel) #Lightning bolt on GUI\n## Must do this before inference\n## Recompiling an already compiled network is harmless\nRNetica Quick Start\n\n\nAside 2: Inference\n## Enter Evidence by setting values for these functions\nNodeValue(sgp) #View or set the value\nNodeLikelihood(sgp) #Virtual evidence\n## Query beliefs\nNodeBeliefs(sgp) #Current probability (given entered evidence)\nNodeExpectedValue(sgp) #If node has values, EAP\n## These aren’t settable\n## Retract Evidence\nRetractNodeFinding(profNodes$ExamplesGeometric)\nRetractNetFindings(profModel)\nRNetica Quick Start\n\n\nAside 2: Example\n## Enter Evidence\nNodeValue(profNodes$CommonRatio) <- “Medium”\n## Enter Evidence “Not Low” (“High or Medium”)\nNodeLikelihood(profNodes$ExamplesGeometric) <- c(1,1,0)\nNodeBeliefs(sgp) #Current probability (given entered evidence)\nNodeExpectedValue(sgp) #If node has values, EAP\n## Retract Evidence\nRetractNetFindings(profModel)\n## Many more examples\nhelp(RNetica)\nRNetica Quick Start\n\n\nBack to work\nLoad the evidence model table\nRow names are task IDs\nEM column contains evidence model name\nEM filename has suffix “.dne” attached.\n## Read in task->evidence model mapping\nEMtable <- read.csv(“MiniACEDEMTable.csv”,row.names=1,\nas.is=2) #Keep EM names as strings\nhead(EMtable)\nRNetica Quick Start\n\n\nA student walks into the test center …\n\nStudent gives the name “Fred”\nStudent is the right grade/age for ACED (8th or 9th grader, pre-algebra)\nBayes net has three states\n\nFred logs into ACED\nFred attempts the task tCommonRatio1a and gets it right\nFred attempts the task tCommonRatio2a and gets it wrong\n\n\nRNetica Quick Start\n\n\nStart a new student\n## Copy the master proficiency model\n## to make student model\nFred.SM <- CopyNetworks(profModel,“Fred”)\nFred.SMvars <- NetworkAllNodes(Fred.SM)\nCompileNetwork(Fred.SM)\n## Setup score history\nprior <- NodeBeliefs(Fred.SMvars$SolveGeometricProblems)\nFred.History <- matrix(prior,1,3)\nrow.names(Fred.History) <- “*Baseline*”\ncolnames(Fred.History) <- names(prior)\nFred.History\nRNetica Quick Start\n\n\nScore 1st Task\n### Fred does a task\nt.name <- “tCommonRatio1a”\nt.isCorrect <- “Yes”\n## Adjoin SM and EM\nEMnet <- ReadNetworks(paste(EMtable[t.name,“EM”],“dne”,sep=“.”))\nobs <- AdjoinNetwork(Fred.SM,EMnet)\nNetworkAllNodes(Fred.SM)\n## Fred.SM is now the Motif for the current task.\nCompileNetwork(Fred.SM)\n## Enter finding\nNodeFinding(obs$isCorrect) <- t.isCorrect\nRNetica Quick Start\n\n\nStats and Cleanup for 1st task\n## Calculate statistics of interest\npost <- NodeBeliefs(Fred.SMvars$SolveGeometricProblems)\nFred.History <- rbind(Fred.History,new=post)\nrownames(Fred.History)[nrow(Fred.History)] <- paste(t.name,t.isCorrect,sep=“=”)\nFred.History\n## Cleanup and Observable no longer needed, so absorb it:\nDeleteNetwork(EMnet) ## Delete EM\n## AbsorbNodes(obs)\n## Currently, there is a  Netica  bug with Absorb Nodes, we will leave\n## this node in place as that is mostly harmless.\nRNetica Quick Start\n\n\n2nd Task\n### Fred does another task\nt.name <- “tCommonRatio2a”\nt.isCorrect <- “No”\nEMnet <- ReadNetworks(paste(EMtable[t.name,“EM”],“dne”,sep=“.”))\nobs <- AdjoinNetwork(Fred.SM,EMnet)\nNetworkAllNodes(Fred.SM)\n## Fred.SM is now the Motif for the current task.\nCompileNetwork(Fred.SM)\nNodeFinding(obs[[1]]) <- t.isCorrect\npost <- NodeBeliefs(Fred.SMvars$SolveGeometricProblems)\nFred.History <- rbind(Fred.History,new=post)\nrownames(Fred.History)[nrow(Fred.History)] <-\npaste(t.name,t.isCorrect,sep=“=”)\nFred.History\n# # Cleanup: Delete EM and Absorb observables\nDeleteNetwork(EMnet) ## Delete EM\n## AbsorbNodes(obs)\nRNetica Quick Start\n\n\nSave and Restore\n## Fred logs out\nWriteNetworks(Fred.SM,“FredSM.dne”)\nDeleteNetwork(Fred.SM)\nis.active(Fred.SM)\n## No longer active in  Netica  space\n## Fred logs back in\nFred.SM <- ReadNetworks(“FredSM.dne”)\nis.active(Fred.SM)\nRNetica Quick Start\n\n\nGetting Serious\n\nACED field test has 230 students attempt all 63 tasks.\nFile miniACED-Geometric contains 30 task subset\n\nThere may be data registration issues here, don’t publish using these data before checking with me for an update\n\nEach row is one student Record\nLets score the first student\n\nAnd build a score history\n\n\nRNetica Quick Start\n\n\nSetup for mini-ACED\nminiACED.data <- read.csv(“miniACED-Geometric.csv”,row.names=1)\nhead(miniACED.data)\nnames(miniACED.data)\n## Mark columns of table corresponding to tasks\nfirst.task <- 9\nlast.task <- ncol(miniACED.data)\n## Code key for numeric values\nt.vals <- c(“No”,“Yes”)\nRNetica Quick Start\n\n\nSetup new Student\n## Pick a student, we might normally iterate over this.\nStudent.row <- 1\n## Setup for student in sample\n## Create Student Model from Proficiency Model\nStudent.SM <- CopyNetworks(profModel,“Student”)\nStudent.SMvars <- NetworkAllNodes(Student.SM)\nCompileNetwork(Student.SM)\n## Initialize history list\nprior <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\nStudent.History <- matrix(prior,1,3)\nrow.names(Student.History) <- “*Baseline*”\ncolnames(Student.History) <- names(prior)\nRNetica Quick Start\n\n\nLoop Part 1: Add Evidence\n## Now loop over tasks\nfor (itask in first.task:last.task) {\n ## Look up the EM for the task, and adjoin it.\ntid <- names(miniACED.data)[itask]\nEMnet <- ReadNetworks(paste(EMtable[tid,“EM”],“dne”,sep=“.”))\nobs <- AdjoinNetwork(Student.SM,EMnet)\nCompileNetwork(Student.SM)\n## Add the evidence\nt.val <- t.vals[miniACED.data[Student.row,itask]] #Decode integer\nNodeFinding(obs[[1]]) <- t.val\nRNetica Quick Start\n\n\nLoop Part 2: Capture Statistics\n## Update the history\npost <- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\nStudent.History <- rbind(Student.History,new=post)\nrownames(Student.History)[nrow(Student.History)] <- paste(tid,t.val,sep=“=”)\n## Cleanup, Delete EM and  Absob  Observables\nDeleteNetwork(EMnet)\n##  AbsorbNodes ( obs ) # Still broken\n}\nRNetica Quick Start\n\n\nWeight of Evidence\nGood (1985)\nH is binary hypothesis, e.g., Proficiency > Medium\nE is evidence for hypothesis\nWeight of Evidence (WOE) is"
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Resources",
    "section": "",
    "text": "These web pages are available in both a static version (https://pluto.coe.fsu.edu/PeanutTutorial) and a source version (https://github.com/ralmond/PeanutTutorial).\nThe easiest way to access the Peanut tutorial is to use R Studio. Go to new project, and select “From Version control”, then “Git” then put ‘https://pluto.coe.fsu.edu/ralmond/PeanutTutorial.git’ in the ‘Repository URL’ box.\n\n\n\nGit dialog\n\n\nThe .qmd files contain a mixture of text and R code. You will need to download the quarto package for these to work properly, but R Studio will guide you through this."
  },
  {
    "objectID": "Session2.html",
    "href": "Session2.html",
    "title": "Peanut Tutorial",
    "section": "",
    "text": "Bayesian Networks in Educational Assessment\nTutorial\nSession II: __ __ Bayes Net Applications \n__ __ ACED: ECD in Action\nDuanli Yan, Diego Zapata, ETS\nRussell Almond, FSU\n2021 NCME Tutorial: Bayesian Networks in Educational Assessment\nSESSION __ __ TOPIC __ __ PRESENTERS\nSession 1 : Evidence Centered Design Diego Zapata Bayesian Networks\nSession 2 : Bayes Net Applications Duanli Yan & ACED: ECD in Action Russell Almond\nSession 3 : Refining Bayes Nets with Russell Almond & Data Duanli Yan\nSession 4 : Bayes Nets with R Duanli Yan & Russell Almond\n\n1. Discrete Item Response Theory (IRT)\nProficiency Model\nTask/Evidence Models\nAssembly Model\nSome Numbers\n\n\nIRT Proficiency Model\n\nThere is one proficiency varaible,  . (Sometimes called an “ability parameter”, but we reserve the term parameter for quantites which are not person specific.)\n takes on values {-2, -1, 0, 1, 2} with prior probabilities of (0.1, 0.2, 0.4, 0.2, 0.1) (Triangular distribution).\nObservable outcome variables are all independent given \nGoal is to draw inferences about \n\nRank order students by \nClassify students according to  above or below a cut point\n\n\n\n\nIRT Task/Evidence Model\nTasks yield an work product which can be unambiguously scored right / wrong .\nEach task has a single observable outcome variable.\nTasks are often called items,  although the common usage often blurs the distinction between the presentation of the item and the outcome variable.\n\n\nIRT (Rasch) Evidence Model\n\nLet X j _ _ be observable outcome variable from Task j\nP(X j _ =right | _  ,   j _ ) _ =\n\n_ _ j  is the difficulty of the item.\n\nCan crank through the formula for each of the five values of  to get values for Conditional Probability Tables (CPT)\n\n\n\nIRT Assembly Model\n5 items\nIncreasing difficulty:\n _ _  _ {-1.5, -0.75, 0, 0.75, 1.5}. _\nAdaptive presentation of items\n\n\nConditional Probability Tables\n\n\n\n\nPrior\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\n\n\n-2\n0.1\n0.3775\n0.2227\n0.1192\n0.0601\n0.0293\n\n\n-1\n0.2\n0.6225\n0.4378\n0.2689\n0.1480\n0.0759\n\n\n0\n0.4\n0.8176\n0.6792\n0.5000\n0.3208\n0.1824\n\n\n1\n0.2\n0.9241\n0.8520\n0.7311\n0.5622\n0.3775\n\n\n2\n0.1\n0.9707\n0.9399\n0.8088\n0.7773\n0.6225\n\n\n\n\n\nProblems Set 1\n\nAssume  =1, what is expected score (sum X j )\nCalculate P(  _ |X_ 1 = right ), E(  _ |X_ 1 = right )\nCalculate P(  _ |X_ 5 = right ), E(  _ |X_ 5 = right )\nScore three students who have the following observable patterns (Tasks 1--5):\n\n1,1,1,0,0\n1,0,0,1,1\n1,1,1,0,1\n\n\n5. Suppose we have observed for a given student X 2 = right and X 3 = right , what is the next best item to present (hint, look for expected probabilities closest to .5,.5\n6. Same thing, with X 2 = right and X 3 = wrong\n7. Same thing, with X 2 = wrong and X 3 = wrong\n\n\n2. “Context” effect –Testlets\n\nStandard assumption of conditional independence of observable variables given Proficiency Variables\nViolation\n\nShared stimulus\nContext\nSpecial knowledge\nShared Work Product\nSequential dependencies\nScoring Dependencies (Multi-step problem)\n\nTestlets (Wainer & Kiely, 1987)\nViolation results in overestimating the evidential value of observables for Proficiency Variables\n\n\n\n“Context” effect – Variables\n\nContext variable – A parent variable introduced to handle conditional dependence among observables (testlet)\n\nConsistent with Stout’s (1987) ‘essential n-dimensionality’\nWang, Bradlow & Wainer (2001) SCORIGHT program for IRT\nPatz & Junker (1999) model for multiple ratings\n\n\n\n\n“Context” effect – example\nSuppose that Items 3 and 4 share common presentation material\nExample: a word problem about “Yacht racing” might use nautical jargon like “leeward” and “tacking”\nPeople familiar with the content area would have an advantage over people unfamiliar with the content area.\nWould never us this example in practice because of DIF (Differential Item Functioning)\n\n\nAdding a context variable\nGroup Items 3 and 4 into a single task with two observed outcome variables\nAdd a person-specific, task-specific latent variable called “context” with values familiar and unfamiliar\nEstimates of  will “integrate out” the context effect\nCan use as a mathematical trick to force dependencies between observables.\n\n\nIRT Model with Context Variable\n\n\n\nProblem Set 2\n\nCompare the following quantities in the context and no context models:\n\nP(X2), P(X3), P(X4)\nP(|X2= right ), P(|X3= right )\nP(X4|X2= right ), P(X4 |X3= right )\nP(|X3= wrong , X4= wrong ), P(|X3= right , X4= wrong ),\nP(|X3= wrong , X4= right ), P(|X3= right , X4= right )\n\n\n\n\nContext Effect Postscript\nIf Context effect is generally construct-irrelevant variance, if correlated with group membership this is bad (DIF)\nWhen calibrating using 2PL IRT model, can get similar joint distribution for  , X 3 , and X 4  by decreasing the discrimination parameter\n\n\n3. Combination Models\nConsider a task which requires two Proficiencies:\nThree different ways to combine those proficiencies:\nCompensatory : More of Proficiency 1 compensates for less of Proficiency 2. Combination rule is sum .\nConjunctive : Both proficiencies are needed to solve the problem. Combination rule is minimum.\nDisjunctive : Two proficiencies represent alternative solution paths to the problem. Combination rule is maximum.\n\n\nCombination Model Graphs\n\n\n\nCommon Setup for All Three Models\nThere are two parent nodes, and both parents are conditionally independent of each other. The difference among the three models lies in the third term below:\n_ P_ ( P 1 , P 2 , X ) = P ( P 1  ) • P ( P 2  ) • P ( X _ _ | P 1 , P 2 )\nThe priors for the parent nodes are the same for the three models with 0.3333 of probability at each of the H, M, and L states.\nThe initial marginal probability for X is the same for the three models (50/50).\n\n\nConditional Probability Tables\nThis table contains the conditional probabilities for the parent nodes (P1 and P2) and the combination model for the three models.\nTable 3 – Part 2\nConditional Problems for Compensatory, Conjunctive, and Disjunctive\nP1 _ P2_ Compensatory Conjunctive Disjunctive “Right” “Right” “Right”\nH H 0.9 0.9 0.7\nH M 0.7 0.7 0.7\nH L 0.5 0.3 0.7\nM H 0.7 0.7 0.7\nM M 0.5 0.7 0.3\nM L 0.3 0.3 0.3\nL H 0.5 0.3 0.7\nL M 0.3 0.3 0.3\nL L 0.1 0.3 0.1\n\n\nProblem Set 3\nVerify that P(P 1 ), P(P 2 ), and P(Obs) are the same for all three models. ( Obs represents either the node Compensatory , Conjunctive, or Disjunctive )\nAssume   Obs  =  right  , Calculate   P(P   1   )   and   P(P   2   )   for all three models. \nAssume   Obs  =  wrong  , Calculate   P(P   1   )   and   P(P   2   )   for all three models.\nAssume   Obs  =  right  , and   P   1   _ = _   H  . Calculate   P(P   2   )   for all three models. \nAssume   Obs  =  right  , and   P   1   _ = _   M  . Calculate   P(P   2   )   for all three models.\nAssume   Obs  =  right  , and   P   1   _ = _   L  . Calculate   P(P   2   )   for all three models.\nExplain the differences\n\n\nActivity 3\n\nGo back to the Driver’s License Exam you built in Session I and add some numbers\nNow put in some observed outcomes\n\nHow did the probabilities change?\nIs that about what you expected?\n\n\n\n\nACED Background\n\nACED (Adaptive Content with Evidence-based Diagnosis)\nVal Shute (PD), Aurora Graf, Jody Underwood, Eric Hansen, Peggy Redman, Russell Almond, Larry Casey, Waverly Hester, Steve Landau, Diego Zapata\nDomain: Middle School Math, Sequences\nProject Goals:\n\nAdaptive Task Selection\nDiagnostic Feedback\nAccessibility\n\n\n\n\nACED Features\n Valid Assessment  .  Based on evidence-centered design (ECD).\n Adaptive Sequencing  .  Tasks presented in line with an adaptive algorithm.\n Diagnostic Feedback  .  Feedback is immediate and addresses common errors and misconceptions.\n Aligned  .  Assessments aligned with (a) state and national standards and (b) curricula in current textbooks.\n\n\nACED Proficiency Model\n\n\n\nTypical Task\n\n\n\nACED Design/Build Process\nIdentify Proficiency variables\nStructure Proficiency Model\nElicit Proficiency Model Parameters\nConstruct Tasks to target proficiencies at Low/Medium/High difficulty\nBuild Evidence Models based on difficulty/Q-Matrix\n\n\nParameterization of Network\n\nProficiency Model:\n\nBased on Regression model of child given parent\nSME provided correlation and intercept\nSME has low confidence in numeric values\n\nEvidence Model Fragment\n\nTasks Scored Right / Wrong\nBased on IRT model\nHigh / Medium / Low corresponds to  = +1/0/-1\nEasy/Medium/Hard corresponds to difficulty -1/0/+1\nDiscrimination of 1\nUsed Q-Matrix to determine which node is parent\n\n\n\n\nPM-EM Algorithm for Scoring\n\nMaster Bayes net with just proficiency model(PM)\nDatabase of Bayes net fragments corresponding to evidence models (EMs), indexed by task ID\nTo score a task:\n\nFind EM fragment corresponding to task\nJoin EM fragment to PM\nEnter Evidence\nAbsorb evidence from EM fragment into network\nDetach EM fragment\n\n\n\n\nAn Example\n\nFive proficiency variables\nThree tasks, with observables {X11}, {X21, X22 , X23}, {X31}.\n\n\nQ: Which observables depend on which proficiency variables?\u000bA: See the Q-matrix (Fischer, Tatsuoka).\n\n\n\n\nq1\nq2\nq3\nq4\nq5\nX23\n\n\n\n\nX11\n1\n0\n0\n0\n0\n–\n\n\nX21\n0\n1\n0\n0\n0\n1\n\n\nX22\n0\n1\n0\n1\n0\n1\n\n\nX23\n0\n0\n0\n0\n0\nN/A\n\n\nX31\n0\n1\n1\n1\n0\n–\n\n\n\n\n\nProficiency Model / Evidence Model Split\n\nFull Bayes net for proficiency model and observables for all tasks can be decomposed into fragments.\n\nProficiency model fragment(s) (PMFs) contain proficiency variables.\nAn evidence model fragment (EMF) for each task.\nEMF contains observables for that task and all proficiency variables that are parents of any of them.\n\nPresumes observables are conditionally independent between tasks, but can be dependent within tasks.\nAllows for adaptively selecting tasks, docking EMF to PMF, and updating PMF on the fly.\n\n\n\nOn the way to PMF and EMFs…\n\nProficiency variables\n\nObservables and proficiency variable parents for the tasks\n\n\nMarry parents, drop directions, and triangulate (in PMF, with respect to all tasks)\n\n\n\n\nFootprints of tasks in proficiency model (figure out from rows in Q-matrix)\n\n\n\n\nResult:\n\nEach EMF implies a join tree for Bayes net propagation.\n\nInitial distributions for proficiency variables are uniform.\n\nThe footprint of the PM in the EMF is a clique intersection between that EMF and the PMF.\nCan “dock” EMFs with PMF one-at-a-time, to …\n\nabsorb evidence from values of observables to that task as updated probabilities for proficiency variables, and\npredict responses in new tasks, to evaluate potential evidentiary value of administering it.\n\n\n\n\nDocking evidence model fragments\n\n\nScoring Exercise\n\n\n\n\n\n\n\n\n\nOutcome\nTask Name\nProficiency Variable\nDifficulty\n\n\n\n\nWrong\ntCommonRatio1a.xml\nCommonRatio\nEasy\n\n\nRight\ntCommonRatio2b.xml\nCommonRatio\nMedium\n\n\nWrong\ntCommonRatio3b.xml\nCommonRatio\nHard\n\n\nWrong\ntExplicitGeometric1a.xml\nExplicitGoemetric\nEasy\n\n\nRight\ntExplicitGeometric2a.xml\nExplicitGoemetric\nMedium\n\n\nWrong\ntExplicitGeometric3b.xml\nExplicitGoemetric\nHard\n\n\nWrong\ntRecursiveRuleGeometric1a.xml\nRecursiveRuleGeometric\nEasy\n\n\nWrong\ntRecursiveRuleGeometric2b.xml\nRecursiveRuleGeometric\nMedium\n\n\nWrong\ntRecursiveRuleGeometric3a.xml\nRecursiveRuleGeometric\nHard\n\n\nRight\ntTableExtendGeometric1a.xml\nTableGeometric\nEasy\n\n\nRight\ntTableExtendGeometric2b.xml\nTableGeometric\nMedium\n\n\nRight\ntTableExtendGeometric3a.xml\nTableGeometric\nHard\n\n\nWrong\ntVerbalRuleExtendModelGeometric1a.xml\nVerbalRuleGeometric\nEasy\n\n\nWrong\ntVerbalRuleExtendModelGeometric1b.xml\nVerbalRuleGeometric\nEasy\n\n\nRight\ntVerbalRuleExtendModelGeometric2a.xml\nVerbalRuleGeometric\nMedium\n\n\nWrong\ntVisualExtendGeometric1a.xml\nVisualGeometric\nEasy\n\n\nWrong\ntVisualExtendGeometric2a.xml\nVisualGeometric\nMedium\n\n\nWrong\ntVisualExtendGeometric3a.xml\nVisualGeometric\nHard\n\n\n\n\n\nWeight of Evidence\nGood (1985)\nH is binary hypothesis, e.g., Proficiency > Medium\nE is evidence for hypothesis\nWeight of Evidence (WOE) is\n\n\n\nProperties of WOE\n“Centibans” (log base 10, multiply by 100)\nPositive for evidence supporting hypothesis, negative for evidence refuting hypothesis\nMovement in tails of distribution as important as movement near center\nBayes theorem using log odds\n\n\nConditional Weight of Evidence\nCan define Conditional Weight of Evidence\nNice Additive properties\nOrder sensitive\nWOE Balance Sheet (Madigan, Mosurski & Almond, 1997)\n\n\n\n\nEvidence Balance Sheet\n63 tasks total\n1 Easy\n2 Medium\n3 Hard\na Item type\nb Isomorph\nP(Solve Geom Sequences)\n\n\n\nTask\nAcc\nH\nM\nL\n\n\n\n\nSolveGeometricProblems2a\n0\n0.16\n0.26\n0.58\n\n\nSolveGeometricProblems3a\n1\n0.35\n0.35\n0.30\n\n\nSolveGeometricProblems3b\n1\n0.64\n0.29\n0.07\n\n\nSolveGeometricProblems2b\n1\n0.83\n0.16\n0.01\n\n\nVisualExtendTable2a\n1\n0.89\n0.10\n0.01\n\n\nSolveGeometricProblems1a\n0\n0.78\n0.21\n0.01\n\n\nSolveGeometricProblems1b\n1\n0.82\n0.18\n0.00\n\n\nVisualExtendVerbalRule2a\n1\n0.85\n0.15\n0.00\n\n\nModelExtendTableGeometric3a\n1\n0.90\n0.10\n0.00\n\n\nExamplesGeometric2a\n0\n0.87\n0.13\n0.00\n\n\nVisualExplicitVerbalRule3a\n1\n0.91\n0.09\n0.00\n\n\nVerbalRuleModelGeometric3a\n1\n0.95\n0.05\n0.00\n\n\n\nWOE for H vs. M, L\n\n\nExpected Weight of Evidence\nWhen choosing next “test” (task/item) look at expected value of WOE where expectation is taken wrt P(E|H) .\nwhere represent the possible results.\n\n\n\n\nCalculating EWOE\nMadigan and Almond (1996)\nEnter any observed evidence into net\nInstantiate Hypothesis = True (may need to use virtual evidence if hypothesis is compound)\nCalculate for each candidate item\nInstantiate Hypothesis = False\nCalculate for each candidate item\n\n\n\n\nRelated Measures\nValue of Information\nS is proficiency state\nd is decision\nu is utility\n\n\n\nRelated Measures (2)\nMutual Information\nExtends to non-binary hypothesis nodes\nKullback-Liebler distance between joint distribution and independence\n\n\n\n\nTask Selection Exercise 1\n\nUse ACEDMotif1.dne\n\nEasy, Medium, and Hard tasks for Common Ratio and Visual Geometric\n\nUse Hypothesis SolveGeometricProblems > Medium\nCalculate EWOE for six observables\nAssume candidate gets first item right and repeat\n\nNext assume candidate gets first item wrong and repeat\nRepeat exercise using hypothesis SolveGeometricProblems > Low\nUse Network ACEDMotif2.dne\nSelect the SolveGeometricProblems node\nRun the program Network>Sensitivity to Findings\nThis will list the Mutual information for all nodes\nSelect the observable with the highest mutual information as the first task\nUse this to process a person who gets every task right\nUse this to process a person who gets every task wrong\n\n\nACED Evaluation\n\nMiddle School Students\nDid not normally study geometric series\nFour conditions:\n\nElaborated Feedback/Adaptive (E/A; n=71)\nSimple Feedback/Adaptive (S/A; n=75)\nElaborated Feedback/Linear (E/L; n=67)\nControl (no instruction; n=55)\n\nStudents given all 61 geometric items\nAlso given pretest/posttest (25 items each)\n\n\n\nACED Scores\n\n\nFor Each Proficiency Variable\n\nMarginal Distribution\nModal Classification\nEAP Score (High=1, Low=-1)\n\n\n\n\nACED Reliability\n\n\n\nProficiency (EAP)\nReliability\n\n\n\n\nSolve Geometric Sequences (SGS)\n0.88\n\n\nFind Common Ratio\n0.90\n\n\nGenerate Examples\n0.92\n\n\nExtend Sequence\n0.86\n\n\nModel Sequence\n0.80\n\n\nUse Table\n0.82\n\n\nUse Pictures\n0.82\n\n\nInduce Rules\n0.78\n\n\nNumber Right\n0.88\n\n\n\nCalculated with Split Halves (ECD design)\nCorrelation of EAP score with posttest is 0.65 (close to reliability of posttest)\nEven with pretest forced into the equation, EAP score accounted for 17% unique variance\nReliability of modal classifications was worse\n\n\nEffect of Adaptivity\n\nFor adaptive conditions, correlation with posttest seems to hit upper limit by 20 items\nStandard Error of Correlations is large\nJump in linear case related to sequence of items\n\n\nEffect of feedback\nE/A showed significant gains\nOthers did not\nLearning and assessment reliability!!!!!\n\n\nAcknowledgements\nSpecial thanks to Val Shute for letting us used ACED data and models in this tutorial.\nACED development and data collection was sponsored by National Science Foundation Grant No. 0313202.\nComplete data available at: http://ecd.ralmond.net/ecdwiki/ACED/ACED"
  },
  {
    "objectID": "ECD.html#evidence-centered-design",
    "href": "ECD.html#evidence-centered-design",
    "title": "ECD Intro",
    "section": "Evidence Centered Design",
    "text": "Evidence Centered Design\n\nEvidence Centered Design (ECD) provides a mechanism for\n\nCapturing and documenting information about the structure and strength of evidentiary relationships.\nCoordinating the work of test developers in authoring tasks and psychometricians in calibrating the measurement model.\nDocumenting the scientific information that provides the foundation for the assessment and its validity."
  },
  {
    "objectID": "ECD.html#the-central-question",
    "href": "ECD.html#the-central-question",
    "title": "ECD Intro",
    "section": "The Central Question",
    "text": "The Central Question\n\nEvidence-centered design centers around the questions:\n\n“What can we observe about an examinee’s performance which will provide evidence that the examinee has or does not have the knowledge, skills and abilities we wish to make claims about?”\n\n\n“How can we structure situations to be able to make those observations?”\n\n\nThis process results in the Conceptual Assessment Framework (CAF)"
  },
  {
    "objectID": "ECD.html#activity-1-drivers-license-exam",
    "href": "ECD.html#activity-1-drivers-license-exam",
    "title": "ECD Intro",
    "section": "Activity 1: Driver’s License Exam",
    "text": "Activity 1: Driver’s License Exam\nRedesign the driver’s licensure exam\nWrite down several claims you would like to make about people who receive a driver’s license\nGroup your claims into several proficiency variables related to the driver’s test\nDo the claims hold for high, medium or low values of those variables?\nUse Netica as a drawing tool and add your variables"
  },
  {
    "objectID": "ECD.html#ecd---bayes-nets",
    "href": "ECD.html#ecd---bayes-nets",
    "title": "ECD Intro",
    "section": "ECD -> Bayes Nets",
    "text": "ECD -> Bayes Nets\nRepresent Qualitative ECD argument with a graph (Domain Modeling) (Session I)\nTurn graphical structure into probability distribution over proficiency variables and observable outcomes (Bayes net; Session I)\nPerform inference (scoring) using that Bayes net (Session II)\nExpress probabilities in terms of unknown parameters – learn parameters (Session III)\nRefine model based on how well it fits data (Session IV)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website is the latest iteration of the notes for the long running series of tutorial Bayesian Networks in Eductional Assessment, which has run at the National Committee for Measurement in Education (NCME) since 2002. It complements the book Bayesian Networks in Educational Assessment available from Springer.\nThe original version was developed by Russell Almond, Bob Mislevy, Duanli Yan and David Williamson with assistance from Linda Steinberg and supported by Educational Testing service.\n\n\n\nETS Logo\n\n\nOver the years, others have contributed materials. In particular, Russell Almond (now at Florida State University), Roy Levy (Arizona State University) and Diego Zapata (Educational Testing Service).\n\n\n\nFSU Seal\n\n\nSpecial thanks to Val Shute (ETS and FSU) for letting us used ACED data and models in this tutorial. ACED development and data collection was sponsored by National Science Foundation Grant No. 0313202. Complete data available at: http://ecd.ralmond.net/ecdwiki/ACED/ACED"
  },
  {
    "objectID": "about.html#current-presenters",
    "href": "about.html#current-presenters",
    "title": "About",
    "section": "Current Presenters",
    "text": "Current Presenters\n Russell Almond is an Associate Professor of Measurement and Statistics at Florida State University. He has long been interested in the intersection of artificial intelligence and statistics. With Bob Mislevy and Linda Steinberg he developed the initial version of evidence-centered assessment design (ECD). His home page is at https://ralmond.net, and he adminsters the machine https://pluto.coe.fsu.edu. His R packages are available through https://ralmond.r-universe.net and the source code is at https://github.com/ralmond, and his Mastadon handle is @ralmond@mlhangout.\n\n\n\nDuanli Yan\n\n\nDuanli Yan is a director of data analysis and computational research in the Research and Measurement Science area of Research & Development division at ETS. There she has led automated scoring model building and evaluation. She was the statistical coordinator responsible for the EXADEP™ test and the TOEIC® institutional programs. Yan was a recipient of the 2011 ETS Presidential Award, 2013 NCME Brenda Loyd Outstanding Dissertation Award from the , 2015 IACAT Early Career Award, and 2016 AERA Division D Significant Contribution to Educational Measurement and Research Methodology Award. She is a coauthor on the following books: Bayesian Networks in Educational Assessment, Computerized Multistage Testing: Theory and Applications, and Computerized Adaptive and Multistage Testing with R.\n\n\n\nDiego Zapata\n\n\nDiego Zapata-Rivera Diego Zapata-Rivera is a distinguished presidential appointee at ETS. He earned a Ph.D. in computer science (with a focus on artificial intelligence in education) from the University of Saskatchewan in 2003. His research at ETS has focused on the areas of innovations in score reporting and technology-enhanced assessment including work on adaptive learning and assessment environments, and game-based assessments. His research interests also include Bayesian student modeling, open student models, conversation-based tasks, caring assessment, virtual communities, authoring tools, and program evaluation. He has produced more than 100 publications including journal articles, book chapters, and technical papers. He has served as a reviewer for several international conferences and journals. He has been a committee member and organizer of international conferences and workshops in his research areas. He is a member of the editorial board of User Modeling and User-Adapted Interaction, an associate editor for AI for Human Learning and Behavior Change, and a former associate editor of the IEEE Transactions on Learning Technologies. Most recently, he has been invited to contribute his expertise to projects sponsored by the National Research Council, the National Science Foundation, NASA, and the U.S. Army Research Laboratory."
  },
  {
    "objectID": "about.html#acknowledgements",
    "href": "about.html#acknowledgements",
    "title": "About",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe have a lot of people to thank in the making of this tutorial. Obviously Bob and David played big roles in the development of the original tutorials (and we are still using some of their slides). They also helped with the planning for this 2nd edition. Linda Steinberg was extremely important in the original development of both evidence-centered assessment design and many of the original applications using Bayesian networks, particularly Biomass. Here project management skills drove us to find practical problems for many issues. Val Shute has generously offered us the Bayesian networks and data from here ACED project to use as examples. Brent Boerlage @ Norsys has generously provided us with time-limited keys for Netica (although many of the class exercises can be done with the student version). More information about both the Netica GUI and API (needed for RNetica to work) can be found at http://norsys.com/. ETS has generously covered the cost of printing and shipping the paper copies of the slides and Springer has been helpful in arranging for copies of the book Bayesian Networks in Educational Assessment to be available at the tutorial. Last but not least, we would like to thank the NCME staff and volunteers for arranging a host of important details without which this would be a much less pleasant version.\nFinally, we want to thank all of you who have come to the tutorial over the past decade. Your questions and feedback have helped us mold the tutorial to better meet the needs of the NCME audience. We hope that you will continue to provide us with questions and feedback."
  },
  {
    "objectID": "about.html#support",
    "href": "about.html#support",
    "title": "About",
    "section": "Support",
    "text": "Support\nDevelopment of the original tutorial was supported by the ETS Research Allocation.\nDevelopment of the Peanut suite of tools has been supported in part by\n\nBill & Melinda Gates Foundation grant “Games as Learning/Assessment: Stealth Assessment” (#0PP1035331, Val Shute, PI)\nNational Science Foundation grant “DIP: Game-based Assessment and Support of STEM-related Competencies” (#1628937, Val Shute, PI)\nNational Scient Foundation grant “Mathematical Learning via Architectual Design and Modeling Using E-Rebuild.” (#1720533, Fengfeng Ke & Russell Almond, PIs)\nInstitute of Educational Statistics Grant: “Exploring adaptive cognitive and affective learning support for next-generation STEM learning games.” (#R305A170376-20, Val Shute and Russell Almond, PIs"
  },
  {
    "objectID": "about.html#legal-stuff",
    "href": "about.html#legal-stuff",
    "title": "About",
    "section": "Legal Stuff",
    "text": "Legal Stuff\nSlides and handouts for the original tutorial are copyright 2002—2015 Educational Testing Service. Sessions I and II are copyright 2017-8 Educational Testing Service. Session III is copyright 2017-8 Russell G. Almond and includes material from the 2002—2015 used by permission of ETS. Session IV is copyright 2017-8 Roy Levy.\nThese materials are an unpublished, proprietary works of their respective rights holders. Any limited distribution shall not constitute publication. This work may not be reproduced or distributed to third parties without prior written consent. Submit request for the ETS material (Sessions I and II) through http://www.ets.org/legal/copyright.html, and for Sessions III to Russell Almond (ralmond@fsu.edu) and Session IV to Roy Levy (roy.levy@asu.edu)."
  },
  {
    "objectID": "BayesNets.html#irt-taskevidence-model",
    "href": "BayesNets.html#irt-taskevidence-model",
    "title": "Bayesian Networks",
    "section": "IRT Task/Evidence Model",
    "text": "IRT Task/Evidence Model\nTasks yield an work product which can be unambiguously scored right or wrong.\nEach task has a single observable outcome variable.\nTasks are often called items, although the common usage often blurs the distinction between the presentation of the item and the outcome variable."
  },
  {
    "objectID": "BayesNets.html#irt-rasch-evidence-model",
    "href": "BayesNets.html#irt-rasch-evidence-model",
    "title": "Bayesian Networks",
    "section": "IRT (Rasch) Evidence Model",
    "text": "IRT (Rasch) Evidence Model\n\nLet \\(X_j\\) be observable outcome variable from Task \\(j\\).\n\\(\\Pr(X_j =\\text{right} | \\theta, \\beta_j) = \\frac{1}{1+e^{-(\\theta-\\beta_j)}}\\) - \\(\\beta_j\\) is the difficulty of the item.\nCan crank through the formula for each of the five possible values of \\(\\theta\\) to get values for Conditional Probability Tables (CPT)"
  },
  {
    "objectID": "BayesNets.html#irt-assembly-model",
    "href": "BayesNets.html#irt-assembly-model",
    "title": "Bayesian Networks",
    "section": "IRT Assembly Model",
    "text": "IRT Assembly Model\n5 items\nIncreasing difficulty:\n\\[ (\\beta_1, \\ldots, \\beta_5) = (-1.5, -0.75, 0, 0.75, 1.5)\\ \\] .\n\nirt1pl <- function(theta,beta) {\n  1/(1+exp(beta-theta))\n}\nirt1pl(c(-2,-1,0,1,2),0)\n\n[1] 0.1192029 0.2689414 0.5000000 0.7310586 0.8807971\n\n\nItems are presented adaptively."
  },
  {
    "objectID": "BayesNets.html#conditional-probability-tables",
    "href": "BayesNets.html#conditional-probability-tables",
    "title": "Bayesian Networks",
    "section": "Conditional Probability Tables",
    "text": "Conditional Probability Tables\n\n\n\nθ\nPrior\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\n\n\n-2\n0.1\n0.3775\n0.2227\n0.1192\n0.0601\n0.0293\n\n\n-1\n0.2\n0.6225\n0.4378\n0.2689\n0.1480\n0.0759\n\n\n0\n0.4\n0.8176\n0.6792\n0.5000\n0.3208\n0.1824\n\n\n1\n0.2\n0.9241\n0.8520\n0.7311\n0.5622\n0.3775\n\n\n2\n0.1\n0.9707\n0.9399\n0.8088\n0.7773\n0.6225"
  },
  {
    "objectID": "BayesNets.html#problems-set-1",
    "href": "BayesNets.html#problems-set-1",
    "title": "Bayesian Networks",
    "section": "Problems Set 1",
    "text": "Problems Set 1\nUse the network CompensatoryConjunctiveNets/IRT5.dne to answer these questions.\n\n#|include: FALSE\n#system(\"netica CompensatoryConjunctiveNets/IRT5.dne\")\n\nInside Netica, to set a node to a value either, click on the value name, or right click and select the value.\nTo clear a value, click on it again, or right click and select “undefined”.\n\nAssume \\(\\theta =1\\), what is expected score (sum \\(X_j\\) )\nCalculate \\(P(\\theta |X_1 \\text{right})\\), \\(E[\\theta |X_1 =\\text{right}]\\).\nCalculate \\(P(\\theta |X_5 = \\text{right})\\), \\(E[\\theta |X_5 = \\text{right}]\\).\nScore three students who have the following observable patterns (Tasks 1--5):\n\n\n1,1,1,0,0\n1,0,0,1,1\n1,1,1,0,1\n\n\nSuppose we have observed for a given student \\(X_2 = \\text{right}\\) and \\(X_3=\\text{right}\\) , what is the next best item to present (hint, look for expected probabilities closest to .5,.5\nSame thing, with \\(X_2 = \\text{right}\\) and \\(X_3=\\text{wrong}\\)\nSame thing, with \\(X_2 = \\text{wrong}\\) and \\(X_3=\\text{wrong}\\)"
  },
  {
    "objectID": "BayesNets.html#context-effect-variables",
    "href": "BayesNets.html#context-effect-variables",
    "title": "Bayesian Networks",
    "section": "“Context” effect – Variables",
    "text": "“Context” effect – Variables\n\nDiagrammeR::grViz('\ndigraph IRTC {\n  subgraph{ Q[label=\"θ\"]; Context }\n  subgraph {\n  X1; X2; X3; X4; X5;\n  }\n  Q -> X1; Q-> X2; Q-> X3; Q->X4; Q->X5\n  Context -> X3; Context -> X4;\n}')\n\n\n\n\n\n\nContext variable – A parent variable introduced to handle conditional dependence among observables (testlet)\n\nConsistent with Stout’s (1987) ‘essential n-dimensionality’\nWang, Bradlow & Wainer (2001) SCORIGHT program for IRT\nPatz & Junker (1999) model for multiple ratings"
  },
  {
    "objectID": "BayesNets.html#context-effect-example",
    "href": "BayesNets.html#context-effect-example",
    "title": "Bayesian Networks",
    "section": "“Context” effect – example",
    "text": "“Context” effect – example\nSuppose that Items 3 and 4 share common presentation material\nExample: a word problem about “Yacht racing” might use nautical jargon like “leeward” and “tacking”\nPeople familiar with the content area would have an advantage over people unfamiliar with the content area.\nWould never us this example in practice because of DIF (Differential Item Functioning)"
  },
  {
    "objectID": "BayesNets.html#adding-a-context-variable",
    "href": "BayesNets.html#adding-a-context-variable",
    "title": "Bayesian Networks",
    "section": "Adding a context variable",
    "text": "Adding a context variable\nGroup Items 3 and 4 into a single task with two observed outcome variables\nAdd a person-specific, task-specific latent variable called “context” with values familiar and unfamiliar\nEstimates of \\(\\theta\\) will “integrate out” the context effect\nCan use as a mathematical trick to force dependencies between observables."
  },
  {
    "objectID": "BayesNets.html#irt-model-with-context-variable",
    "href": "BayesNets.html#irt-model-with-context-variable",
    "title": "Bayesian Networks",
    "section": "IRT Model with Context Variable",
    "text": "IRT Model with Context Variable\nUse the network CompensatoryConjunctiveNets/IRT5C.dne to answer these questions.\n\n#|include: FALSE\n#system(\"netica CompensatoryConjunctiveNets/IRT5C.dne\")\n\nThe CPTs in IRT5C have been set so that the marginal predictions (if the context is unknown), should have the same value.\n\n\n\nIRT model with Context Effect"
  },
  {
    "objectID": "BayesNets.html#problem-set-2",
    "href": "BayesNets.html#problem-set-2",
    "title": "Bayesian Networks",
    "section": "Problem Set 2",
    "text": "Problem Set 2\n\nCompare the following quantities in the context and no context models:\n\n\\(P(X_2)\\), \\(P(X_3)\\), \\(P(X_4)\\)\n\\(P(\\theta|X_2= \\text{right})\\), \\(P(\\theta|X_3= \\text{right} )\\)\n\\(P(X_4|X_2= \\text{right} )\\), \\(P(X_4 |X_3= \\text{right} )\\)\n\\(P(\\theta|X_3=\\text{wrong}, X_4=\\text{wrong})\\), \\(P(\\theta|X_3=\\text{right}, X_4=\\text{wrong})\\),\n\\(P(\\theta|X_3=\\text{wrong}, X_4=\\text{right})\\), \\(P(\\theta|X_3= \\text{right}, X_4=\\text{right})\\)"
  },
  {
    "objectID": "BayesNets.html#context-effect-postscript",
    "href": "BayesNets.html#context-effect-postscript",
    "title": "Bayesian Networks",
    "section": "Context Effect Postscript",
    "text": "Context Effect Postscript\nIf Context effect is generally construct-irrelevant variance, if correlated with group membership this is bad (DIF)\nWhen calibrating using 2PL IRT model, can get similar joint distribution for \\(\\theta\\), \\(X_3\\), and \\(X_4\\) by decreasing the discrimination parameter"
  },
  {
    "objectID": "BayesNets.html#common-setup-for-all-three-models",
    "href": "BayesNets.html#common-setup-for-all-three-models",
    "title": "Bayesian Networks",
    "section": "Common Setup for All Three Models",
    "text": "Common Setup for All Three Models\nThere are two parent nodes, and both parents are conditionally independent of each other. The difference among the three models lies in the third term below:\n\\[P( P_1, P_2, X) = P( P_1) \\cdot P(P_2) \\cdot P(X| P_1,P_2 )\\]\nThe priors for the parent nodes are the same for the three models with 0.3333 of probability at each of the H, M, and L states.\nThe initial marginal probability for X is the same for the three models (50/50)."
  },
  {
    "objectID": "BayesNets.html#conditional-probability-tables-1",
    "href": "BayesNets.html#conditional-probability-tables-1",
    "title": "Bayesian Networks",
    "section": "Conditional Probability Tables",
    "text": "Conditional Probability Tables\nThis table contains the conditional probabilities for the parent nodes (P1 and P2) and the combination model for the three models.\n{r CPTcombine3 Pcomp <- c(H=.9,M=.5,L=.1) Pconj <- c(H=.9,M=.7,L=.3) Pdisj <- c(H=.7,M=.3,L=.1) cpts <- data.frame(P1=rep(names(Pcomp),each=3),P2=rep(names(Pcomp),3),                    Compensatory=as.vector(outer(Pcomp,Pcomp,\"+\")/2),                    Conjunctive=as.vector(outer(Pconj,Pconj,\"min\")),                    Disjunctive=as.vector(outer(Pdisj,Pdisj,\"max\"))) knitr::kable(cpts)"
  },
  {
    "objectID": "BayesNets.html#problem-set-3",
    "href": "BayesNets.html#problem-set-3",
    "title": "Bayesian Networks",
    "section": "Problem Set 3",
    "text": "Problem Set 3\nUse the network CompensatoryConjunctiveNets/Comb3Same_1.dne to answer these questions.\n\n#|include: FALSE\n#system(\"netica CompensatoryConjunctiveNets/Comb3Same_1.dne\")\n\n\nVerify that \\(P(P_1)\\), \\(P(P_2)\\) and \\(P(Obs)\\) are the same for all three models. ( Obs represents either the node Compensatory , Conjunctive, or Disjunctive )\nAssume Obs=right; calculate \\(P(P_1)\\) and \\(P(P_2)\\) for all three models.\nAssume Obs=wrong; calculate \\(P(P_1)\\) and \\(P(P_2)\\) for all three models.\nAssume Obs=right, and \\(P_1\\)=H; Calculate \\(P(P_2)\\) for all three models.\nAssume Obs=right, and \\(P_1\\)=M; Calculate \\(P(P_2)\\) for all three models.\nAssume Obs=right, and \\(P_1\\)=L; Calculate \\(P(P_2)\\) for all three models.\n\nExplain the differences"
  },
  {
    "objectID": "BayesNets.html#dibello-models-a-look-ahead",
    "href": "BayesNets.html#dibello-models-a-look-ahead",
    "title": "Bayesian Networks",
    "section": "DiBello Models: A look ahead",
    "text": "DiBello Models: A look ahead\nMap states of parents to points on IRT \\(\\theta\\) scale.\nUse Compensatory (average), Conjunctive (minimum) or Disjunctive (maximum) to combine effective thetas.\nUse IRT (logistic) model to map to probabilities."
  },
  {
    "objectID": "BayesNets.html#activity-3",
    "href": "BayesNets.html#activity-3",
    "title": "Bayesian Networks",
    "section": "Activity 3",
    "text": "Activity 3\n\nGo back to the Driver’s License Exam you built in Session I and add some numbers\nNow put in some observed outcomes\n\nHow did the probabilities change?\nIs that about what you expected?"
  },
  {
    "objectID": "BayesNets.html#aced-background",
    "href": "BayesNets.html#aced-background",
    "title": "Bayesian Networks",
    "section": "ACED Background",
    "text": "ACED Background\n\nACED (Adaptive Content with Evidence-based Diagnosis)\nVal Shute (PD), Aurora Graf, Jody Underwood, Eric Hansen, Peggy Redman, Russell Almond, Larry Casey, Waverly Hester, Steve Landau, Diego Zapata\nDomain: Middle School Math, Sequences\nProject Goals:\n\nAdaptive Task Selection\nDiagnostic Feedback\nAccessibility"
  },
  {
    "objectID": "BayesNets.html#aced-features",
    "href": "BayesNets.html#aced-features",
    "title": "Bayesian Networks",
    "section": "ACED Features",
    "text": "ACED Features\nValid Assessment . Based on evidence-centered design (ECD).\nAdaptive Sequencing . Tasks presented in line with an adaptive algorithm.\nDiagnostic Feedback . Feedback is immediate and addresses common errors and misconceptions.\nAligned . Assessments aligned with (a) state and national standards and (b) curricula in current textbooks."
  },
  {
    "objectID": "BayesNets.html#pm-em-algorithm-for-scoring",
    "href": "BayesNets.html#pm-em-algorithm-for-scoring",
    "title": "Bayesian Networks",
    "section": "PM-EM Algorithm for Scoring",
    "text": "PM-EM Algorithm for Scoring\n\nMaster Bayes net with just proficiency model(PM)\nDatabase of Bayes net fragments corresponding to evidence models (EMs), indexed by task ID\nTo score a task:\n\nFind EM fragment corresponding to task\nJoin EM fragment to PM\nEnter Evidence\nAbsorb evidence from EM fragment into network\nDetach EM fragment"
  },
  {
    "objectID": "BayesNets.html#an-example",
    "href": "BayesNets.html#an-example",
    "title": "Bayesian Networks",
    "section": "An Example",
    "text": "An Example\n\nFive proficiency variables\nThree tasks, with observables {X11}, {X21, X22 , X23}, {X31}."
  },
  {
    "objectID": "BayesNets.html#q-which-observables-depend-on-which-proficiency-variables",
    "href": "BayesNets.html#q-which-observables-depend-on-which-proficiency-variables",
    "title": "Bayesian Networks",
    "section": "Q: Which observables depend on which proficiency variables?",
    "text": "Q: Which observables depend on which proficiency variables?\nA: See the Q-matrix (Fischer, Tatsuoka).\n\n\n\n\nq1\nq2\nq3\nq4\nq5\nX23\n\n\n\n\nX11\n1\n0\n0\n0\n0\n–\n\n\nX21\n0\n1\n0\n0\n0\n1\n\n\nX22\n0\n1\n0\n1\n0\n1\n\n\nX23\n0\n0\n0\n0\n0\nN/A\n\n\nX31\n0\n1\n1\n1\n0\n–"
  },
  {
    "objectID": "BayesNets.html#proficiency-model-evidence-model-split",
    "href": "BayesNets.html#proficiency-model-evidence-model-split",
    "title": "Bayesian Networks",
    "section": "Proficiency Model / Evidence Model Split",
    "text": "Proficiency Model / Evidence Model Split\n\nFull Bayes net for proficiency model and observables for all tasks can be decomposed into fragments.\n\nProficiency model fragment(s) (PMFs) contain proficiency variables.\nAn evidence model fragment (EMF) for each task.\nEMF contains observables for that task and all proficiency variables that are parents of any of them.\n\nPresumes observables are conditionally independent between tasks, but can be dependent within tasks.\nAllows for adaptively selecting tasks, docking EMF to PMF, and updating PMF on the fly."
  },
  {
    "objectID": "BayesNets.html#on-the-way-to-pmf-and-emfs",
    "href": "BayesNets.html#on-the-way-to-pmf-and-emfs",
    "title": "Bayesian Networks",
    "section": "On the way to PMF and EMFs…",
    "text": "On the way to PMF and EMFs…\n\nProficiency variables\n\nObservables and proficiency variable parents for the tasks"
  },
  {
    "objectID": "BayesNets.html#marry-parents-drop-directions-and-triangulate-in-pmf-with-respect-to-all-tasks",
    "href": "BayesNets.html#marry-parents-drop-directions-and-triangulate-in-pmf-with-respect-to-all-tasks",
    "title": "Bayesian Networks",
    "section": "Marry parents, drop directions, and triangulate (in PMF, with respect to all tasks)",
    "text": "Marry parents, drop directions, and triangulate (in PMF, with respect to all tasks)\n\n\n$# Footprints of tasks in proficiency model (figure out from rows in Q-matrix)"
  },
  {
    "objectID": "BayesNets.html#result",
    "href": "BayesNets.html#result",
    "title": "Bayesian Networks",
    "section": "Result:",
    "text": "Result:\n\nEach EMF implies a join tree for Bayes net propagation.\n\nInitial distributions for proficiency variables are uniform.\n\nThe footprint of the PM in the EMF is a clique intersection between that EMF and the PMF.\nCan “dock” EMFs with PMF one-at-a-time, to …\n\nabsorb evidence from values of observables to that task as updated probabilities for proficiency variables, and\npredict responses in new tasks, to evaluate potential evidentiary value of administering it."
  },
  {
    "objectID": "BayesNets.html#docking-evidence-model-fragments",
    "href": "BayesNets.html#docking-evidence-model-fragments",
    "title": "Bayesian Networks",
    "section": "Docking evidence model fragments",
    "text": "Docking evidence model fragments"
  },
  {
    "objectID": "BayesNets.html#properties-of-woe",
    "href": "BayesNets.html#properties-of-woe",
    "title": "Bayesian Networks",
    "section": "Properties of WOE",
    "text": "Properties of WOE\n“Centibans” (log base 10, multiply by 100)\nPositive for evidence supporting hypothesis, negative for evidence refuting hypothesis\nMovement in tails of distribution as important as movement near center\nBayes theorem using log odds"
  },
  {
    "objectID": "BayesNets.html#conditional-weight-of-evidence",
    "href": "BayesNets.html#conditional-weight-of-evidence",
    "title": "Bayesian Networks",
    "section": "Conditional Weight of Evidence",
    "text": "Conditional Weight of Evidence\nCan define Conditional Weight of Evidence\nNice Additive properties\nOrder sensitive\nWOE Balance Sheet (Madigan, Mosurski & Almond, 1997)"
  },
  {
    "objectID": "BayesNets.html#evidence-balance-sheet",
    "href": "BayesNets.html#evidence-balance-sheet",
    "title": "Bayesian Networks",
    "section": "Evidence Balance Sheet",
    "text": "Evidence Balance Sheet\n63 tasks total\n1 Easy\n2 Medium\n3 Hard\na Item type\nb Isomorph\nP(Solve Geom Sequences)\n\n\n\nTask\nAcc\nH\nM\nL\n\n\n\n\nSolveGeometricProblems2a\n0\n0.16\n0.26\n0.58\n\n\nSolveGeometricProblems3a\n1\n0.35\n0.35\n0.30\n\n\nSolveGeometricProblems3b\n1\n0.64\n0.29\n0.07\n\n\nSolveGeometricProblems2b\n1\n0.83\n0.16\n0.01\n\n\nVisualExtendTable2a\n1\n0.89\n0.10\n0.01\n\n\nSolveGeometricProblems1a\n0\n0.78\n0.21\n0.01\n\n\nSolveGeometricProblems1b\n1\n0.82\n0.18\n0.00\n\n\nVisualExtendVerbalRule2a\n1\n0.85\n0.15\n0.00\n\n\nModelExtendTableGeometric3a\n1\n0.90\n0.10\n0.00\n\n\nExamplesGeometric2a\n0\n0.87\n0.13\n0.00\n\n\nVisualExplicitVerbalRule3a\n1\n0.91\n0.09\n0.00\n\n\nVerbalRuleModelGeometric3a\n1\n0.95\n0.05\n0.00\n\n\n\nWOE for H vs. M, L"
  },
  {
    "objectID": "BayesNets.html#expected-weight-of-evidence",
    "href": "BayesNets.html#expected-weight-of-evidence",
    "title": "Bayesian Networks",
    "section": "Expected Weight of Evidence",
    "text": "Expected Weight of Evidence\nWhen choosing next “test” (task/item) look at expected value of WOE where expectation is taken wrt P(E|H) .\nwhere represent the possible results."
  },
  {
    "objectID": "BayesNets.html#calculating-ewoe",
    "href": "BayesNets.html#calculating-ewoe",
    "title": "Bayesian Networks",
    "section": "Calculating EWOE",
    "text": "Calculating EWOE\nMadigan and Almond (1996)\nEnter any observed evidence into net\nInstantiate Hypothesis = True (may need to use virtual evidence if hypothesis is compound)\nCalculate for each candidate item\nInstantiate Hypothesis = False\nCalculate for each candidate item\n\n\n$# Related Measures\nValue of Information\nS is proficiency state\nd is decision\nu is utility"
  },
  {
    "objectID": "BayesNets.html#related-measures-2",
    "href": "BayesNets.html#related-measures-2",
    "title": "Bayesian Networks",
    "section": "Related Measures (2)",
    "text": "Related Measures (2)\nMutual Information\nExtends to non-binary hypothesis nodes\nKullback-Liebler distance between joint distribution and independence"
  },
  {
    "objectID": "BayesNets.html#task-selection-exercise-1",
    "href": "BayesNets.html#task-selection-exercise-1",
    "title": "Bayesian Networks",
    "section": "Task Selection Exercise 1",
    "text": "Task Selection Exercise 1\n\nUse ACEDMotif1.dne\n\nEasy, Medium, and Hard tasks for Common Ratio and Visual Geometric\n\nUse Hypothesis SolveGeometricProblems > Medium\nCalculate EWOE for six observables\nAssume candidate gets first item right and repeat\n\nNext assume candidate gets first item wrong and repeat\nRepeat exercise using hypothesis SolveGeometricProblems > Low\nUse Network ACEDMotif2.dne\nSelect the SolveGeometricProblems node\nRun the program Network>Sensitivity to Findings\nThis will list the Mutual information for all nodes\nSelect the observable with the highest mutual information as the first task\nUse this to process a person who gets every task right\nUse this to process a person who gets every task wrong"
  },
  {
    "objectID": "BayesNets.html#aced-evaluation",
    "href": "BayesNets.html#aced-evaluation",
    "title": "Bayesian Networks",
    "section": "ACED Evaluation",
    "text": "ACED Evaluation\n\nMiddle School Students\nDid not normally study geometric series\nFour conditions:\n\nElaborated Feedback/Adaptive (E/A; n=71)\nSimple Feedback/Adaptive (S/A; n=75)\nElaborated Feedback/Linear (E/L; n=67)\nControl (no instruction; n=55)\n\nStudents given all 61 geometric items\nAlso given pretest/posttest (25 items each)"
  },
  {
    "objectID": "BayesNets.html#aced-scores",
    "href": "BayesNets.html#aced-scores",
    "title": "Bayesian Networks",
    "section": "ACED Scores",
    "text": "ACED Scores\n\n\n\nACED Marginal Distributions\n\n\n\nFor Each Proficiency Variable\n\nMarginal Distribution\nModal Classification\nEAP Score (High=1, Low=-1)"
  },
  {
    "objectID": "BayesNets.html#aced-reliability",
    "href": "BayesNets.html#aced-reliability",
    "title": "Bayesian Networks",
    "section": "ACED Reliability",
    "text": "ACED Reliability\n\n\n\nProficiency (EAP)\nReliability\n\n\n\n\nSolve Geometric Sequences (SGS)\n0.88\n\n\nFind Common Ratio\n0.90\n\n\nGenerate Examples\n0.92\n\n\nExtend Sequence\n0.86\n\n\nModel Sequence\n0.80\n\n\nUse Table\n0.82\n\n\nUse Pictures\n0.82\n\n\nInduce Rules\n0.78\n\n\nNumber Right\n0.88\n\n\n\nCalculated with Split Halves (ECD design)\nCorrelation of EAP score with posttest is 0.65 (close to reliability of posttest)\nEven with pretest forced into the equation, EAP score accounted for 17% unique variance\nReliability of modal classifications was worse"
  },
  {
    "objectID": "BayesNets.html#effect-of-adaptivity",
    "href": "BayesNets.html#effect-of-adaptivity",
    "title": "Bayesian Networks",
    "section": "Effect of Adaptivity",
    "text": "Effect of Adaptivity\n\n\n\nACED Validity by test length\n\n\nFor adaptive conditions, correlation with posttest seems to hit upper limit by 20 items\nStandard Error of Correlations is large\nJump in linear case related to sequence of items"
  },
  {
    "objectID": "BayesNets.html#effect-of-feedback",
    "href": "BayesNets.html#effect-of-feedback",
    "title": "Bayesian Networks",
    "section": "Effect of feedback",
    "text": "Effect of feedback\nE/A showed significant gains\nOthers did not\nLearning and assessment reliability!!!!!"
  },
  {
    "objectID": "BayesNets.html#acknowledgements",
    "href": "BayesNets.html#acknowledgements",
    "title": "Bayesian Networks",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nSpecial thanks to Val Shute for letting us used ACED data and models in this tutorial.\nACED development and data collection was sponsored by National Science Foundation Grant No. 0313202.\nComplete data available at: http://ecd.ralmond.net/ecdwiki/ACED/ACED"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PeanutTutorial",
    "section": "",
    "text": "This website contains the downloads and handouts for the tutorial Bayesian Networks in Educational Assessment. using the Peanut Bayesian network toolkit.\nYou can access this tutorial in two ways:\n\nOnline viewing through https://pluto.coe.fsu.edu/PeanutTutorial.\nDownload the source code from https://github.com/ralmond/PeanutTutorial.git.\n\nThe latter allows you to reproduce the calculations on your own computer using R Studio. See the Resources page for more information about the software you need to download.\nThe complete tutorial has the following parts:\n\nIntroduction to Evidence-Centered Assessment Design and Bayesian Networks\nExample Bayesian Networks\nUsing Peanut to Manipulate Bayes nets\n\n\n\nScoring using RNetica\nDiBello Models for Conditional Probability Tables\nBuilding a Net with Peanut\nCalibrating Model Parameters to Data\n\n\n\nDynamic Bayesian Networks\n\nAn older version of the resources with powerpoint slides is available at https://pluto.coe.fsu.edu/BNinEA/NCMETutorial"
  },
  {
    "objectID": "Resources.html#netica",
    "href": "Resources.html#netica",
    "title": "Resources",
    "section": "Netica",
    "text": "Netica\nThere are a number of packages that will do the basic Bayesian network computations. For the purposes of this tutorial, we are using NeticaⓇ. Netica is available in two versions: a graphical user interface (GUI) version and an application programmers interface (API). We will use both in this class.\nNetica also comes in both a free (but not open source) version and a paid version. The paid version is used by purchasing an unlock code from Norsys. The free version is specifically for training and evaluation. It is limited in the size of the networks, but it has sufficient capacity for most of the exercises in this tutorial.\n\nGo to https://norsys.com and download the latest version of Netica.exe.\nUnix/linux users: Netica.exe runs without major problems under Wine64.\n\n\n\nCode\nsudo apt install wine64\ncurl https://norsys.com/downloads/Netica_Win.exe > Netica_Win.exe\nwine64 Netica_Win.exe\nwine64 \"C:\\\\Netica\\Netica 609\\Netica.exe\"\n\n\n\nMac OS Intel – I have built a wineskin version of Netica.exe. It is available at https://pluto.coe.fsu.edu/MacNetica2.app.zip (You may need to right click on the app, and then open the Wineskin app inside).\nMac OS ARM – I don’t have access to an ARM Mac to do testing. I am seeing reports that the latest version of Wine supports Rosetta. If you are using homebrew, you can try:\n\n\n\nCode\nbrew install --cask wine-stable\ncurl https://norsys.com/downloads/Netica_Win.exe > Netica_Win.exe\nwine Netica_Win.exe\nwine \"C:\\\\Netica\\Netica 609\\Netica.exe\""
  },
  {
    "objectID": "Resources.html#peanut-and-rnetia",
    "href": "Resources.html#peanut-and-rnetia",
    "title": "Resources",
    "section": "Peanut and RNetia",
    "text": "Peanut and RNetia\nThe R package RNetica provides an R binding of the Netica API (hence, it requires the API and not GUI license). RNetica is available at https://ralmond.r-universe.dev/ (source code is at https://github.com/ralmond/).\nIf you have not already done so, we recommend downloading R Studio, in the process of installing R Studio, you will need to install R itself (<https://cloud.r-project.org/).\nOnce you have R installed, you can install RNetica and running related packages by running the following code:\n\nif (!require('PNetica')) \n  install.packages(c('CPTtootls','RNetica','Peanut','PNetica'),\n                   repos = c('https://ralmond.r-universe.dev',    'https://cloud.r-project.org'))\n\nLoading required package: PNetica\n\n\nLoading required package: RNetica\n\n\nLoading required package: CPTtools\n\n\nLoading required package: Peanut\n\n\nThis bundle contains the following packages:\n\nCPTtools — A collection of functions for making conditional probability tables. (Does not depend on Netica.)\nRNetica — An R binding of the Netica API. Note that the installer downloads and installs Netica, so cognizant of the Netica License.\nPeanut — An object oriented layer for working with parameterized networks (PNets).\n\nPNetica — All of the Netica specific implementation of the Peanut protocol.\n\nThe intention is that Peanut (and CPTtools) are open implementation protocols that are independent of the specific Bayes net implementation. I’m currently looking for volunteers to build the equivalent of PNetica for other Bayes net engines."
  },
  {
    "objectID": "Resources.html#sample-networks.",
    "href": "Resources.html#sample-networks.",
    "title": "Resources",
    "section": "Sample Networks.",
    "text": "Sample Networks.\nThe following zip files contain the sample networks used in the tutorial. (If you have downloaded the complete tutorial from github, you have already downloaded these and can skip this step.)\n\nIntroNets.zip Used in Session 1\nCompensatoryConjunctiveNets.zip Used in Session 2\nminiACED Used in Session 3a\nLearningNets Used in Session ??\nDBNExamples Used in Session 4\n\nDownload these and unzip them in the directory from which you will be running R.\n(If you put them somewhere else, you may need to adjust scripts to point to the right directory.)"
  },
  {
    "objectID": "DiBelloModels.html",
    "href": "DiBelloModels.html",
    "title": "DiBello Models",
    "section": "",
    "text": "In a discrete Bayesian Network, the parameters are the conditional probability tables (CPTs).\nSize of CPT grows exponentially with number of parents.\nIn educational models, CPTs should be monotonic: higher skill states should imply higher probability of success.\nWhen learning CPTs from data, if skill variables are correlated certain combinations will be rare in data:\n\nSkill 1 is high and Skill 2 is low\nSkill 2 is low and Skill 1 is high\nThis makes for low effective sample size (high standard errors) when estimating CPTs from data."
  },
  {
    "objectID": "DiBelloModels.html#lets-make-a-some-simple-cpts",
    "href": "DiBelloModels.html#lets-make-a-some-simple-cpts",
    "title": "DiBello Models",
    "section": "Lets Make a some Simple CPTs",
    "text": "Lets Make a some Simple CPTs\nLoad PNetica (which loads Peanut and CPTtools), and start session. Build a blank Network.\n\nlibrary(PNetica)\n\nLoading required package: RNetica\n\n\nLoading required package: CPTtools\n\n\nLoading required package: Peanut\n\nsess <- NeticaSession()\nstartSession(sess)\n\nNetica 6.07 Linux (AFCl64), (C) 1992-2019 Norsys Software Corp.\n\nNetica is operating without a password; there are some limitations.\n\ntNet <- CreateNetwork(\"tNet\",sess)\n\nThe following packages are loaded.\n\nCPTtools – Tools for building Conditional Probability Tables.\nRNetica – Interface to Netica\nPeanut – Object-oriented parameterized network protocol.\nPNetica – Peanut implementation for Netica"
  },
  {
    "objectID": "DiBelloModels.html#create-a-simple-network.",
    "href": "DiBelloModels.html#create-a-simple-network.",
    "title": "DiBello Models",
    "section": "Create a Simple Network.",
    "text": "Create a Simple Network.\nCreate two parent nodes: Skill1 and Skill2, and a child node, CRItem.\n\nSkills <- NewDiscreteNode(tNet,paste(\"Skill\",1:2,sep=\"\"),c(\"H\",\"M\",\"L\"))\nCRItem <- NewDiscreteNode(tNet,\"CRItem\",c(\"FullCredit\",\"PartialCredit\",\"NoCredit\"))\nNodeParents(CRItem) <- Skills\nNodeParents(Skills[[2]]) <- Skills[1]"
  },
  {
    "objectID": "DiBelloModels.html#the-shape-of-cpts",
    "href": "DiBelloModels.html#the-shape-of-cpts",
    "title": "DiBello Models",
    "section": "The Shape of CPTs",
    "text": "The Shape of CPTs\nSkill1 has no parents, so unconditional CPT.\n\nSkills[[1]][]\n\nSkill1.H Skill1.M Skill1.L \n      NA       NA       NA \n\n\nNode that in RNetica the [] operator can be used to access the CPT of a node."
  },
  {
    "objectID": "DiBelloModels.html#conditional-probability-table",
    "href": "DiBelloModels.html#conditional-probability-table",
    "title": "DiBello Models",
    "section": "Conditional Probability Table",
    "text": "Conditional Probability Table\nSkill2 has one parent, so conditional CPT.\n\nSkills[[2]][]\n\n  Skill1 Skill2.H Skill2.M Skill2.L\n1      H       NA       NA       NA\n2      M       NA       NA       NA\n3      L       NA       NA       NA"
  },
  {
    "objectID": "DiBelloModels.html#multiple-parents",
    "href": "DiBelloModels.html#multiple-parents",
    "title": "DiBello Models",
    "section": "Multiple Parents",
    "text": "Multiple Parents\nCRItem has two parents, for total of nine (3x3) rows.\n\nCRItem[]\n\n  Skill1 Skill2 CRItem.FullCredit CRItem.PartialCredit CRItem.NoCredit\n1      H      H                NA                   NA              NA\n2      M      H                NA                   NA              NA\n3      L      H                NA                   NA              NA\n4      H      M                NA                   NA              NA\n5      M      M                NA                   NA              NA\n6      L      M                NA                   NA              NA\n7      H      L                NA                   NA              NA\n8      M      L                NA                   NA              NA\n9      L      L                NA                   NA              NA"
  },
  {
    "objectID": "DiBelloModels.html#the---operator-for-neticanodes",
    "href": "DiBelloModels.html#the---operator-for-neticanodes",
    "title": "DiBello Models",
    "section": "The [<- operator for NeticaNodes",
    "text": "The [<- operator for NeticaNodes\nRNetica maps the assignment operator for [] ([<-) to provide a wide variety of behaviors.\nBoth [ and [<- allow the user to specify a specific row or cell, or group of rows and cells. This is similar, but not quite the same as the [ operator behavior for matrixes and data frames.\nSingle Row:\n\nCRItem[Skill1=\"H\",Skill2=\"H\"] <- c(.7,.2,.1)\nCRItem[\"H\",\"H\"]\n\n  Skill1 Skill2 CRItem.FullCredit CRItem.PartialCredit CRItem.NoCredit\n1      H      H               0.7                  0.2             0.1"
  },
  {
    "objectID": "DiBelloModels.html#conditional-tables",
    "href": "DiBelloModels.html#conditional-tables",
    "title": "DiBello Models",
    "section": "Conditional Tables",
    "text": "Conditional Tables\nMultiple Rows:\n\nCRItem[Skill2=\"M\"] <- c(.25,.5,.25)\nCRItem[\"M\",c(\"H\",\"M\")] <-c(.25,.5,.25)\nCRItem[,\"M\"]\n\n  Skill1 Skill2 CRItem.FullCredit CRItem.PartialCredit CRItem.NoCredit\n1      H      M              0.25                  0.5            0.25\n2      M      M              0.25                  0.5            0.25\n3      L      M              0.25                  0.5            0.25\n\nCRItem[\"M\",]\n\n  Skill1 Skill2 CRItem.FullCredit CRItem.PartialCredit CRItem.NoCredit\n1      M      H      2.500000e-01         5.000000e-01    2.500000e-01\n2      M      M      2.500000e-01         5.000000e-01    2.500000e-01\n3      M      L     -3.396031e+38        -3.396031e+38   -3.396031e+38"
  },
  {
    "objectID": "DiBelloModels.html#fill-out-a-conjunctive-model.",
    "href": "DiBelloModels.html#fill-out-a-conjunctive-model.",
    "title": "DiBello Models",
    "section": "Fill out a conjunctive model.",
    "text": "Fill out a conjunctive model.\n\nCRItem[\"L\",] <- c(.1,.2,.7)\nCRItem[,\"L\"] <- c(.1,.2,.7)\nCRItem[]\n\n  Skill1 Skill2 CRItem.FullCredit CRItem.PartialCredit CRItem.NoCredit\n1      H      H              0.70                  0.2            0.10\n2      M      H              0.25                  0.5            0.25\n3      L      H              0.10                  0.2            0.70\n4      H      M              0.25                  0.5            0.25\n5      M      M              0.25                  0.5            0.25\n6      L      M              0.10                  0.2            0.70\n7      H      L              0.10                  0.2            0.70\n8      M      L              0.10                  0.2            0.70\n9      L      L              0.10                  0.2            0.70"
  },
  {
    "objectID": "DiBelloModels.html#conditional-probability-frames-and-conditional-probability-arrays",
    "href": "DiBelloModels.html#conditional-probability-frames-and-conditional-probability-arrays",
    "title": "DiBello Models",
    "section": "Conditional Probability Frames and Conditional Probability Arrays",
    "text": "Conditional Probability Frames and Conditional Probability Arrays\nWhen there are two or more parent variables, there are two possible views of the CPT:\n\nConditional Probability Frame (CPF) which is a data frame where rows represent configurations of parent variables.\n\nFirst \\(p\\) columns represent parent configuration\nLast \\(|States|\\) columns represent child states.\nNumeric part is the conditional probability table\n\nNote: calcXXXFrame() and calcXXXTable() methods in CPTtools\n\nCPFs can be used on the RHS of [<- operator for NeticaNodes, to set the CPT."
  },
  {
    "objectID": "DiBelloModels.html#array-view",
    "href": "DiBelloModels.html#array-view",
    "title": "DiBello Models",
    "section": "Array view",
    "text": "Array view\n\nConditional Probability Array (CPA) which is \\(p+1\\) dimenional array.\n\nThe functions as.CPF and as.CPA convert back and forth between the two views:\n\nas.CPA(CRItem[])\n\n, , CRItem = FullCredit\n\n      Skill2\nSkill1    H    M   L\n     H 0.70 0.25 0.1\n     M 0.25 0.25 0.1\n     L 0.10 0.10 0.1\n\n, , CRItem = PartialCredit\n\n      Skill2\nSkill1   H   M   L\n     H 0.2 0.5 0.2\n     M 0.5 0.5 0.2\n     L 0.2 0.2 0.2\n\n, , CRItem = NoCredit\n\n      Skill2\nSkill1    H    M   L\n     H 0.10 0.25 0.7\n     M 0.25 0.25 0.7\n     L 0.70 0.70 0.7\n\nattr(,\"class\")\n[1] \"CPA\"   \"array\""
  },
  {
    "objectID": "DiBelloModels.html#graphing-a-conditional-probability-table.",
    "href": "DiBelloModels.html#graphing-a-conditional-probability-table.",
    "title": "DiBello Models",
    "section": "Graphing a conditional probability table.",
    "text": "Graphing a conditional probability table.\nThe function barchart.CPF (which extends the lattice function barchart) will build a visualization of the CPF.\nThe baseCol argument can be any R color specification, it is then used as the base color for the graph."
  },
  {
    "objectID": "DiBelloModels.html#barchart",
    "href": "DiBelloModels.html#barchart",
    "title": "DiBello Models",
    "section": "Barchart",
    "text": "Barchart\n\nbarchart.CPF(CRItem[],baseCol=\"chocolate\")"
  },
  {
    "objectID": "DiBelloModels.html#how-far-have-we-come",
    "href": "DiBelloModels.html#how-far-have-we-come",
    "title": "DiBello Models",
    "section": "How far have we come?",
    "text": "How far have we come?\n\nThe [] function for NeticaNodes is a very convenient way for accessing and manipulating CPTs\nIt uses a data-frame representation, the CPF\nIf we can make a CPF, we can set the table for a node.\nThe package CPTtools is all about making CPFs!"
  },
  {
    "objectID": "DiBelloModels.html#a-short-history",
    "href": "DiBelloModels.html#a-short-history",
    "title": "DiBello Models",
    "section": "A Short History",
    "text": "A Short History\n\nWhen building CPT for Biomass, Lou DiBello had an idea.\nMap each row of the CPT onto an effective Theta\nThen use IRT model (Samejima’s Graded Response) to calculate CPTs for each row.\nFor multivariate parents, use a structure function or combination rule to combine indivitual effective thetas for each parent into a single effective theta.\n\nCompensatory: (weighted) average of parents\nConjunctive: minimum of parents\nDisjunctive: maximum of parents"
  },
  {
    "objectID": "DiBelloModels.html#the-dibello-procedure-effective-thetas",
    "href": "DiBelloModels.html#the-dibello-procedure-effective-thetas",
    "title": "DiBello Models",
    "section": "The DiBello procedure: Effective Thetas",
    "text": "The DiBello procedure: Effective Thetas\n1 Map the states of the parent variables onto the standard normal (theta) scale.\n\neffectiveThetas(2)\n\n[1]  0.6744898 -0.6744898\n\neffectiveThetas(3)\n\n[1]  0.9674216  0.0000000 -0.9674216\n\neffectiveThetas(4)\n\n[1]  1.1503494  0.3186394 -0.3186394 -1.1503494"
  },
  {
    "objectID": "DiBelloModels.html#setting-effective-thetas",
    "href": "DiBelloModels.html#setting-effective-thetas",
    "title": "DiBello Models",
    "section": "Setting Effective Thetas",
    "text": "Setting Effective Thetas\nThe function PnodeStateValues() sets the effective Thetas for a node.\nThe function ParentNodeTVals() fetches the effective Thetas for the\n\nPnodeStateValues(Skills[[1]]) <- effectiveThetas(PnodeNumStates(Skills[[1]]))"
  },
  {
    "objectID": "DiBelloModels.html#dibello-procedure-combination-rules",
    "href": "DiBelloModels.html#dibello-procedure-combination-rules",
    "title": "DiBello Models",
    "section": "DiBello Procedure: Combination Rules",
    "text": "DiBello Procedure: Combination Rules\n2 Combine the parent variables using a combination rule to create a single effective theta for each row.\n+ The combination rule generally has slope (discrimination)\nparameters ($\\alpha$'s or $a$'s)\n\n+ The combination rule generally has difficulty (intercept)\nparameters ($\\beta$'s or $b$'s)\n\n+ Some rules (e.g., `Compensatory`) have multiple-a's, some (e.g.,\n`OffsetConjuctive`) have multiple-b's\n\n+ Some link functions allow (or even require) different b's for\neach state of the child variable (step difficulties).\n\n+ The partial credit link function allows different a's as well."
  },
  {
    "objectID": "DiBelloModels.html#dibello-procedure-3",
    "href": "DiBelloModels.html#dibello-procedure-3",
    "title": "DiBello Models",
    "section": "DiBello Procedure 3:",
    "text": "DiBello Procedure 3:\n3 Convert the effective thetas to conditional probablities using a link function (IRT-like models)\n+ `gradedResponse` -- Lou's original suggestion\n+ `partialCredit` -- More flexible alternative\n+ `normalLink` -- Regression-like model for proficiency variables.\n    - Requires a _link scale parameters_\nThe function calcDPCFrame in the CPTtools package does the work. (DPC = Discrete Partial Credit)"
  },
  {
    "objectID": "DiBelloModels.html#mapping-parent-states-onto-the-theta-scale",
    "href": "DiBelloModels.html#mapping-parent-states-onto-the-theta-scale",
    "title": "DiBello Models",
    "section": "Mapping Parent States onto the Theta Scale",
    "text": "Mapping Parent States onto the Theta Scale\n\nEffective theta scale is a logit scale corresponds to mean 0 SD 1 in a “standard” population.\nWant the effective theta values to be equally spaced on this scale\nWant the marginal distribution implied by the effective thetas to be uniform (unit of the combination operator)\n\n*What the effective theta transformation to be effectively invertible (this is reason to add the 1.7 to the IRT equation)."
  },
  {
    "objectID": "DiBelloModels.html#equally-spaced-points-in-normal-measure",
    "href": "DiBelloModels.html#equally-spaced-points-in-normal-measure",
    "title": "DiBello Models",
    "section": "Equally spaced points in Normal Measure",
    "text": "Equally spaced points in Normal Measure\n\nAssume variable has \\(M\\) states: \\(0, \\ldots, M-1\\)\nRegion \\(m\\) will have lower bound \\(m/M\\) quantile and upper bound at \\((m+1)/M\\) quantile.\nMidpoint will be at \\((m+\\frac{1}{2})/M\\) quantile"
  },
  {
    "objectID": "DiBelloModels.html#setting-up-effective-thetas",
    "href": "DiBelloModels.html#setting-up-effective-thetas",
    "title": "DiBello Models",
    "section": "Setting up effective Thetas",
    "text": "Setting up effective Thetas\nThe function effectiveThetas calculates this.\n\neffectiveThetas\n\nfunction (nlevels) \n{\n    rev(qnorm((2 * (1:nlevels) - 1)/(2 * nlevels), 0, 1))\n}\n<bytecode: 0x555a2e188cf0>\n<environment: namespace:CPTtools>\n\neffectiveThetas(3)\n\n[1]  0.9674216  0.0000000 -0.9674216\n\n## We will need this for building CPTs later.\nPnodeStateValues(Skills[[1]]) <- effectiveThetas(PnodeNumStates(Skills[[1]]))\nPnodeStateValues(Skills[[2]]) <- effectiveThetas(PnodeNumStates(Skills[[2]]))"
  },
  {
    "objectID": "DiBelloModels.html#combination-rules",
    "href": "DiBelloModels.html#combination-rules",
    "title": "DiBello Models",
    "section": "Combination Rules",
    "text": "Combination Rules\n\nCompensatory – more of one skill compensates for lack of another\nConjunctive – weakest skill dominates relationship\nDisjunctive – strongest skill dominates relationship\nInhibitor – minimum threshold of Skill 1 needed, then Skill 2 takes over (special case of conjuctive) Multi-b rules:\nOffsetConjunctive – like conjunctive model, but with separate \\(b\\)’s for each parent instead of separate \\(a\\)’s\nOffset Disjunctive – like disjunctive rule, but with separate \\(b\\)’s for each parent instead of separate \\(a\\)’s."
  },
  {
    "objectID": "DiBelloModels.html#compensatory-rule",
    "href": "DiBelloModels.html#compensatory-rule",
    "title": "DiBello Models",
    "section": "Compensatory Rule",
    "text": "Compensatory Rule\n\nWeighted average of inputs\nOne \\(\\alpha\\) (slope) for each parent variable (\\(k\\)) and state (\\(s\\)): \\(\\alpha_{k,s}\\)\nOne \\(\\beta\\) (difficulty) for each state of the child variable (except the last): \\(\\beta_s\\)\n\n\\[ \\tilde\\theta = \\frac{1}{\\sqrt{K}} \\sum_k \\alpha_{k,s}\\tilde\\theta_{k,m_k} - \\beta_s\\] * Factor \\(1/\\sqrt{K}\\) is a variance stabilization term. (Make variance independent of number of parents.)"
  },
  {
    "objectID": "DiBelloModels.html#conjunctive-and-disjunctive-rules-multi-a",
    "href": "DiBelloModels.html#conjunctive-and-disjunctive-rules-multi-a",
    "title": "DiBello Models",
    "section": "Conjunctive and Disjunctive Rules (Multi-a)",
    "text": "Conjunctive and Disjunctive Rules (Multi-a)\n\nReplace sum (and square root of K) with min or max\nConjunctive: All skills needed; weakest skill dominates\n\n\\[ \\tilde\\theta = \\min_k \\alpha_{k,s}\\tilde\\theta_{k,m_k} - \\beta_s\\]\n\nDisjunctive: Any skills needed; strongest skill dominates\n\n\\[ \\tilde\\theta = \\max_k \\alpha_{k,s}\\tilde\\theta_{k,m_k} - \\beta_s\\]\n\nNot sure what the different slopes mean in this context"
  },
  {
    "objectID": "DiBelloModels.html#conjunctive-and-disjunctive-rules-multi-b",
    "href": "DiBelloModels.html#conjunctive-and-disjunctive-rules-multi-b",
    "title": "DiBello Models",
    "section": "Conjunctive and Disjunctive Rules (Multi-b)",
    "text": "Conjunctive and Disjunctive Rules (Multi-b)\n\nDifferent skills may have different demands in a task\n\nSkill 1 must be high, but Skill 2 only medium\n\nModel this with different difficulties (\\(b\\)’s) for each parent skill.\nOffsetConjunctive: All skills needed; weakest skill dominates\n\n\\[ \\tilde\\theta =  \\alpha_{s}\\min_k\\left(\\tilde\\theta_{k,m_k} - \\beta_{k,s} \\right)\\]\n\nDisjunctive: Any skills needed; strongest skill dominates\n\n\\[ \\tilde\\theta =  \\alpha_{s}\\max_k\\left(\\tilde\\theta_{k,m_k} - \\beta_{k,s} \\right)\\]"
  },
  {
    "objectID": "DiBelloModels.html#implementation-in-cpttools",
    "href": "DiBelloModels.html#implementation-in-cpttools",
    "title": "DiBello Models",
    "section": "Implementation in CPTtools",
    "text": "Implementation in CPTtools\n\nCompensatory, Conjunctive, Disjunctive, OffsetConjunctive and OffsetDisjunctive are implemented as functions in CPTtools\n\nThis set is expandable by adding new functions with the same signature\n\nThe function eThetaFrame demonstrates how this works.\nNote: Uses \\(\\log(\\alpha)\\) rather than \\(\\alpha\\) as slope parameter.\n\n\neThetaFrame(ParentStates(CRItem),log(c(Skill1=1.2,Skill2=.8)),0,\n            Compensatory)\n\n  Skill1 Skill2 Skill1.theta Skill2.theta Effective.theta\n1      H      H    0.9674216    0.9674216       1.3681407\n2      M      H    0.0000000    0.9674216       0.5472563\n3      L      H   -0.9674216    0.9674216      -0.2736281\n4      H      M    0.9674216    0.0000000       0.8208844\n5      M      M    0.0000000    0.0000000       0.0000000\n6      L      M   -0.9674216    0.0000000      -0.8208844\n7      H      L    0.9674216   -0.9674216       0.2736281\n8      M      L    0.0000000   -0.9674216      -0.5472563\n9      L      L   -0.9674216   -0.9674216      -1.3681407\n\n\nTry changing the slopes and intercepts"
  },
  {
    "objectID": "DiBelloModels.html#offset-style-rules",
    "href": "DiBelloModels.html#offset-style-rules",
    "title": "DiBello Models",
    "section": "Offset Style Rules:",
    "text": "Offset Style Rules:\nAlmost the same, except now we expect beta to be a vector instead of alpha.\n\neThetaFrame(ParentStates(CRItem),log(1),c(Skill1=.5,Skill2=-.5),\n            OffsetConjunctive)\n\n  Skill1 Skill2 Skill1.theta Skill2.theta Effective.theta\n1      H      H    0.9674216    0.9674216       0.4674216\n2      M      H    0.0000000    0.9674216      -0.5000000\n3      L      H   -0.9674216    0.9674216      -1.4674216\n4      H      M    0.9674216    0.0000000       0.4674216\n5      M      M    0.0000000    0.0000000      -0.5000000\n6      L      M   -0.9674216    0.0000000      -1.4674216\n7      H      L    0.9674216   -0.9674216      -0.4674216\n8      M      L    0.0000000   -0.9674216      -0.5000000\n9      L      L   -0.9674216   -0.9674216      -1.4674216\n\n\nTry changing the slopes and intercepts, and changing the rule for OffSetDisjunctive"
  },
  {
    "objectID": "DiBelloModels.html#link-functions",
    "href": "DiBelloModels.html#link-functions",
    "title": "DiBello Models",
    "section": "Link Functions",
    "text": "Link Functions\n\nGraded Response Model\nNormal (Regression) Model\nPartial Credit Model\n2PL (Special case of graded response or partial credit)"
  },
  {
    "objectID": "DiBelloModels.html#graded-response-model",
    "href": "DiBelloModels.html#graded-response-model",
    "title": "DiBello Models",
    "section": "Graded Response model",
    "text": "Graded Response model\n\nModels \\(\\Pr(X \\ge s)\\)\nProbabilities are differences between curves\nTo keep the curves from crossing, discrimination parameters must be the same for all \\(s\\)\n\n\nPnodeLink(node) <- \"gradedResponse\""
  },
  {
    "objectID": "DiBelloModels.html#normal-regression-model",
    "href": "DiBelloModels.html#normal-regression-model",
    "title": "DiBello Models",
    "section": "Normal (Regression) model",
    "text": "Normal (Regression) model\n\nEffective theta is mean predictor\nAdd a residual variance (link scale parameter)\nCalculate probabilities that value falls into certain regions\n\n\nPnodeLink(node) <- \"normalLink\"\nPnodeLinkScale(node) <- 1"
  },
  {
    "objectID": "DiBelloModels.html#generalized-partial-credit-model",
    "href": "DiBelloModels.html#generalized-partial-credit-model",
    "title": "DiBello Models",
    "section": "Generalized partial credit model",
    "text": "Generalized partial credit model\n\nModels state transitions: \\(\\Pr(X \\ge s | X \\ge s-1)\\)\nDoes not need the discrimination parameters to be the same\nDoes not even need the combination rules to be the same\n\n\nPnodeLink(node) <- \"partialCredit\"\nnode <- Pnode(node) ## Partial credit is the default."
  },
  {
    "objectID": "DiBelloModels.html#graded-response-dibellosamejima-models",
    "href": "DiBelloModels.html#graded-response-dibellosamejima-models",
    "title": "DiBello Models",
    "section": "Graded Response (DiBello–Samejima models)",
    "text": "Graded Response (DiBello–Samejima models)\nSamejima’s (1969) psychometric model for graded responses\n\\[\\Pr(X_{i,j} \\ge k|\\theta_i) = {\\rm logit}^{-1}(1.7(a_j\\theta_i - b_{j,k}))\\]\n\\[\\Pr(X_{i,j}=k|\\theta_i) = \\Pr(X_{i,j} \\ge k|\\theta_i) - \\Pr(X_{i,j} \\ge k+1|\\theta_i)\\]"
  },
  {
    "objectID": "DiBelloModels.html#continuous---discrete",
    "href": "DiBelloModels.html#continuous---discrete",
    "title": "DiBello Models",
    "section": "Continuous -> Discrete",
    "text": "Continuous -> Discrete\nEvaluate Samejima’s graded response model at the effective theta values.\n\n\n\n\n\n     Theta State2 State1 State0\nLow   -1.8  0.057  0.253  0.690\nMed   -0.4  0.198  0.448  0.354\nHigh   1.0  0.500  0.381  0.119"
  },
  {
    "objectID": "DiBelloModels.html#representing-graded-response-models-in-peanut",
    "href": "DiBelloModels.html#representing-graded-response-models-in-peanut",
    "title": "DiBello Models",
    "section": "Representing Graded Response Models in Peanut",
    "text": "Representing Graded Response Models in Peanut\n\nPeanut is a framework that allows us to attach the parameters to nodes in the graph.\n\nPNetica implements this for NeticaNode objects\n\nPnodeLink(node) accesses the link function\n\nShould have value “gradedResponse” for graded response models.\n\nPnodeRules(node) accesses the rules.\n\nFor now, stick to multiple-a types: Compensatory, Conjunctive and Disjunctive."
  },
  {
    "objectID": "DiBelloModels.html#setting-the-parameters",
    "href": "DiBelloModels.html#setting-the-parameters",
    "title": "DiBello Models",
    "section": "Setting the parameters",
    "text": "Setting the parameters\n\nPnodeLnAlphas(node) or PnodeAlphas(node) gives the slope parameters.\n\nThis should be a vector which components corresponding to the parents.\nIn general, vectors are used to represent multiple parents.\n\nPnodeBetas(node) gives the difficulty parameters.\n\nThis should be a list with one fewer elements than there are states (the last state is used for normalization).\nIn general, lists are used to represent multiple states."
  },
  {
    "objectID": "DiBelloModels.html#building-the-table",
    "href": "DiBelloModels.html#building-the-table",
    "title": "DiBello Models",
    "section": "Building the Table",
    "text": "Building the Table\n\nBuildTable(node) builds the table.\n\nPnodeStateValues of parents need to be set.\nPnodePriorWeight (used in learning algorithm) needs to be set.\n\n\n\nCRItem <- Pnode(CRItem) ## Force into Pnode protocol.\nPnodeLink(CRItem) <- \"gradedResponse\"\nPnodeRules(CRItem) <- \"Compensatory\"\nPnodeAlphas(CRItem) <- c(1.2,.8)\nPnodeBetas(CRItem) <- list(.25, -.25)\nPnodePriorWeight(CRItem) <- 10 ## Used for learning\ncalcDPCFrame(ParentStates(CRItem),PnodeStates(CRItem),\n             PnodeLnAlphas(CRItem),PnodeBetas(CRItem),\n             PnodeRules(CRItem),PnodeLink(CRItem))\n\n  Skill1 Skill2 FullCredit PartialCredit   NoCredit\n1      H      H 0.86998648    0.06997425 0.06003927\n2      M      H 0.62371241    0.17128816 0.20499942\n3      L      H 0.29107519    0.19888420 0.51004061\n4      H      M 0.72521985    0.13540669 0.13937347\n5      M      M 0.39532092    0.20935817 0.39532092\n6      L      M 0.13937347    0.13540669 0.72521985\n7      H      L 0.51004061    0.19888420 0.29107519\n8      M      L 0.20499942    0.17128816 0.62371241\n9      L      L 0.06003927    0.06997425 0.86998648\n\nBuildTable(CRItem)\n\n\nCRItem <- CompensatoryGadget(CRItem)\nCRItem[]\n\nhttps://pluto.coe.fsu.edu/rdemos/Peanut/CompensatoryGadget.Rmd"
  },
  {
    "objectID": "DiBelloModels.html#dont-cross-the-curves",
    "href": "DiBelloModels.html#dont-cross-the-curves",
    "title": "DiBello Models",
    "section": "Don’t Cross the curves!",
    "text": "Don’t Cross the curves!\n\nEgon Spengler: There’s something very important I forgot to tell you.\n\n\nPeter Venkman: What?\n\n\nEgon: Don’t cross the streams.\n\n\nVenkman: Why?\n\n\nEgon: It would be bad.\n\n\nVenkman: I’m fuzzy on the whole good/bad thing. What do you mean, “bad”?\n\n\nEgon: Try to imagine all life as you know it stopping instantaneously, and every molecule in your body exploding at the speed of light.\n\n\nRay Stantz: [shocked gasp] Total protonic reversal.\n\n\nVenkman: Right. That’s bad. Okay. All right. Important safety tip. Thanks, Egon.\n\n\nGhostbusters"
  },
  {
    "objectID": "DiBelloModels.html#graded-response-when-the-curves-cross",
    "href": "DiBelloModels.html#graded-response-when-the-curves-cross",
    "title": "DiBello Models",
    "section": "Graded Response when the curves cross",
    "text": "Graded Response when the curves cross\nActually, not as bad as crossing the proton beams, can produce negative probabilities.\nCPTtools corrects, but still puts restrictions on parameters.\nIn particular, must have a common discrimination for all states of the child variable to ensure curves don’t cross."
  },
  {
    "objectID": "DiBelloModels.html#downside-of-graded-response-model",
    "href": "DiBelloModels.html#downside-of-graded-response-model",
    "title": "DiBello Models",
    "section": "Downside of Graded Response Model",
    "text": "Downside of Graded Response Model\n\nNeed to keep curves from crossing restricts discrimination parameter\n\nIn Physics Playground (v. 1) for some levels difference between Silver trophy and Gold trophy had more evidence (higher discrimination) than difference between Silver and none\nAll steps must have the same parent variables and the same combination rule.\n\nModels probility of achiving certain level of performance, not step between levels.\nGeneralized Partial Credit (GPC) model does not have these downsides.\nNote Graded Response and GPC are the same when child variable has only two states."
  },
  {
    "objectID": "DiBelloModels.html#normal-regression-model-1",
    "href": "DiBelloModels.html#normal-regression-model-1",
    "title": "DiBello Models",
    "section": "Normal (Regression) Model",
    "text": "Normal (Regression) Model\n\nAs with effective theta transformation, start by dividing theta region up into equally spaced intervals\nCalculate offset curve:\n\nmean is effective theta\nSD, \\(s\\), is link scale parameter *Conditional probabilities:\narea under curve between cut points\n\n\n\n\n\n\n\n      Low    Medium      High \n0.1223318 0.3431666 0.5345016"
  },
  {
    "objectID": "DiBelloModels.html#normal-link-regression-model-features",
    "href": "DiBelloModels.html#normal-link-regression-model-features",
    "title": "DiBello Models",
    "section": "Normal Link (Regression Model) features",
    "text": "Normal Link (Regression Model) features\n\nLink function is inverse of the mapping from states to effective thetas\n\nRounding error, but no scale distortion\nGood for proficiency variables\n\nCan be used for no parent case.\nOften better to use intercept (negative difficulty) rather than difficulty.\nCan use \\(R^2\\) instead of the link scale parameter, \\(\\sigma\\)\n\n\\[ R^2 = \\frac{1/K \\sum_k \\alpha_{k,s}^2}\n{1/K \\sum_k \\alpha_{k,s}^2 +\\sigma^2} \\]\n\nNote: Latent (tetrachoric) correlations, not observed score correlations\nCan use factor analysis output to get model structure and parameters (Almond, 2010)"
  },
  {
    "objectID": "DiBelloModels.html#normal-link-no-parent-case",
    "href": "DiBelloModels.html#normal-link-no-parent-case",
    "title": "DiBello Models",
    "section": "Normal Link: No Parent case",
    "text": "Normal Link: No Parent case\n\nPnodeLink(node) is now “normalLink”\nNow need PnodeLinkScale(node), residual standard deviation (\\(\\sigma\\))\nRule doesn’t matter, use PnodeRules(node)=\"Compensatory\"\nShould be only one PnodeBeta(node)\n\n\nSkill1 <- Pnode(Skills[[1]]) ## Force into Pnode protocol.\nPnodeLink(Skill1) <- \"normalLink\"\nPnodeLinkScale(Skill1) <- .8\nPnodeRules(Skill1) <- \"Compensatory\"\nPnodeAlphas(Skill1) <- numeric()\nPnodeBetas(Skill1) <- list(.25)\nPnodePriorWeight(Skill1) <- 10 ## Used for learning\nBuildTable(Skill1)\n\nWARN [2023-04-11 10:54:12] Alpha vector for node Skill1 is empty.\n\ncalcDPCFrame(ParentStates(Skill1),PnodeStates(Skill1),\n             PnodeLnAlphas(Skill1),PnodeBetas(Skill1),\n             PnodeRules(Skill1),PnodeLink(Skill1),\n             PnodeLinkScale(Skill1))\n\n          H        M         L\n1 0.1974099 0.391954 0.4106361"
  },
  {
    "objectID": "DiBelloModels.html#regression-gadget-no-parents",
    "href": "DiBelloModels.html#regression-gadget-no-parents",
    "title": "DiBello Models",
    "section": "Regression Gadget, no parents",
    "text": "Regression Gadget, no parents\n\nSkill1 <- RegressionGadget(Skill1)\nSkill1[]\n\nhttps://pluto.coe.fsu.edu/rdemos/Peanut/EASD_EASD.Rmd"
  },
  {
    "objectID": "DiBelloModels.html#normal-link-one-parent-case",
    "href": "DiBelloModels.html#normal-link-one-parent-case",
    "title": "DiBello Models",
    "section": "Normal Link: One Parent case",
    "text": "Normal Link: One Parent case\n\nPnodeLink(node) is now “normalLink”\nNow need PnodeLinkScale(node), residual standard deviation (\\(\\sigma\\))\nWorks best with PnodeRules(node)=\"Compensatory\"\nShould be only one PnodeBeta(node)\n\n\nSkill2 <- Pnode(Skills[[2]]) ## Force into Pnode protocol.\nPnodeLink(Skill2) <- \"normalLink\"\nPnodeLinkScale(Skill2) <- .6\nPnodeRules(Skill2) <- \"Compensatory\"\nPnodeAlphas(Skill2) <- c(.8)\nPnodeBetas(Skill2) <- list(-.25)\nPnodePriorWeight(Skill2) <- 10 ## Used for learning\nBuildTable(Skill2)\ncalcDPCFrame(ParentStates(Skill2),PnodeStates(Skill2),\n             PnodeLnAlphas(Skill2),PnodeBetas(Skill2),\n             PnodeRules(Skill2),PnodeLink(Skill2),\n             PnodeLinkScale(Skill2))\n\n  Skill1          H         M           L\n1      H 0.83859093 0.1537431 0.007665989\n2      M 0.38162636 0.4900907 0.128282900\n3      L 0.05579268 0.3824800 0.561727270"
  },
  {
    "objectID": "DiBelloModels.html#regression-gadget-multiple-parents",
    "href": "DiBelloModels.html#regression-gadget-multiple-parents",
    "title": "DiBello Models",
    "section": "Regression Gadget, multiple parents",
    "text": "Regression Gadget, multiple parents\n\nSkill2 <- RegressionGadget(Skill2)\nSkill2[]\n\nhttps://pluto.coe.fsu.edu/rdemos/Peanut/RegressionGadget.Rmd"
  },
  {
    "objectID": "DiBelloModels.html#partial-credit-models",
    "href": "DiBelloModels.html#partial-credit-models",
    "title": "DiBello Models",
    "section": "Partial Credit Models",
    "text": "Partial Credit Models\n\nObservable variable takes on states \\(0, \\ldots, S\\)\nModel transition probabilities:\n\n\\[P_{s|s-1}(\\tilde{\\bf \\theta}) = \\Pr(X \\ge s | X \\ge s-1, \\tilde{\\bf \\theta}) =\n{\\rm logit}^{-1} 1.7 Z_s(\\tilde{\\bf \\theta})  \\]\n\nDefine \\(Z_0()=0\\).\n\\(Z_s()\\) can vary will \\(s\\):\n\nDifferent parameters\nDifferent functional forms\nCan easily switch between multi-a and multi-b combination rules\nCan use only a subset of the parents!\n\nNeed to define combination rule and parameters for each state (except state 0).\nPnodeLnAlphas,PnodeBetas and PnodeRules are now lists (one element per state)"
  },
  {
    "objectID": "DiBelloModels.html#partial-credit-link",
    "href": "DiBelloModels.html#partial-credit-link",
    "title": "DiBello Models",
    "section": "Partial Credit Link:",
    "text": "Partial Credit Link:\n\nProbability of \\(X\\) being in state \\(s\\) is:\n\n\\[\\Pr(X = s | \\tilde{\\bf\\theta}) = \\frac{\\prod_{r=0}^s P_{r|r-1}(\\tilde{\\bf\\theta})}{C}, \\]\nwhere \\(C\\) is a normalization constant.\n\nCan convert the products to sums\n\n\\[\\Pr(X = s | \\tilde{\\bf\\theta}) = \\frac{\\exp\\left(1.7\\sum_{r=0}^s Z_r(\\tilde{\\bf\\theta})\\right)}{\\sum_{R=0}^S \\exp\\left(1.7\\sum_{r=0}^R Z_r(\\tilde{\\bf\\theta})\\right)}\\]"
  },
  {
    "objectID": "DiBelloModels.html#simple-case-1-multiple-a-rules",
    "href": "DiBelloModels.html#simple-case-1-multiple-a-rules",
    "title": "DiBello Models",
    "section": "Simple Case 1: Multiple-A rules",
    "text": "Simple Case 1: Multiple-A rules\n\nThese look a lot like graded response\nPnodeLink(pnode) = \"partialCredit\"\nPnodeRules(pnode) is a single value\n\n“Compensatory”, “Conjunctive”, “Disjunctive”\n\nPnodeLnAlphas(pnode) is a vector corresponding to parents\nPnodeBetas(pnode) is a list corresponding to states.\n\n\\(Z_s()\\) has the same functional form and the same parameters except for \\(b\\)’s\n\nUse CompensatoryGadget to edit this style table."
  },
  {
    "objectID": "DiBelloModels.html#simple-case-example",
    "href": "DiBelloModels.html#simple-case-example",
    "title": "DiBello Models",
    "section": "Simple Case: Example",
    "text": "Simple Case: Example\n\nCRItem <- Pnode(CRItem) ## Force into Pnode protocol.\nPnodeLink(CRItem) <- \"partialCredit\"\nPnodeRules(CRItem) <- \"Compensatory\"\nPnodeAlphas(CRItem) <- c(1.2,.8)\nPnodeBetas(CRItem) <- list(.25, -.25)\nPnodePriorWeight(CRItem) <- 10 ## Used for learning\nBuildTable(CRItem)\ncalcDPCFrame(ParentStates(CRItem),PnodeStates(CRItem),\n             PnodeLnAlphas(CRItem),PnodeBetas(CRItem),\n             PnodeRules(CRItem),PnodeLink(CRItem))\n\n  Skill1 Skill2  FullCredit PartialCredit    NoCredit\n1      H      H 0.862821165     0.1289427 0.008236118\n2      M      H 0.568546469     0.3430058 0.088447725\n3      L      H 0.167478982     0.4079015 0.424619527\n4      H      M 0.694323128     0.2630736 0.042603245\n5      M      M 0.283318992     0.4333620 0.283318992\n6      L      M 0.042603245     0.2630736 0.694323128\n7      H      L 0.424619527     0.4079015 0.167478982\n8      M      L 0.088447725     0.3430058 0.568546469\n9      L      L 0.008236118     0.1289427 0.862821165"
  },
  {
    "objectID": "DiBelloModels.html#compensator-gadget",
    "href": "DiBelloModels.html#compensator-gadget",
    "title": "DiBello Models",
    "section": "Compensator Gadget",
    "text": "Compensator Gadget\n\nCRItem <- CompensatoryGadget(CRItem)\nCRItem[]\n\nhttps://pluto.coe.fsu.edu/rdemos/Peanut/CompensatoryGadget.Rmd"
  },
  {
    "objectID": "DiBelloModels.html#simple-case-2-multiple-b-rules",
    "href": "DiBelloModels.html#simple-case-2-multiple-b-rules",
    "title": "DiBello Models",
    "section": "Simple Case 2: Multiple-B rules",
    "text": "Simple Case 2: Multiple-B rules\n\nThese look a lot like multiple-a rules.\nPnodeLink(pnode) = \"partialCredit\"\nPnodeRules(pnode) is a single value\n\n“OffsetConjunctive”, “OffsetDisjunctive”\n\nPnodeLnAlphas(pnode) is a list corresponding to states\nPnodeBetas(pnode) is a vector corresponding to parent.\n\n\\(Z_s()\\) has the same functional form and the same parameters except for \\(b\\)’s\n\nUse OffsetGadget to edit this style table."
  },
  {
    "objectID": "DiBelloModels.html#case-2-example",
    "href": "DiBelloModels.html#case-2-example",
    "title": "DiBello Models",
    "section": "Case 2 example",
    "text": "Case 2 example\n\nCRItem <- Pnode(CRItem) ## Force into Pnode protocol.\nPnodeLink(CRItem) <- \"partialCredit\"\nPnodeRules(CRItem) <- \"OffsetDisjunctive\"\nPnodeAlphas(CRItem) <- list(1.2,.8)\nPnodeBetas(CRItem) <- c(.25, -.25)\nPnodePriorWeight(CRItem) <- 10 ## Used for learning\nBuildTable(CRItem)\ncalcDPCFrame(ParentStates(CRItem),PnodeStates(CRItem),\n             PnodeLnAlphas(CRItem),PnodeBetas(CRItem),\n             PnodeRules(CRItem),PnodeLink(CRItem))\n\n  Skill1 Skill2 FullCredit PartialCredit   NoCredit\n1      H      H 0.90960164     0.0759038 0.01449456\n2      M      H 0.90960164     0.0759038 0.01449456\n3      L      H 0.90960164     0.0759038 0.01449456\n4      H      M 0.75835519     0.1754952 0.06614958\n5      M      M 0.49311841     0.2961154 0.21076617\n6      L      M 0.49311841     0.2961154 0.21076617\n7      H      L 0.75835519     0.1754952 0.06614958\n8      M      L 0.19980267     0.3327296 0.46746769\n9      L      L 0.05957531     0.2574386 0.68298611"
  },
  {
    "objectID": "DiBelloModels.html#offset-gadget",
    "href": "DiBelloModels.html#offset-gadget",
    "title": "DiBello Models",
    "section": "Offset Gadget",
    "text": "Offset Gadget\n\nCRItem <- OffsetGadget(CRItem)\nCRItem[]\n\nhttps://pluto.coe.fsu.edu/rdemos/Peanut/OffsetGadget.Rmd"
  },
  {
    "objectID": "DiBelloModels.html#discrete-partial-credit-model-unleashed",
    "href": "DiBelloModels.html#discrete-partial-credit-model-unleashed",
    "title": "DiBello Models",
    "section": "Discrete Partial Credit model unleashed",
    "text": "Discrete Partial Credit model unleashed\n\nPnodeRules(pnode) is now a list\n\ndifferent rule for each state\n\nPnodeLnAlphas(node) is also a list\n\nelements correspond to states\nwill be a vector or scalar according to corresponding rule.\n\nPnodeBetas(node) is also a list\n\nelements correspond to states\nwill be a vector or scalar according to corresponding rule."
  },
  {
    "objectID": "DiBelloModels.html#peanut-convention-lists-versus-vectors",
    "href": "DiBelloModels.html#peanut-convention-lists-versus-vectors",
    "title": "DiBello Models",
    "section": "Peanut convention: Lists versus Vectors",
    "text": "Peanut convention: Lists versus Vectors\n\nA list corresponds to states of child variable (except last)\n\nIf a vector or scalar shows up where a list is expected, the same value is used for all states.\n\nA vector (within a list) corresponds to parents\nCould eliminate some parents (0 alpha, or infinite beta)\n\nOr use inner (node specific) \\(Q\\)-matrix"
  },
  {
    "objectID": "DiBelloModels.html#example-math-word-problem",
    "href": "DiBelloModels.html#example-math-word-problem",
    "title": "DiBello Models",
    "section": "Example: Math Word Problem",
    "text": "Example: Math Word Problem\n\nBased on unpublished analysis by Cocke and Guo (personal communication 2011-07-11)\nNext Generation Sunshine State Standards Benchmark, MA.6.A.5.1, “Use equivalent forms of fractions, decimals, and percents to solve problems” (NGSSS, 2013)\nSample problem:\n\n\nJohn scored 75% on a test and Mary has 8 out of 12 correct on the same test. Each test item is worth the same amount of points. Who has the better score?"
  },
  {
    "objectID": "DiBelloModels.html#scoring-rubric",
    "href": "DiBelloModels.html#scoring-rubric",
    "title": "DiBello Models",
    "section": "Scoring Rubric",
    "text": "Scoring Rubric\n\n\n\nState\nDescription\nSkills\n\n\n\n\nNo Credit\nNull response\nN/A\n\n\n———–\n————————\n——————\n\n\nPartial\nRecognizes 75% and\nMathematical\n\n\nCredit 1\n8/12 as key elements\nLanguage\n\n\n———–\n————————\n——————\n\n\nPartial\nConverts two fractions\nConvert\n\n\nCredit 2\nto a common form\nFractions\n\n\n———–\n————————\n——————\n\n\nFull\nMakes the correct\nCompare Fractions\n\n\nCredit\ncomparison\n& Math Lang.\n\n\n———–\n————————\n——————"
  },
  {
    "objectID": "DiBelloModels.html#model-refinement",
    "href": "DiBelloModels.html#model-refinement",
    "title": "DiBello Models",
    "section": "Model Refinement",
    "text": "Model Refinement\n\nCollapse “Partial Credit 2” and “Full Credit”\n\nFew “Partial Credit 2”’s in practice\n\nSkill1 = Mathematical Language\nSkill2 = Convert Fractions and Compare Fractions\n\nFraction Manipulation\n\nNeed two combination rules\n\nNo Credit -> Partial Credit. Only one skill relevant.\n\nCan use any rule (“Compensatory” is default choice)\n\nPartial Credit -> Full Credit.\n\nConjunctive model: both skills needed.\nLess of Skill1 than of Skill2"
  },
  {
    "objectID": "DiBelloModels.html#inner-q-matrix",
    "href": "DiBelloModels.html#inner-q-matrix",
    "title": "DiBello Models",
    "section": "Inner Q-matrix",
    "text": "Inner Q-matrix\n\nQ-matrix inside node:\n\nRows are state transitions\nColumns are skills (parent variables)\n\n\n\nSkill1\nSkill2\nRule\n\n\n\n\n\nPartial\n1\n0\nCompensatory\n\n\n———\n——–\n——–\n————–\n\n\nFull\n1\n1\nConjunctive\n\n\n———\n——–\n——–\n————–\n\n\n\n\nThe function PnodeQ(node) allows setting the node level Q-matrix.\nPnodeQ(node) = TRUE implies all 1’s in Q-matrix."
  },
  {
    "objectID": "DiBelloModels.html#complex-example",
    "href": "DiBelloModels.html#complex-example",
    "title": "DiBello Models",
    "section": "Complex Example",
    "text": "Complex Example\n\nNow use function DPCGadget() to edit with full model.\n\n\nCRItem <- Pnode(CRItem) ## Force into Pnode protocol.\nPnodeLink(CRItem) <- \"partialCredit\"\nPnodeRules(CRItem) <- list(\"Compensatory\",\"OffsetDisjunctive\")\nPnodeAlphas(CRItem) <- list(c(Skill1=1),1)\nPnodeBetas(CRItem) <- list(-1,c(Skill1=-1,Skill2=1))\nPnodeQ(CRItem) <- matrix(as.logical(c(1,0,1,1)),2,2,byrow = TRUE)\nPnodePriorWeight(CRItem) <- 10 ## Used for learning\ncalcDPCFrame(ParentStates(CRItem),PnodeStates(CRItem),\n             PnodeLnAlphas(CRItem),PnodeBetas(CRItem),\n             PnodeRules(CRItem),PnodeLink(CRItem),Q=PnodeQ(CRItem))\n\n  Skill1 Skill2 FullCredit PartialCredit   NoCredit\n1      H      H  0.9647686    0.03403099 0.00120040\n2      M      H  0.8223300    0.15022614 0.02744384\n3      L      H  0.3519553    0.33299278 0.31505193\n4      H      M  0.9647686    0.03403099 0.00120040\n5      M      M  0.8223300    0.15022614 0.02744384\n6      L      M  0.3519553    0.33299278 0.31505193\n7      H      L  0.9647686    0.03403099 0.00120040\n8      M      L  0.8223300    0.15022614 0.02744384\n9      L      L  0.3519553    0.33299278 0.31505193\n\nBuildTable(CRItem)"
  },
  {
    "objectID": "DiBelloModels.html#the-dpcgadget",
    "href": "DiBelloModels.html#the-dpcgadget",
    "title": "DiBello Models",
    "section": "The DPCGadget",
    "text": "The DPCGadget\n\nCRItem <- DPCGadget(CRItem)\nCRItem[]\nPnodeQ(CRItem)\n\nhttps://pluto.coe.fsu.edu/rdemos/Peanut/DPCGadget.Rmd"
  },
  {
    "objectID": "DiBelloModels.html#peanut-functions-summary",
    "href": "DiBelloModels.html#peanut-functions-summary",
    "title": "DiBello Models",
    "section": "Peanut Functions Summary",
    "text": "Peanut Functions Summary\n\nPnode() – converts a Netica node into a Peanut node\nPnodeStateValues(node) <- effectiveThetas(PnodeNumStates(node)) – Sets up parent states.\nPnodeLink(node) – Link Function\nPnodeRules(node) – (list of) Combination Rules\nPnodeLnAlphas(node), PnodeAlphas(node) – (list of) (vectors of) discriminations\nPnodeBetas(node) – (list of) (vectors of) difficulties\nPnodeLinkScale(node) – link scale parameter (for “normalLink”)\nPnodeQ(node) – Inner Q-matrix (TRUE = all 1’s)\nPnodePriorWeight(node) – prior strength for GEM algorithm learning\nBuildTable(node) – builds the table."
  },
  {
    "objectID": "DiBelloModels.html#peanut-node-gadgets",
    "href": "DiBelloModels.html#peanut-node-gadgets",
    "title": "DiBello Models",
    "section": "Peanut Node Gadgets",
    "text": "Peanut Node Gadgets\n\nCompensatoryGadget – For simple Multiple-A models\nOffsetGadget – For simple Multiple-B models\nRegressionGadget – For normal link function models (no parent case)\nDPCGadget – For complex models\n\ninner Q-matrix\ndifferent rules per row"
  },
  {
    "objectID": "leaningCPTs.html",
    "href": "leaningCPTs.html",
    "title": "Peanut Tutorial",
    "section": "",
    "text": "Bayesian Networks in Educational Assessment\nTutorial\nSession III: Bayes Net with R\nDuanli Yan, Diego Zapata, ETS\nRussell Almond, FSU\n2021 NCME Tutorial: Bayesian Networks in Educational Assessment\nSESSION TOPIC PRESENTERS\nSession 1 : Evidence Centered Design Diego Zapata Bayesian Networks\nSession 2 : Bayes Net Applications Duanli Yan & ACED: ECD in Action Russell Almond\nSession 3 : Bayes Nets with R Russell Almond & Duanli Yan\nSession 4 : Refining Bayes Nets with Duanli Yan &\nData Russell Almond\n\nLearning CPTs\n\n\nThanks to Bob Mislevy for letting me use some of the slides from his class.\n\n\nFirst Layer\nA simple model with two skills and 3 observables\n\n\nDistributions and Variables\nVariables (values are person specific)\nDistributions provide probabilities for variables\n\n\nDifferent People, Same Distributions\n\n\nSecond Layer\n\nDistributions have Parameters\nParameters are the same across all people\nParameters drop down into first layer to do person specific computations (e.g., scoring)\nProbability distributions of parameters are called Laws\n\nDistributions have Parameters\nParameters are the same across all people\nParameters drop down into first layer to do person specific computations (e.g., scoring)\nProbability distributions of parameters are called Laws\n\n\n\n\nSpeigelhalter And Lauritzen (1990)\nGlobal Parameter Independence – parameters of laws for different CPTs are independent\nLocal Parameter Independence – parameters for laws for different rows of CPTs are independent\nUnder these two assumptions, the natural conjugate law of a Bayesian network is a hyper- Dirichlet law , a law where the probabilities on each row of each CPT follow aDirichletlaw.\nAbusing the definition, we say that a CPT for which each rows is given an independentDirichletlaw follows a hyper- Dirichlet distribution ( really should be law).\n\n\nA closer look at the binomial distribution\nBinomial. For counts of successes in binary trials, each with probability p, in n independent trials. E.g., n coin flips, with p the common probability of heads.\nCount of successes\nCount of failures\nThe “success probability” parameter\nThe failure probability\nThe success probability\nWe will be using this as a likelihood in an example of the use of conjugate distributions.\n\n\nA closer look at the Beta distribution\nBeta. Defined on [0,1]. Conjugate prior for the probability parameter in Bernoulli & binomial models.\np ~ dbeta(a,b)\nMean(p):\nVariance(p):\nMode(p):\nPseudoCount\nof successes\nPseudoCount\nof failures\nThe variable:\n“success probability”\nThe failure\nprobability\nShape, or\n“prior sample info”\nThe success\nprobability\n\n\nAn example with a continuous variable: A beta-binomial example–the Prior Distribution\n\nThe prior distribution:\nLet’s suppose we think it is more likely that the coin is close to fair, sopis probably nearer to .5 than it is to either 0 or 1. We don’t have any reason to think it is biased toward either heads or tails, so we’ll want a prior distribution that is symmetric around .5. We’re not real sure about whatpmight be--say about as sure as only 6 observations. This corresponds to 3 pseudo-counts of H and 3 of T, which, if we want to use a beta distribution to express this belief, corresponds to beta(4,4):\n\nBeta. Defined on [0,1]. Conjugate prior for the probability parameter in Bernoulli & binomial models.\np~ dbeta(4,4)\nMean(p):\nVariance(p):\nMode(p):\nPseudoCount\nof successes\nPseudoCount\nof failures\nThe variable:\n“success probability”\nThe failure\nprobability\nShape, or\n“prior sample info”\nThe success\nprobability\n\n\nAn example with a continuous variable: A beta-binomial example–the Likelihood\n\nThe likelihood:\nNext we will flip the coin ten times. Assuming the same true (but unknown to us) value ofpis in effect for each of ten independent trials, we can use the binomial distribution to model the probability of getting any number of heads: i.e.,\n\nCount of observed successes\nCount of observed\nfailures\nThe “success probability” parameter\nThe failure probability\nThe success probability\n\nThe likelihood:\nWe flip the coin ten times, and observe 7 heads; i.e., r=7. The likelihood is obtained now using the same form as in the preceding slide, except now r is fixed at 7 and we are interested in the relative value of this function at different possible values ofp:\n\n\n\nAn example with a continuous variable: Obtaining the posterior by Bayes Theorem\nposterior likelihood prior\nGeneral form:\nIn our example, 7 plays the role of x*, and p plays the role of y. Before normalizing:\nThis function is proportional to a beta(11,7) distribution.\nAfter normalizing:\nNow, how can we get an idea of what this means we believe aboutpafter combining our prior belief and our observations?\n\n\nAn example with a continuous variable: In pictures\nPrior\nx\nLikelihood\nPosterior\n\n\nDirichlet—Categorical conjugate distribution\nAssume a variable X takes on category 1,…,K with probabilities p 1 ,… p K\nTake N draws from this distribution and observe counts N=X 1 + … + X K\nLikelihood is\nDirichletPrior:\nPosterior:\n\n\n\n\n\nUpdating an unconditional probability table (no parent variables)\nPrior is a table of alphas:\nSum of alphas is pseudo-sample size for prior:Neticacalls this Node Experience\nSufficient statistic is a table of counts in each category\nPosterior is an updated table\nWith updated Node Experience A’=A+N\n\n\n\nDetails\nEquivalent to beta-binomial when variable only takes two values\nAlphas must be positive, but don’t need to be integers\nAlpha = ½ is non-informative prior\nA (sum of alphas) acts like a pseudo-sample size for the prior\nCan also write as\n\n\n\nCPT updating when parents are fully observed\nData are contingency table of child variable given parents\nPrior is a table of pseudo-counts\nGet posterior by adding them together\nNote: Both prior and posterior effective sample size (Node Experience) can be different for each row.\n\n\n\nNetica example – fully observed\n\n2011 NCME Tutorial: Bayesian Networks in Educational Assessment - Session IV\n\n\nRNetica example (Ex 8.3)\nSet up network\nTwo parents, one child\nhdnet<-CreateNetwork(“hyperDirchlet”)\nskills <-NewDiscreteNode(hdnet,c(“Skill1”,“Skill2”),c(“High”,“Medium”,“Low”))\nobs<-NewDiscreteNode(hdnet,“Observable”,c(“Right”,“Wrong”))\nNodeParents(obs) <- skills\n\n\nSet up prior for Observation\nDo this by setting CPT andNodeExperience(row pseudo-sample sizes)\nptab<-data.frame(Skill1=rep(c(“High”,“Medium”,“Low”),3), Skill2=rep(c(“High”,“Medium”,“Low”),each=3), Right=c(.975,.875,.5,.875,.5,.125,.5,.125,.025),\nWrong=1-c(.975,.875,.5,.875,.5,.125,.5,.125,.025))\nobs[] <-ptab\nNodeExperience(obs) <- 10 #All rows equally weighted\n\n\nPrior CPT\nptab\nrescaleTable(ptab,10)\nSkill1 Skill2 Right Wrong\n1 HighHigh0.975 0.025\n2 Medium High 0.875 0.125\n3 Low High 0.500 0.500\n4 High Medium 0.875 0.125\n5 MediumMedium0.500 0.500\n6 Low Medium 0.125 0.875\n7 High Low 0.500 0.500\n8 Medium Low 0.125 0.875\n9 LowLow0.025 0.975\nSkill1 Skill2 Right Wrong\n1 HighHigh9.75 0.25\n2 Medium High 8.75 1.25\n3 Low High 5.00 5.00\n4 High Medium 8.75 1.25\n5 MediumMedium5.00 5.00\n6 Low Medium 1.25 8.75\n7 High Low 5.00 5.00\n8 Medium Low 1.25 8.75\n9 LowLow0.25 9.75\n\n\nNetica Case files\nText file, column separated by tabs (same as .xlsfiles, but have .casextension)\nOne column for each observed variable (need both parents and child in this case)\nOptionalIDnumcolumn\nOptionalNumCasescolumn gives replication count\nSo can either repeat out cases, or use summary counts.\nwrite.CaseFile() writes out a case file for use withNetica\n\n\nCase Table for Ex 8.3\ndtab<-data.frame(Skill1=rep(c(“High”,“Medium”,“Low”),3,each=2),\nSkill2=rep(c(“High”,“Medium”,“Low”),each=6),\nObservable=rep(c(“Right”,“Wrong”),9),\nNumCases=c(293,3,\n112,16,\n0,1,\n14,1,\n92,55,\n4,5,\n5,1,\n62,156,\n8,172))\nwrite.CaseFile(dtab,“Ex8.3.cas”)\n\n\nExample Case File\nSkill1 Skill2 ObservableNumCases\n1 HighHighRight 293\n2 HighHighWrong 3\n3 Medium High Right 112\n4 Medium High Wrong 16\n5 Low High Right 0\n6 Low High Wrong 1\n7 High Medium Right 14\n8 High Medium Wrong 1\n9 MediumMediumRight 92\n10 MediumMediumWrong 55\n11 Low Medium Right 4\n12 Low Medium Wrong 5\n13 High Low Right 5\n14 High Low Wrong 1\n15 Medium Low Right 62\n16 Medium Low Wrong 156\n17 LowLowRight 8\n18 LowLowWrong 172\n\n\nLearn CPTs\nLearnCasesdoes complete data hyper-Dirichletupdating\nLearnCases(“Ex8.3.cas”,obs)\nNodeExperience(obs)\nSkill2\nSkill1 High Medium Low\nHigh 306 25 16\nMedium 138 157 228\nLow 11 19 190\n\n\nPrior and Posterior CPTs\nPrior\nPosterior\nSkill1 Skill2 Right Wrong\n1 HighHigh0.975 0.025\n2 Medium High 0.875 0.125\n3 Low High 0.500 0.500\n4 High Medium 0.875 0.125\n5 MediumMedium0.500 0.500\n6 Low Medium 0.125 0.875\n7 High Low 0.500 0.500\n8 Medium Low 0.125 0.875\n9 LowLow0.025 0.975\nSkill1 Skill2 Right Wrong\n1 HighHigh0.989 0.011\n2 Medium High 0.848 0.152\n3 Low High 0.795 0.205\n4 High Medium 0.760 0.240\n5 MediumMedium0.588 0.412\n6 Low Medium 0.276 0.724\n7 High Low 0.859 0.141\n8 Medium Low 0.277 0.723\n9 LowLow0.068 0.932\n\n\nPrior and Posterior Alphas\nPrior\nPosterior\nSkill1 Skill2 Right Wrong\n1 HighHigh9.75 0.25\n2 Medium High 8.75 1.25\n3 Low High 5.00 5.00\n4 High Medium 8.75 1.25\n5 MediumMedium5.00 5.00\n6 Low Medium 1.25 8.75\n7 High Low 5.00 5.00\n8 Medium Low 1.25 8.75\n9 LowLow0.25 9.75\nSkill1 Skill2 Right Wrong\n1 HighHigh302.75 3.25\n2 Medium High 117.00 21.00\n3 Low High 8.75 2.25\n4 High Medium 19.00 6.00\n5 MediumMedium92.25 64.75\n6 Low Medium 5.25 13.75\n7 High Low 13.75 2.25\n8 Medium Low 63.25 164.75\n9 LowLow13.00 177.00\n\n\nProblems with hyper-Dirichlet approach\n\nLearn more about some rows than others\nLocal parameter independence assumption is unrealistic – often want CPT to be monotonic (increasing skill means increasing chance of success)\n\nl 2,2 > l 2,1 > l 1,1 and l 2,2 > l 1,2 > l 1,1\n\nSolution is to use parametric models for CPT:\n\nNoisy-min & Noisy-max\nDiBello-Samejimafamilies\nDiscrete Partial Credit families\n\n\n\n\nLearning CPTs for a parametric family\n\nContingency table is sufficient statistic for law for any CPT!\nPick value of law parameters that maximize the posterior probability (or likelihood) of the observed contingency table.\nFully Bayesian method\n\nPut hyper-laws over lawhyperparameters\nCalculate observed contingency table\nMAP estimates maximize posterior probability of contingency table\n\nSemi-Bayesian method\n\nUse priorhyperparametersto calculate prior table.\nEstablish a pseudo-sample size for each row and calculate prior alphas\nDo hyper-Dirichletupdating to get posterior alphas\nMAP estimates maximize posterior probability of posterior alphas (treating them as if they were data)\nCPTtoolsfunctionmapCPTdoes this\n\n\n\n\nNewton-Raphson Method\n\nOriginally for finding zeros of a function, but if we apply it to the first derivative, then it finds local maxima and minima of the function\nEach step moves closer to the maximum\nMay be multiple maxima\n\nMultiple starting points\nSimulated annealing & other modifications\n\n\n\n\nAnimation\nhttp://en.wikipedia.org/wiki/File:NewtonIteration_Ani.gif\n\n\n\nGradient Decent\nhttp://vis.supstat.com/2013/03/gradient-descent-algorithm-with-r/\n\n\nMissing and Latent Variables\n\nMissing completely at random (MCAR) – whether or not Y i is missing is independent of both Y i and any observed covariate X i\n\nCasewisedeletion provides an unbiased estimate only in this case!\n\nMissing at Random (MAR) – whether or not Y i is missing is independent of Y i given observed covariates X i\n\nEM algorithm & MCMC work here\nLatent variables are a special case\n\nNon-ignorable missingness – Not MAR\n\n\n\nMCAR, MAR or Non-ignorable (Ex 9.2)\nA survey of high school seniors asks the school administrator to provide grade point average and college entrance exam scores. College entrance exam scores are missing for students who have not taken the test.\nSame survey except now survey additionally asks whether or not student has declared an intent to apply for college.\nTo reduce the burden on thestudents fillingout the survey, the background questions are divided into several sections, and each student is assigned only some of the sections using a spiral pattern. Responses on the unassigned section are missing.\nSome students when asked their race decline to answer.\n1. John did not answer Task j because it was not on the test form he was administered.\n2.Diwakardid not answer Task j because there are linked harder and easier test forms, intended for fourth and sixth grade students; Task j is an easy item that only appears on the fourth grade form; andDiwakaris in sixth grade, so he was administered the hard form.\n3. Rodrigo took an adaptive test. He did well, the items he was administered tended to be harder as he went along, and Task j was not selected to administer because his responses suggested it was too easy to provide much information about his proficiency.\n4. Task j was near the end of the test, and Ting did not answer it because she ran out of time.\n5.Shahrukhlooked at Task j and decided not to answer it because she thought she would probably not do well on it.\n6. Howard was instructed to examine four items and choose two of them to answer. Task j was one of the four, and not one that Howard chose.\n\n\nEM Algorithm (Dempster, Laird & Rubin, 1977)\nKey idea:\nPick a set of value for parameters\nE-step (a): Calculate distribution for missing variables given observed variables & current parameter values.\nE-step (b): Calculate expected value of sufficient statistics\nM-step: Use Gradient Decent to produce MAP/MLE estimates for parameters given sufficient statistics\nLoop 2—4 until convergence\n\n\nEM algorithm details\n\nOnly need to take a few steps of the gradient algorithm in Step 4 (Generalized EM)\nCan exploit conditional independence conditions, particularly global parameter independence (StructualEM,Mengand van Dyke)\n\nOnce CPT at a time\n\nCan be slow\n\nBut not as slow as MCMC\n\nNeticaprovides built-in support for special case of hyper-Dirichletlaw\n\n\n\nExpected value of missing (latent) node\nCan calculate this using ordinaryNeticaoperations (instantiate all observed variables and read off joint beliefs)\nInstead of adding count to the table, add fractional count to the table\nSimilarly use joint beliefs when more than one parent is missing\n\n\nExample\n\nObservable X in {0, 1}; Latent q in {H,M,L}\nObservations:\n\nX=1; p( q ) = H:.33, M:.33, L:.33\nX=1; p( q ) = H:.5, M:.33, L:.2\nX=0; p( q ) = H:.2, M:.3, L:.5\n\nExpected table:\n\n\n\nEM for hyper-Dirichlet (RNetica LearnCPTs function)\nUse current CPTs to calculate expected tables for all of the CPTs we are learning\nUse the hyper-Dirichletconjugate updating to update the CPTs\nLoop 1 and 2 until convergence\nNote: RNetica LearnCPT function currently does not reveal whether or not convergence was reached.\n\n\nNetica example – partially observed\n\n2011 NCME Tutorial: Bayesian Networks in Educational Assessment - Session IV\n\n\nParameterized tables\nUse current parameters to set initial CPTs\nUseNetica’sLearnCPTsto calculate posterior tables\nMultiple posterior tables by node experience to get pseudo-table for each CPT\nUse gradient decent to optimize CPT parameters\nLoop 1—4 until convergence\nI’m currently working on an implementation in R.\n\n\nBreakdown of global parameter independence\nEven if parameters are a priori independent, when there is missing (or latent) data then parameters are not independent a posteriori .\nEM algorithm only gives point estimate, does not capture this dependence\nThere might also be other information which makes parameters dependent.\n\n\n\nMarkov Chain Monte Carlo (MCMC)\nIn place of E-step, randomly sample values for unknown (latent & missing) variables\nIn place of M-step, randomly sample values for parameters\nTakes longer than EM, but gives you an impression of the whole distributionrather than just a part."
  },
  {
    "objectID": "PeanutVisuals.html",
    "href": "PeanutVisuals.html",
    "title": "Interactive Visualizations of the Relationships between Skills and Observations Using Peanut",
    "section": "",
    "text": "Cognitively diagnostic models (CDMs) contain factors which describes the relationship between variables describing examinee skills and the observable outcome variables from tasks used to assess those skills. For discrete variables, this factor can be expressed as a conditional probability table (CPT; the term comes from Bayesian networks, but applicable to any discrete CDM). This paper introduces a visualization of the CPT where each conditional distribution is represented with a split bar. There is one bar for each possible combinations of skills. The visualization is interactive: the view changes as the model parameters change. This tool should be useful for teaching the differences between conjunctive, disjunctive and compensatory models as well as for eliciting prior opinion about model parameters from experts."
  },
  {
    "objectID": "PeanutVisuals.html#introduction",
    "href": "PeanutVisuals.html#introduction",
    "title": "Interactive Visualizations of the Relationships between Skills and Observations Using Peanut",
    "section": "Introduction:",
    "text": "Introduction:\nBayesian networks (Almond, Mislevy, Steinberg, Yan and Williamson, 2015) are a mathematical notation for representing cognitively diagnostic models (CDMs). The distribution for discrete Bayesian networks associates a conditional probability table (CPT) with each node (variable) in the network. Configurations of the parent variables define the rows of the table and the columns correspond to the possible states of the target variable. Each row is a (conditional) probability distribution, whose values sum to one. There is a close correspondence between the graphical structure of the Bayes net and the Q-matrix (Almond, 2010); ones in the Q-matrix map to the arrows in the directed graph."
  },
  {
    "objectID": "PeanutVisuals.html#dibello-models",
    "href": "PeanutVisuals.html#dibello-models",
    "title": "Interactive Visualizations of the Relationships between Skills and Observations Using Peanut",
    "section": "DiBello Models:",
    "text": "DiBello Models:\nLou DiBello (Almond, et al, 2001) devised a three-step procedure to parameterize conditional probability tables for a target variable. This procedure (a) associates an effective theta (a real value) to each possible level of each parent (skill) variable, (b) defines a structure function that projects the parent effective thetas into a linear scale for the target variable, and (c) a link function that maps the projected effective theta onto a conditional probability distribution over the target. Different forms for the structure functions yield different kinds of relationships. Functions based on sums produce compensatory models, minimums produce conjunctive models, and maximums produce disjunctive models."
  },
  {
    "objectID": "PeanutVisuals.html#visualization",
    "href": "PeanutVisuals.html#visualization",
    "title": "Interactive Visualizations of the Relationships between Skills and Observations Using Peanut",
    "section": "Visualization:",
    "text": "Visualization:\nThe CPT is an array of probability distributions with each cell corresponding to a configuration of the parent variables. The distributions are represented with a split bar using a color intensity scale. The bottom half of Figure 1 shows an example. Note that Figure 1 is a tabbed: the viewer can switch between a tabular view of the probabilities or the effective thetas, or the graphical representation.\nFigure 1 is an R shiny (Chang, Cheng, Allaire, Xie and McPherson, 2018) gadget. The sliders represent model parameters and the drop down menus select structure and link functions. Manipulating these controls produces immediate feedback in the graph and tables below.\nFigure 1 represents a version of the gadget that uses the compensatory structure function. In this version of the gadget, there is one slope (alpha) per parent variable and one beta (difficulty, negative intercept) per state of the child variable.\nFigure 2 represents an alternative view for conjunctive and disjunctive models. In this representation, there is a different beta for each parent and a different alpha for each level.\nFigure 3 represents and alternative link function, one based on the normal distribution. This link function has an extra scale parameter. With the compensatory structure function, this produces a discrete regression. This can also be used with no parent variables (Figure 4).\nFigures 5a and 5b show the most complex case. The use of the partial credit link function allows the use of a different structure function and a different set of parent variables for each transition. These are represented in separate tabs. This allows construction of models using an inner Q-matrix (Almond, 2018).\nThe software for these visualizations are available from https://pluto.coe.fsu.edu/RNetica/Peanut.html or https://github.com/ralmond/Peanut."
  },
  {
    "objectID": "PeanutVisuals.html#references",
    "href": "PeanutVisuals.html#references",
    "title": "Interactive Visualizations of the Relationships between Skills and Observations Using Peanut",
    "section": "References:",
    "text": "References:\nAlmond, R. G. (2010). `I can name that Bayesian Network in Two Matrixes.’ International Journal of Approximate Reasoning, 51, 167-178.\nAlmond, R. G. (2015). An IRT-based Parameterization for Conditional Probability Tables. In Agosta, J. M. & Carvalho, R. N. (Eds.). Proceedings of the Twelfth UAI Bayesian Modeling Applications Workshop (BMAW 2015), CEUR Workshop Proceedings 1565, 14-23. (URL http://ceur-ws.org/Vol-1565/bmaw2015_paper4.pdf)\nAlmond, R. G. (2018). Inner and Outer Q-matrixes for Multi-step Problems. Presentation at the Annual Meeting of the American Educational Research Association, Cognition and Assessment SIG, Toronto, Ontario, Canada.\nAlmond, R. G., Dibello, L. V., Jenkins, F., Mislevy, R. J., Senturk, D., Steinberg, L. S, & Yan, D. (2001). Models for Conditional Probability Tables in Educational Assessment. In Jaakkola, T. & Richardson, T. (Eds.). Artificial Intelligence and Statistics 2001, Morgan Kaufmann, 137-143.\nAlmond, R. G., Mislevy, R. J., Steinberg, L. S., Yan, D. & Williamson, D. M. (2015). Bayesian Networks in Educational Assessment. Springer.\nChang, W., Cheng, J., Alaire J. J., Xie, Y & McPherson, J. (2018). shiny: Web Application Framework for R. R package version 1.1.0. (URL https://CRAN.R-project.org/package=shiny.)"
  },
  {
    "objectID": "Building.html",
    "href": "Building.html",
    "title": "Building Networks Models with Expert Opinion",
    "section": "",
    "text": "First, the Peanut libraries need to be loaded, along with an implementation. As PNetica is the currently available implementation, that will be used in this example. The Resources section shows how to install this.\nAfter some experience using various graphical tools, -(matrix2?) found that the easiest way to work with experts in eliciting a Bayesian network structure is to use a spreadsheet. The spreadsheet for the miniACED example is available through Google Sheets\nNote that the format here is a bit of a work in progress. I am looking for ways to simplify the presentation, in particular, collapsing over lines, so look for htat in the future.\nAs the example network data is stored in a Google sheet, we will also need the googlesheets4 library. The knitr package provides table formatting.\nWith Google sheets, Google needs to verify that your computer is authorized to read the sheet. Fortunately, the example sheet is readable by the public. So we will call gs4_deauth() to make sure that Google uses the public interface. (See the googlesheets4 documentation for more information about OAuth support.) The sheet ID is the long string of digits and numbers after spreadsheets/d/ in the URI for the sheet."
  },
  {
    "objectID": "Building.html#defining-networks",
    "href": "Building.html#defining-networks",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Defining Networks",
    "text": "Defining Networks\nNote that this sheet has five tabs which the analyst must complete to specify the model.\n\nNets—describes the competency and evidence models.\nNodes—describes the nodes and their states.\nQ—describes the distribution of variables representing observable indicators that appear in the evidence models.\nOmega—describes the relationship and distribution of the competency model variables.\nStatistics—describes what values will be written out.\n\nNote that there is a extra tab Dropdowns. This is used as part of the network validation, and doesn’t need to be filled out by the analyst."
  },
  {
    "objectID": "Building.html#nets",
    "href": "Building.html#nets",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Nets",
    "text": "Nets\nThe first spreadsheet is the network manifest; which basically lists all of the nets.\n\nknitr::kable(netman)\n\n\n\nTable 1: Nets Tab\n\n\n\n\n\n\n\n\n\nName\nTitle\nHub\nPathname\nDescription\n\n\n\n\nminiACEDPM\nACED subset\nNA\nminiACEDPM.dne\nA subset of the full ACED model\n\n\nCommonRatioEasyEM\nCommon Ratio (Easy variant)\nminiACEDPM\nCommonRatioEasyEM.dne\nAn easy task using the common ratio skill\n\n\nCommonRatioMedEM\nCommon Ratio (Medium variant)\nminiACEDPM\nCommonRatioMedEM.dne\nA moderate task using the common ratio skill\n\n\nCommonRatioHardEM\nCommon Ratio (Hard variant)\nminiACEDPM\nCommonRatioHardEM.dne\nA hard task using the common ratio skill\n\n\nExamplesEasyEM\nExamples (Easy variant)\nminiACEDPM\nExamplesEasyEM.dne\nAn easy task using the examples skill\n\n\nExamplesMedEM\nExamples (Medium variant)\nminiACEDPM\nExamplesMedEM.dne\nA moderate task using the examples skill\n\n\nExamplesHardEM\nExamples (Hard variant)\nminiACEDPM\nExamplesHardEM.dne\nA hard task using the examples skill\n\n\nExtendEasyEM\nExtend (Easy variant)\nminiACEDPM\nExtendEasyEM.dne\nAn easy task using the Extend skill\n\n\nExtendMedEM\nExtend (Medium variant)\nminiACEDPM\nExtendMedEM.dne\nA moderate task using the Extend skill\n\n\nExtendHardEM\nExtend (Hard variant)\nminiACEDPM\nExtendHardEM.dne\nA hard task using the Extend skill\n\n\nTableExtendEasyEM\nTableExtend (Easy variant)\nminiACEDPM\nTableExtendEasyEM.dne\nAn easy task using both the table and extend skills.\n\n\nTableExtendMedEM\nTableExtend (Medium variant)\nminiACEDPM\nTableExtendMedEM.dne\nA moderate task using both the table and extend skills.\n\n\nTableExtendHardEM\nTableExtend (Hard variant)\nminiACEDPM\nTableExtendHardEM.dne\nA hard task using both the table and extend skills.\n\n\nModelTableExtendEasyEM\nModelTableExtend (Easy variant)\nminiACEDPM\nModelTableExtendEasyEM.dne\nAn easy task using the model, table, and extend skills.\n\n\nModelTableExtendMedEM\nModelTableExtend (Medium variant)\nminiACEDPM\nModelTableExtendMedEM.dne\nA moderate task using the model, table, and extend skills.\n\n\nModelTableExtendHardEM\nModelTableExtend (Hard variant)\nminiACEDPM\nModelTableExtendHardEM.dne\nA hard task using the model, table, and extend skills.\n\n\n\n\n\n\nThis table is the easiest to understand. Each row represents a Bayes net model, either a comptency model or an evidence model. The columns have the following meaning.\n\nName — the identifier of the network. Note that projects using Netica will need to follow the Netica syntax for identifiers, which is the identifier, must start with a letter and may contain only letters, numbers or underscores (_). It is also limited to 32 characters.\nTitle — A longer version of the name, with no character restrictions.\nHub — For evidence models, this is the name of the corresponding competency model; for competency models, leave if blank.\nPathname — The name of the file for storing the network. This defaults to the name plus the .dne extension, which is Netica’s extension for the text representation of a Bayes net.\nDescription — A place for user comments.\n\nThe Peanut package works by creating a Warehouse for the networks. The idea is that if the network has already been created, then the Net Warehouse will go ahead and build it using the specifications in the manifest; which is the netman file.\nNote here the use of the PNetica package and the RNetica::Session object, sess. This is one of two places in which the explicit dependence on Netica is required. (The other is the creation of the Node Warehouse in the next session). The rest of the commands use function calls in the Peanut or CPTtools frameworks, which are agnostic as to the Bayes net engine used (although so far, only the PNetica implementation exists)."
  },
  {
    "objectID": "Building.html#nodes",
    "href": "Building.html#nodes",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Nodes",
    "text": "Nodes\nThe next spreadsheet defines the nodes. There are two things which are important to note about the nodes. The first is that each network defines its own namespace. Thus, the node isCorrect in the CommonRatioEasyEM and in the ExamplesGeometricEasyEM are different. Thus, the variable is identified by the first two columns in the spreadsheet.\nThe second thing of note is that there are some information which is specific to the possible states of the node. Thus, if the node has 3 possible states, then there are three rows for each node.\n[Note: This should change in a future version. The idea is to instead use a a vector notation, i.e., [state1,state2,state3], to express multiple states.]\n\nknitr::kable(nodeman)\n\n\n\nTable 2: Nodes Tab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nNodeName\nModelHub\nNodeTitle\nNodeDescription\nNodeLabels\nContinuous\nNstates\nStateName\nStateTitle\nStateDescription\nStateValue\nLowerBound\nUpperBound\n\n\n\n\nminiACEDPM\nSolveGeometricProblems\nNA\nSolve Geometric Problems\nCombination of all Geometric Problem Skills\nProficiencies,pnodes,Highlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nSolveGeometricProblems\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nSolveGeometricProblems\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nminiACEDPM\nCommonRatio\nNA\nCommon Ratio\nSolve problems involving finding the common ratio\nProficiencies,pnodes,Lowlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nCommonRatio\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nCommonRatio\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nminiACEDPM\nExamplesGeometric\nNA\nExamples of Geometric Sequeces\nSolve problems involving examples of Geometric sequences\nProficiencies,pnodes,Lowlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nExamplesGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nExamplesGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nminiACEDPM\nExtendGeometric\nNA\nExtending Geometric Sequeces\nSolve problems involving extending Geometric sequences\nProficiencies,pnodes,Lowlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nExtendGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nExtendGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nminiACEDPM\nTableGeometric\nNA\nExtending Geometric Sequeces in Tables\nSolve problems involving extending Geometric sequences in tabular form\nProficiencies,pnodes,Lowlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nTableGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nTableGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nminiACEDPM\nModelGeometric\nNA\nExtending Geometric Sequeces in Tables using Models\nSolve problems involving extending Geometric sequences in tabular form with models\nProficiencies,pnodes,Lowlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nModelGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nModelGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nCommonRatioEasyEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nCommonRatioEasyEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nCommonRatioMedEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nCommonRatioMedEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nCommonRatioHardEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nCommonRatioHardEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesEasyEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesEasyEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesMedEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesMedEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesHardEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesHardEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExtendEasyEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExtendEasyEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExtendMedEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExtendMedEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExtendHardEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExtendHardEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendEasyEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendEasyEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendMedEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendMedEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendHardEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendHardEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendEasyEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendEasyEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendMedEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendMedEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendHardEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendHardEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nNode Primary Key\nThe following columns are present in the spreadsheet:\n\nModel — The name of the network, this should appear in the Name column of the Nets sheet.\nNodeName — The name of the node. This should correspond to the Netica naming rules.\n\nNote that the (Model, NodeName) ordered pair is the primary key for the table.\n\n\nNode-level properties\nThe following values are assigned at the node level.\n\nModelHub — The name of the proficiency/competency model. (This information is redundant with that in the Nets sheet.\nNodeTitle — A longer name for the node which does not need to conform to variable naming rules.\nNodeDescription — Documentation for the node.\nContinuous — A logical value. If FALSE, then the node has a number of discrete values. If TRUE, then the node is a discritized version of an underlying continuous value.\nNstates — The number of states of the node. If the node is discrete (Continuous=FALSE), then this is the number of possible values. If the node is continuous, then this is the number of states the node will be partitioned into.\n\nNote that most of the state level value must be vectors of the same length as Nstates for that row.\n\n\nState-level descriptors\n\nStateName — An identifier for the state. Must follow variable rules.\nStateTitle — A longer descriptor which doesn’t need to follow varaible rules.\nStateDescription — Documentation for the state.\nStateValue — This is used in both the CPT construction algorithms and in calculating expected values and varainces for nodes. The best practice here is to map these values to quantiles of the normal distribution ((bninea?)). Note that for three levels these are approximately 1, 0 and -1.\nLowerBound, UpperBound [Depricated, next version will use Cuts]. These are lists of upper and lower bounds for an underlying continuous variable. Note that -Inf and Inf are acceptable values. Note also that the upper bound for one state must match the lower bound for the next.\nCuts — Only used for continuous nodes. These are the upper and lower cuts for the underlying continuous variable. There should be one more value in the vector than there are states in the node. The continuous variable is chopped into intervals with the given cut points (including the minimum and maximum). Note that -Inf and Inf are acceptable values.\n\n\n\nNode Warehouse\nPeanut uses the same Warehouse metaphor to build the nodes. The Nodes spreadsheet is the instruction for building the node (if it is not already built).\nThis is the last place in which we explicitly reference Netica. The WarehouseSupply method on the Net or Node Warehouse returns an object of the abstract type Peanut::Pnet or Peanut::Pnode, which then supports all of the other operations.\nThe two warehouses give us the basic definitions of the networks and the nodes. The next step is to describe the relationships among the nodes and the conditional probability tables that are defined.\n(matrix2?) divides this into two pieces: the \\(Q\\)-matrix which describes the relationship between the competency nodes and the observable outcomes(Section 6), and the \\(\\Omega\\) matrix which describes the relationship among the competency variables (Section Section 5). ?@sec-growth)."
  },
  {
    "objectID": "Building.html#sec-Omega",
    "href": "Building.html#sec-Omega",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Omega",
    "text": "Omega\nThe Omega (\\(\\Omega\\)) matrix is named for the inverse of the covariance matrix. Is is based on the observation that zero in the inverse covariance matrix leads to a conditional independence assumption (and hence lack of edge) in the graphical model (dempster1972?; whittaker1990?). However, rather than specify the covariance matrix, the approach used in Peanut is to specify the joint distribution of the variable in the competency (proficiency) model through a series of regressions.\n\nOmega Matrix Structure\nThe first part of the model is specifying which variables should be predictors (inputs) to the others. In the \\(\\Omega\\)-matrix (Figure 1), the rows represent the variables in their roles as dependent (output) variables and the columns the role as input variables. A check mark indicates that the column variable is a parent of the row variable (an input in its regression).\n\n\n\nFigure 1: Omega Matrix Structure\n\n\nThis defines the graphical structure. An edge is placed between the nodes corresponding to the edges. Figure Figure 2 shows this graph.\n\n\n\n\n\n\n\n\nEASD\n\n  \n\nSGP\n\n Solve Geometric Problems   \n\nCR\n\n Common Ratio   \n\nSGP->CR\n\n    \n\nExamG\n\n Example Geometric   \n\nSGP->ExamG\n\n    \n\nExtG\n\n Extend Geometric   \n\nSGP->ExtG\n\n    \n\nTabEG\n\n Table   Extend Geometric   \n\nSGP->TabEG\n\n    \n\nModTEG\n\n Model Table   Extend Geometric  \n\n\nFigure 2: Graph of EASD Net\n\n\n\n\nNote that the nodes are ordered (in both rows and columns) so that the parents of a given node always fall earlier in the sequence than the child. With this topological sorting, all of the checked boxes (selected edges) should be in the lower triangle; this will ensure that the model graph has no directed cycles.\n\n\nAncestral Nodes (without parents)\nThe regression model with no parents is just a normal distribution. The parameters specify the mean and variance of the model.\n\nSGP <- PnetFindNode(CM1,\"SolveGeometricProblems\")\ngadget1 <- MakeRegressionGadget(SGP)\nshiny::shinyApp(gadget1$ui,gadget1$server, options(height=2000))\n\nAn online version of this gadget can be seen at https://pluto.coe.fsu.edu/rdemos/Peanut/miniACED_SGP.Rmd.\nThe distribution is discretized, but adjusting mean parameter controls which bars the distribution is concentrated in, and how concentrated it is.\n\n\nCompetency Variables with one parent.\nHere again the normalLink is chosen because the parent variable states will be mapped to quantiles of the normal distribution.\nRather expressing the regression in terms of the residual standard deviation, it is easier to think of it in terms of \\(R^2\\). In the final model, the coefficients are scaled so that final model has the appropriate \\(R^2\\).\nIn particular, the residual standard deviation is set to\n\\[ \\sqrt{ \\left ( \\frac{1}{K}\\sum_{k \\in \\text{parents}} a_k^2 \\right\n) \\left ( \\frac{1}{R^2} - 1 \\right ) } .\\]\nHere, \\(K\\) is the number of parents and \\(k\\) is an index that ranges over the parent variables. The \\(a_k\\) are the discriminations (regression weights) input through the system and \\(R^2\\) is the selected \\(R^2\\) value. The \\(1/K\\) scale factor is set so that if all of the parent variables have an SD of 1, and all of the coefficients are 1, the child variable has a marginal SD of 1 as well.\nFinally, the intercept column represents a shift in the mean of the distribution relative to the parents. Positive values mean that more people in the population have this skill relative to the parents, and negative values mean fewer.\n\nCR <- PnetFindNode(CM1,\"CommonRatio\")\ngadget2 <- MakeRegressionGadget(CR)\nshiny::shinyApp(gadget2$ui,gadget2$server, options(height=2000))\n\nThe resulting gadget is in: https://pluto.coe.fsu.edu/rdemos/Peanut/miniACED_CR.Rmd.\n\n\nColumns in the Omega Matrix\nIn this model, SolveGeometricProblems is the ancestral node. The others all have just the one parent. Therefore\n\n\n\nFigure 3: Omega Matrix Parameters\n\n\nOverall, the columns in the \\(\\Omega\\)-matrix are as follows:\n\nNode – The name of the nodes in the competency model. The order is important, generally, the parents of a given node should come before the child nodes in the sequence.\nNodeNames – The names of the nodes appear also in the columns, in the same order as the first column. The cells here provide check boxes for the edges. Note that the diagonal is not used.\nLink – The link function used (see next section). Note that currently the normalLink is the only available choice for the \\(\\Omega\\)-matrix.\nRules – The combination function used (see next section). The choice of Compensatory gives the discrete regression model.\nR2 – The \\(R^2\\) is the measure of how much of the variance of the child variable can be explained by the parents.\nA.NodeName – There is one column here for each node. This gives the slope/discrimination value for that node.\nIntercept – The intercept for the regression; positive is the child skill is more prevalent in the population than the parent skills, and negative if it is less prevalent. Note this is a negative difficulty parameter.\nPriorWeight – Used in Bayesian parameter updating, the prior parameter values are assigned the equivalent weight as this many observations."
  },
  {
    "objectID": "Building.html#sec-Q",
    "href": "Building.html#sec-Q",
    "title": "Building Networks Models with Expert Opinion",
    "section": "\\(Q\\)-matrix",
    "text": "\\(Q\\)-matrix\nThe two warehouses give us the basic definitions of the networks and the nodes. The \\(\\Omega\\)-matrix has the distributions for the competency model variables. The next step is to describe the relationships between the competency variables and the observable outcomes in the evidence models. Note that the same variable name, e.g., isCorrect can be reused across many evidence models, so the full name of the variable has both the model and variable name.\n\nRepresenting Structure\nThe \\(Q\\)-matrix is the place to start. (tatsuoka1983?) popularized the use of a matrix \\(Q\\) to represent the cognitive structure of an assessment, where the rows represent items (or in the case of more complex assessment observable variables) and the columns represent attributes. Tatsuoka used the term attribute to emphasize the duality between the cognitive skill possessed by the subjects and the items which demand that skill. Tatsuoka’s rule space model regarded the attributes as binary, but the notation works equally well if the cognitive variables are ordinal or scale.\nThere is a simple relationship between the \\(Q\\)-matrix and the Bayesian network. If an element \\(q_{jk}=1\\), then there is an edge between Competency Variable \\(k\\) and Observable Variable \\(j\\) in the corresponding evidence model. For every observable variable, the parents can be determined by looking across the row to find all cell entries with a one.\n\n\n\nFigure 4: Q-matrix in the Spreadhseet\n\n\nNote that the first two columns are Model and Node. The isCorrect nodes in the models CommonRatioEasyEM, ExamplesGeometricMedEM and ModelTableExtendGeometricHardEM are all different variables. In particular, they have different entries in the \\(Q\\)-matrix. In ACED, the observable names reflect the targeted competencies, but that was merely a convention used by the design team.\nNote also that in the spreadsheet the cells of the \\(Q\\)-matrix is represented with check boxes. This emphasizes the idea that checking the box is showing that skill referenced in the column is needed for the observable node referenced in the row.\nThe Bayes net structure (for the evidence models) follows directly from the \\(Q\\)-matrix. The parents for the observable variables are the nodes (in the competency model) that correspond to the checked columns in the \\(Q\\)-matrix.\n\n\nDefining Conditional Probability Tables.\nIn addition to the graphical structure, a conditional probability table for each node, given its parents must be specified. Lou DiBello pressed some models from item response theory (IRT) into service to provide a language for describing relationships using the familiar psychometric parameters “difficulty” and “discrimination” (bninea?).\n\nThe DiBello Framework\nThe DiBello method has three steps:\n\nMap the states of the parent variable onto a unit normal \\(\\theta\\) scale. Let \\(\\theta_{k,s_{k,m}}\\) be the numeric value associated with the Parent Variable \\(S_k\\) being in State \\(s_{k,m}\\). These are represented by the StateValue column in the “Nodes” sheet.\nCombine the parent \\(\\theta\\) values into a single variable using combination Rules. This yields a single set of effective \\(\\theta\\) values for each combination of the parent variables. Let \\(\\eta_{j,c}(\\theta_1, \\ldots, \\theta_K)\\) be the function which calculates this effective theta for observable \\(Y_j\\) associated with the transition between State \\(c-1\\) and State \\(c\\) for the child variable. (In the simple cases below, \\(\\eta_{j,c}(\\cdot)\\) has the same functional form for all \\(c\\), but possibly different values of the parameters.) This is specified through the Rules column in the “Q” matrix spreadsheet.\nChange the effective \\(\\theta\\)s into conditional probabilities using a Link function. Let \\(g_j(\\eta_{j,1},\\ldots,\\eta_{j,C})\\) be the link function. This is specified through the Link column in the “Q” matrix spreadsheet.\n\nThe setup is similar in many ways to generalized linear models, where \\(\\eta(\\cdot)\\) takes the place of the linear predictor. (The method allows non-linear functions, but favors monotonic combination functions.)\nThe first step was already done in the “Nodes” sheet, as the NodeValues column provides the values. Note that the chosen values of \\([1, 0, -1]\\) are close to the midpoints (with respect to the normal distribution) of three equal probability intervals, which would be \\([.97, 0, -.97]\\).\nFor the last step, the partialCredit link function is chosen (see the last column of Figure 4). This is the most flexible of the currently available link functions, as it allows for a different probability to transition from NoCredit to Partial credit, and Partial credit to Full credit. Note that in general, the link function needs to address the transitions between states and not the states themselves.1 In this case, all of the observables are binary, so there is no difference between the partialCredit and gradedResponse link functions.\nFinally, the combination rule is chosen in consultation with the task designers and subject matter experts. The Compensatory rule states that all skills are necessary for solving the problem so that performance will be dominated by the weakest skill.2\n\n\n\nMultiple-A Rules\nIn the compensatory model, there needs to be one slope parameter for each parent variable, and a difficulty (negative interncept) parameter. The first is named “A._VariableName_”, and the latter is named “B”. Note that only the cells corresponding to the 1’s in the Q-matrix columns.\n\n\n\nQ-matrix A-parameters\n\n\nNote that there is another set of combination rules (the OffsetConjuctive and OffsetDisjunctive) rules in which there is a different difficulty (demand) parameter for each parent and a single slope (\\(A\\)). This is the Multiple-B style. These columns are currently hidden.\nNote also, that if the observable values has more than two states, there will in general be one fewer numbers in each column than the number of states. (The old way of representing this is with multiple rows in the Q-matrix; the revised method will include multiple entries in each cell.\n\n\nA graphical example\n\nEM <- WarehouseSupply(Nethouse,\"TableExtendGadget\")\nisC <- PnetFindNode(EM,\"isCorrect\")\ngadget3 <- MakeDPCGadget(isC)\nshiny::shinyApp(gadget3$ui,gadget3$server)\n\nThe gadget can be seen at https://pluto.coe.fsu.edu/rdemos/Peanut/miniACED_TE.Rmd.\n\n\nColumns in the Extended Q-matrix\nThe first two columns in the \\(Q\\)-matrix sheet identify the variable.\n\nModel, Node – The name of the model and node. Note that two variables with the same name (Node column) but different models are distinct.\nNStates – Number of States the node has. This should agree with the Nodes sheet.\nStates – The names of all but one of the states. In general, probabilities are built for the transition between states. States are assume to be in decreasing order: (High, Medium, Low), so in the \\(Q\\)-matrix the state High refers to the transition from Medium to High and Medium refers to the transition from Low to Medium. It is assumed that all responses are at least Low, so no parameters are needed for the last state.\nLink – The name of the link function. Currently supported are partialCredit and gradedResponse. There is limited support for normalLink as well.\nLinkScale – An additional scale parameter (e.g., residual standard deviation) for the link function. Currently only the normalLink uses this. If supplied, it should be a positive number.\n\\(Q\\)-matrix proper (NodeName) – Next, there are columns in the \\(Q\\)-matrix corresponding to the variables in the competency model. There is a check in the corresponding box if the observable referenced in the row depends on the competency variable in the column.\nRules – This is the name of the combination function that is used. Note that under the partialCredit link function, this can be either a single value or a distinct value for each state transition. (If a single value is given it is replicated out to the number of sites minus 1).\n\nThere are two types of rules, which require different sets of parameters. Multi-A Rules (Compensatory and all Conjuctive and Disjunctive3) have one discrimination (\\(A_p\\)) parameter for each parent variable, and a single difficulty/demand (\\(B\\)) parameter. Multi-B Rules (OffsetConjunctive and OffsetDisjunctive) have one demand parameter (\\(B_p\\)) for each parent variable, and a single discrimination ($A%). Therefore different columns in the spreadsheet are used depending on the choice of rules.\n\nMulti-A Table Columns.\nThese are regression-like models. There is one slope (\\(A_p\\)) for each parent variable, and a single intercept (\\(B\\)).\n\nA.NodeName – is used to give the weight for the corresponding parent variable. This should be left blank if the column is not a parent variable (not checked in the \\(Q\\)-matrix).\nB – Difficulty/demand. The B column without a variable name attached is used to specify the overall difficulty of the item.\n\nIn each case there can be a single value, or a list of values corresponding to the state transitions.\nThere are some additional rules depending on the link function.\n\npartialCredit – no restrictions on the parameters. Some discriminations can be 0 or missing to indicate that a particular parent variable has no role in that transition.\ngradedResponse – This model requires parameter restrictions to prevent negative probabilities. In particular, there must be one difficulty (\\(B\\)) parameter for each transition and they must be non-increasing. Although it is possible to have multiple discrimination parameters, restricting the discrimination parameters to be the same for every level ensures there will be no negative probabilities.\nnormalLink – This link function requires a single set of parameters for all state transitions. (It also requres the LinkScale).\n\n\n\nMulti-B Parameters\nThe OffsetConjunctive and OffsetDisjunctive models use min() and max() respectively to collapse from the \\(p\\) dimensions of the parents to the single dimension of the child. There is one offset parameter (\\(B_p\\)), a difficulty or a demand, for each parent. There is only a single discrimination (\\(A\\)).\n\nA – Discrimination (slope). The A column without a variable name attached is used to specify the overall discrimination of the item.\nB._NodeName_ – There is a demand parameter for each relevant (box checked in the \\(Q\\)-matrix) competency variable.\n\nIn each case there can be a single value, or a list of values corresponding to the state transitions.\nThere are some additional rules depending on the link function.\n\npartialCredit – no restrictions on the parameters. Some discriminations can be infinite or missing to indicate that a particular parent variable has no role in that transition.\ngradedResponse – Each difficulty parameter should be a non-increasing series. Again, is safest to use a common discrimination parameter for all transitions.\nnormalLink – Again, a single set of parameters for all transitions is used here.\n\nFinally, the last column in the spreadsheet for both multi-\\(A\\) and multi-\\(B\\) rules is the PriorWeight. This has the same interpretation as it does in the \\(\\Omega\\)-matrix."
  },
  {
    "objectID": "Building.html#statistics",
    "href": "Building.html#statistics",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Statistics",
    "text": "Statistics\nAfter student specific evidence, the Bayesian network contains the posterior distribution over the possible skill states for the student based on both the prior (population) model and the observed evidence. Rather than look at the full posterior distribution, usually the Bayesian networks outputs certain statistics of the posterior distribution. Most of these are focused on a single node.\nThe currently supported statistics are:\n\nPnodeMargin – This returns the marginal posterior probability for the node. It is a vector (simplex) over the states of the node.\nPnodeMode – The state of the node that has the highest marginal probability. (Value is a character scalar).\nPnodeMedian – (Assumes states are ordered). The state of the node which is associated with cumulative probability of .5. (Values is a character scalar).\n\nThe next two statistics assume that real values have been assigned to the states, making it a random variable.\n\nPnodeEAP – The expected a posterior value or posterior mean for the random variable’s marginal distribution.\nPnodeSD – The standard deviation of the marginal distribution of the random variable.\n\n\n\n\nStatistics Table"
  },
  {
    "objectID": "Building.html#putting-it-all-together",
    "href": "Building.html#putting-it-all-together",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Putting it all together",
    "text": "Putting it all together\nFirst download and clean all the sheets.\n\nnetman <- read_sheet(sheetID,\"Nets\")\n\n✔ Reading from \"MiniACED\".\n\n\n✔ Range ''Nets''.\n\nnodeman <- read_sheet(sheetID,\"Nodes\")\n\n✔ Reading from \"MiniACED\".\n\n\n✔ Range ''Nodes''.\n\nomega <- read_sheet(sheetID,\"Omega\")\n\n✔ Reading from \"MiniACED\".\n\n\n✔ Range ''Omega''.\n\nQQ <- read_sheet(sheetID,\"Q\")\n\n✔ Reading from \"MiniACED\".\n\n\n✔ Range ''Q''.\n\nstats <- read_sheet(sheetID,\"Statistics\")\n\n✔ Reading from \"MiniACED\".\n\n\n✔ Range ''Statistics''.\n\n\nNext, build the Network and Node warehouse. Note that for PNetica this requires a reference to the NeticaSession object. Note also, that the network warehouse has a reference to a directory where the network file will be stored.\n\nNethouse <- PNetica::BNWarehouse(manifest=netman,session=sess,\n                                 address=file.path(\"miniACED1\"),\n                                 key=\"Name\")\nNodehouse <- PNetica::NNWarehouse(manifest=nodeman,\n                                  key=c(\"Model\",\"NodeName\"),\n                                  session=sess)\n\nNext build the competency model. First create a blank network from the Network arehouse, then, use the Omega2Pnet function.\n\nCM <- WarehouseSupply(Nethouse,\"miniACEDPM\")\nOmega2Pnet(omega,CM,Nodehouse,override=TRUE)\n\nINFO [2023-04-11 11:07:18] Proficiency variables are\n\n[1] \"SolveGeometricProblems\" \"CommonRatio\"            \"ExamplesGeometric\"     \n[4] \"ExtendGeometric\"        \"TableGeometric\"         \"ModelGeometric\"        \n\n\nWarning: Setting row names on a tibble is deprecated.\nSetting row names on a tibble is deprecated.\n\n\nINFO [2023-04-11 11:07:18] Building list of nodes.\nINFO [2023-04-11 11:07:18] Processing links.\nWARN [2023-04-11 11:07:18] While processing links for node: CommonRatio\nWARN [2023-04-11 11:07:18] Node has parents: \n\n                         \n\"SolveGeometricProblems\" \nWARN [2023-04-11 11:07:18] But Omega matrix has parents: \n\ncharacter(0)\nWARN [2023-04-11 11:07:18] Changing node CommonRatio to match Omega matrix.\nWARN [2023-04-11 11:07:18] While processing links for node: ExamplesGeometric\nWARN [2023-04-11 11:07:18] Node has parents: \n\n                         \n\"SolveGeometricProblems\" \nWARN [2023-04-11 11:07:18] But Omega matrix has parents: \n\ncharacter(0)\nWARN [2023-04-11 11:07:18] Changing node ExamplesGeometric to match Omega matrix.\nWARN [2023-04-11 11:07:18] While processing links for node: ExtendGeometric\nWARN [2023-04-11 11:07:18] Node has parents: \n\n                         \n\"SolveGeometricProblems\" \nWARN [2023-04-11 11:07:18] But Omega matrix has parents: \n\ncharacter(0)\nWARN [2023-04-11 11:07:18] Changing node ExtendGeometric to match Omega matrix.\nWARN [2023-04-11 11:07:18] While processing links for node: TableGeometric\nWARN [2023-04-11 11:07:18] Node has parents: \n\n                         \n\"SolveGeometricProblems\" \nWARN [2023-04-11 11:07:18] But Omega matrix has parents: \n\ncharacter(0)\nWARN [2023-04-11 11:07:18] Changing node TableGeometric to match Omega matrix.\nWARN [2023-04-11 11:07:18] While processing links for node: ModelGeometric\nWARN [2023-04-11 11:07:18] Node has parents: \n\n                         \n\"SolveGeometricProblems\" \nWARN [2023-04-11 11:07:18] But Omega matrix has parents: \n\ncharacter(0)\nWARN [2023-04-11 11:07:18] Changing node ModelGeometric to match Omega matrix.\nINFO [2023-04-11 11:07:18] Processing CPTs.\nWARN [2023-04-11 11:07:18] Alpha vector for node SolveGeometricProblems is empty.\nWARN [2023-04-11 11:07:18] Alpha vector for node CommonRatio is empty.\nWARN [2023-04-11 11:07:18] Alpha vector for node ExamplesGeometric is empty.\nWARN [2023-04-11 11:07:18] Alpha vector for node ExtendGeometric is empty.\nWARN [2023-04-11 11:07:18] Alpha vector for node TableGeometric is empty.\nWARN [2023-04-11 11:07:18] Alpha vector for node ModelGeometric is empty.\n\n\nNetica Network named  miniACEDPM \n  Network is currently active.\n  Nodes :  CommonRatio ExamplesGeometric ExtendGeometric ModelGeometric SolveGeometricProblems TableGeometric .\n\n\nOnce the competency model is built, build the evidence models.\n\nQmat2Pnet(QQ,Nethouse,Nodehouse,override=TRUE)\n\nINFO [2023-04-11 11:07:18] Proficiency variables:\n\n[1] \"SolveGeometricProblems\" \"CommonRatio\"            \"ExamplesGeometric\"     \n[4] \"ExtendGeometric\"        \"TableGeometric\"         \"ModelGeometric\"        \nINFO [2023-04-11 11:07:18] Processing net CommonRatioEasyEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net CommonRatioEasyEM\nINFO [2023-04-11 11:07:18] Processing net CommonRatioMedEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net CommonRatioMedEM\nINFO [2023-04-11 11:07:18] Processing net CommonRatioHardEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net CommonRatioHardEM\nINFO [2023-04-11 11:07:18] Processing net ExamplesEasyEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net ExamplesEasyEM\nINFO [2023-04-11 11:07:18] Processing net ExamplesMedEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net ExamplesMedEM\nINFO [2023-04-11 11:07:18] Processing net ExamplesHardEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net ExamplesHardEM\nINFO [2023-04-11 11:07:18] Processing net ExtendEasyEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net ExtendEasyEM\nINFO [2023-04-11 11:07:18] Processing net ExtendMedEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net ExtendMedEM\nINFO [2023-04-11 11:07:18] Processing net ExtendHardEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net ExtendHardEM\nINFO [2023-04-11 11:07:18] Processing net TableExtendEasyEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net TableExtendEasyEM\nINFO [2023-04-11 11:07:18] Processing net TableExtendMedEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net TableExtendMedEM\nINFO [2023-04-11 11:07:18] Processing net TableExtendHardEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net TableExtendHardEM\nINFO [2023-04-11 11:07:18] Processing net ModelTableExtendEasyEM\nINFO [2023-04-11 11:07:18] Processing node isCorrect in net ModelTableExtendEasyEM\nINFO [2023-04-11 11:07:19] Processing net ModelTableExtendMedEM\nINFO [2023-04-11 11:07:19] Processing node isCorrect in net ModelTableExtendMedEM\nINFO [2023-04-11 11:07:19] Processing net ModelTableExtendHardEM\nINFO [2023-04-11 11:07:19] Processing node isCorrect in net ModelTableExtendHardEM\n\n\nNext write out the nets. The Warehouse takes care of the mechanics here.\n\nfor (name in netman$Name) {\n  if (nchar(name)>0L) {\n    net <- WarehouseSave(Nethouse,name)\n  }\n}\n\nWrite out the manifest. In this case, there are six variants of each task type: 1a and 1b are easy, 2a and 2b are medium and 3a and 3b are hard. So this can be used to build up the manifest file which maps tasks to evidence models.\n\n#: eval: false\nEMs <- c(\"CommonRatio\",\"ExamplesGeometric\",\"ExtendGeometric\",\n         \"TableExtendGeometric\",\"ModelTableExtend\")\nEMTable <- data.frame(\n    Task=paste(\"t\",rep(EMs,each=6),\n               rep(c(\"1a\",\"1b\",\"2a\",\"2b\",\"3a\",\"3b\"),5),sep=\"\"),\n    EM=paste(rep(EMs,each=6),rep(c(\"Easy\",\"Easy\",\"Med\",\"Med\",\"Hard\",\"Hard\"),5),\n             \"EM\",sep=\"\"),\n    X=rep(c(108,342,588,1134,858),each=6),\n    Y=rep(c(282,402,522,642,762,882),5))\nwrite.csv(EMTable,file.path(\"miniACED1\",\"EMTable.csv\"))\nwrite.csv(stats,file.path(\"miniACED1\",\"Statistics.csv\"))"
  }
]