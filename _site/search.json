[
  {
    "objectID": "BayesNets.html#irt-taskevidence-model",
    "href": "BayesNets.html#irt-taskevidence-model",
    "title": "Bayesian Networks",
    "section": "IRT Task/Evidence Model",
    "text": "IRT Task/Evidence Model\nTasks yield an work product which can be unambiguously scored right or wrong.\nEach task has a single observable outcome variable.\nTasks are often called items, although the common usage often blurs the distinction between the presentation of the item and the outcome variable."
  },
  {
    "objectID": "BayesNets.html#irt-rasch-evidence-model",
    "href": "BayesNets.html#irt-rasch-evidence-model",
    "title": "Bayesian Networks",
    "section": "IRT (Rasch) Evidence Model",
    "text": "IRT (Rasch) Evidence Model\n\nLet \\(X_j\\) be observable outcome variable from Task \\(j\\).\n\\(\\Pr(X_j =\\text{right} | \\theta, \\beta_j) = \\frac{1}{1+e^{-(\\theta-\\beta_j)}}\\) - \\(\\beta_j\\) is the difficulty of the item.\nCan crank through the formula for each of the five possible values of \\(\\theta\\) to get values for Conditional Probability Tables (CPT)"
  },
  {
    "objectID": "BayesNets.html#irt-assembly-model",
    "href": "BayesNets.html#irt-assembly-model",
    "title": "Bayesian Networks",
    "section": "IRT Assembly Model",
    "text": "IRT Assembly Model\n5 items\nIncreasing difficulty:\n\\[ (\\beta_1, \\ldots, \\beta_5) = (-1.5, -0.75, 0, 0.75, 1.5)\\ \\] .\n\nirt1pl &lt;- function(theta,beta) {\n  1/(1+exp(beta-theta))\n}\nirt1pl(c(-2,-1,0,1,2),0)\n\n[1] 0.1192029 0.2689414 0.5000000 0.7310586 0.8807971\n\n\nItems are presented adaptively."
  },
  {
    "objectID": "BayesNets.html#conditional-probability-tables",
    "href": "BayesNets.html#conditional-probability-tables",
    "title": "Bayesian Networks",
    "section": "Conditional Probability Tables",
    "text": "Conditional Probability Tables\n\n\n\nθ\nPrior\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\n\n\n-2\n0.1\n0.3775\n0.2227\n0.1192\n0.0601\n0.0293\n\n\n-1\n0.2\n0.6225\n0.4378\n0.2689\n0.1480\n0.0759\n\n\n0\n0.4\n0.8176\n0.6792\n0.5000\n0.3208\n0.1824\n\n\n1\n0.2\n0.9241\n0.8520\n0.7311\n0.5622\n0.3775\n\n\n2\n0.1\n0.9707\n0.9399\n0.8088\n0.7773\n0.6225"
  },
  {
    "objectID": "BayesNets.html#problems-set-1",
    "href": "BayesNets.html#problems-set-1",
    "title": "Bayesian Networks",
    "section": "Problems Set 1",
    "text": "Problems Set 1\nUse the network CompensatoryConjunctiveNets/IRT5.dne to answer these questions.\n\n#|include: FALSE\n#system(\"netica CompensatoryConjunctiveNets/IRT5.dne\")\n\nInside Netica, to set a node to a value either, click on the value name, or right click and select the value.\nTo clear a value, click on it again, or right click and select “undefined”.\n\nAssume \\(\\theta =1\\), what is expected score (sum \\(X_j\\) )\nCalculate \\(P(\\theta |X_1 \\text{right})\\), \\(E[\\theta |X_1 =\\text{right}]\\).\nCalculate \\(P(\\theta |X_5 = \\text{right})\\), \\(E[\\theta |X_5 = \\text{right}]\\).\nScore three students who have the following observable patterns (Tasks 1--5):\n\n\n1,1,1,0,0\n1,0,0,1,1\n1,1,1,0,1\n\n\nSuppose we have observed for a given student \\(X_2 = \\text{right}\\) and \\(X_3=\\text{right}\\) , what is the next best item to present (hint, look for expected probabilities closest to .5,.5\nSame thing, with \\(X_2 = \\text{right}\\) and \\(X_3=\\text{wrong}\\)\nSame thing, with \\(X_2 = \\text{wrong}\\) and \\(X_3=\\text{wrong}\\)"
  },
  {
    "objectID": "BayesNets.html#context-effect-variables",
    "href": "BayesNets.html#context-effect-variables",
    "title": "Bayesian Networks",
    "section": "“Context” effect – Variables",
    "text": "“Context” effect – Variables\n\n\n\nIRT model with context effect\n\n\n\nContext variable – A parent variable introduced to handle conditional dependence among observables (testlet)\n\nConsistent with Stout’s (1987) ‘essential n-dimensionality’\nWang, Bradlow & Wainer (2001) SCORIGHT program for IRT\nPatz & Junker (1999) model for multiple ratings"
  },
  {
    "objectID": "BayesNets.html#context-effect-example",
    "href": "BayesNets.html#context-effect-example",
    "title": "Bayesian Networks",
    "section": "“Context” effect – example",
    "text": "“Context” effect – example\nSuppose that Items 3 and 4 share common presentation material\nExample: a word problem about “Yacht racing” might use nautical jargon like “leeward” and “tacking”\nPeople familiar with the content area would have an advantage over people unfamiliar with the content area.\nWould never us this example in practice because of DIF (Differential Item Functioning)"
  },
  {
    "objectID": "BayesNets.html#adding-a-context-variable",
    "href": "BayesNets.html#adding-a-context-variable",
    "title": "Bayesian Networks",
    "section": "Adding a context variable",
    "text": "Adding a context variable\nGroup Items 3 and 4 into a single task with two observed outcome variables\nAdd a person-specific, task-specific latent variable called “context” with values familiar and unfamiliar\nEstimates of \\(\\theta\\) will “integrate out” the context effect\nCan use as a mathematical trick to force dependencies between observables."
  },
  {
    "objectID": "BayesNets.html#irt-model-with-context-variable",
    "href": "BayesNets.html#irt-model-with-context-variable",
    "title": "Bayesian Networks",
    "section": "IRT Model with Context Variable",
    "text": "IRT Model with Context Variable\nUse the network CompensatoryConjunctiveNets/IRT5C.dne to answer these questions.\n\n#|include: FALSE\n#system(\"netica CompensatoryConjunctiveNets/IRT5C.dne\")\n\nThe CPTs in IRT5C have been set so that the marginal predictions (if the context is unknown), should have the same value.\n\n\n\nIRT model with Context Effect"
  },
  {
    "objectID": "BayesNets.html#problem-set-2",
    "href": "BayesNets.html#problem-set-2",
    "title": "Bayesian Networks",
    "section": "Problem Set 2",
    "text": "Problem Set 2\n\nCompare the following quantities in the context and no context models:\n\n\\(P(X_2)\\), \\(P(X_3)\\), \\(P(X_4)\\)\n\\(P(\\theta|X_2= \\text{right})\\), \\(P(\\theta|X_3= \\text{right} )\\)\n\\(P(X_4|X_2= \\text{right} )\\), \\(P(X_4 |X_3= \\text{right} )\\)\n\\(P(\\theta|X_3=\\text{wrong}, X_4=\\text{wrong})\\), \\(P(\\theta|X_3=\\text{right}, X_4=\\text{wrong})\\),\n\\(P(\\theta|X_3=\\text{wrong}, X_4=\\text{right})\\), \\(P(\\theta|X_3= \\text{right}, X_4=\\text{right})\\)"
  },
  {
    "objectID": "BayesNets.html#context-effect-postscript",
    "href": "BayesNets.html#context-effect-postscript",
    "title": "Bayesian Networks",
    "section": "Context Effect Postscript",
    "text": "Context Effect Postscript\nIf Context effect is generally construct-irrelevant variance, if correlated with group membership this is bad (DIF)\nWhen calibrating using 2PL IRT model, can get similar joint distribution for \\(\\theta\\), \\(X_3\\), and \\(X_4\\) by decreasing the discrimination parameter"
  },
  {
    "objectID": "BayesNets.html#common-setup-for-all-three-models",
    "href": "BayesNets.html#common-setup-for-all-three-models",
    "title": "Bayesian Networks",
    "section": "Common Setup for All Three Models",
    "text": "Common Setup for All Three Models\nThere are two parent nodes, and both parents are conditionally independent of each other. The difference among the three models lies in the third term below:\n\\[P( P_1, P_2, X) = P( P_1) \\cdot P(P_2) \\cdot P(X| P_1,P_2 )\\]\nThe priors for the parent nodes are the same for the three models with 0.3333 of probability at each of the H, M, and L states.\nThe initial marginal probability for X is the same for the three models (50/50)."
  },
  {
    "objectID": "BayesNets.html#conditional-probability-tables-1",
    "href": "BayesNets.html#conditional-probability-tables-1",
    "title": "Bayesian Networks",
    "section": "Conditional Probability Tables",
    "text": "Conditional Probability Tables\nThis table contains the conditional probabilities for the parent nodes (P1 and P2) and the combination model for the three models.\n{r CPTcombine3 Pcomp &lt;- c(H=.9,M=.5,L=.1) Pconj &lt;- c(H=.9,M=.7,L=.3) Pdisj &lt;- c(H=.7,M=.3,L=.1) cpts &lt;- data.frame(P1=rep(names(Pcomp),each=3),P2=rep(names(Pcomp),3),                    Compensatory=as.vector(outer(Pcomp,Pcomp,\"+\")/2),                    Conjunctive=as.vector(outer(Pconj,Pconj,\"min\")),                    Disjunctive=as.vector(outer(Pdisj,Pdisj,\"max\"))) knitr::kable(cpts)"
  },
  {
    "objectID": "BayesNets.html#problem-set-3",
    "href": "BayesNets.html#problem-set-3",
    "title": "Bayesian Networks",
    "section": "Problem Set 3",
    "text": "Problem Set 3\nUse the network CompensatoryConjunctiveNets/Comb3Same_1.dne to answer these questions.\n\n#|include: FALSE\n#system(\"netica CompensatoryConjunctiveNets/Comb3Same_1.dne\")\n\n\nVerify that \\(P(P_1)\\), \\(P(P_2)\\) and \\(P(Obs)\\) are the same for all three models. ( Obs represents either the node Compensatory , Conjunctive, or Disjunctive )\nAssume Obs=right; calculate \\(P(P_1)\\) and \\(P(P_2)\\) for all three models.\nAssume Obs=wrong; calculate \\(P(P_1)\\) and \\(P(P_2)\\) for all three models.\nAssume Obs=right, and \\(P_1\\)=H; Calculate \\(P(P_2)\\) for all three models.\nAssume Obs=right, and \\(P_1\\)=M; Calculate \\(P(P_2)\\) for all three models.\nAssume Obs=right, and \\(P_1\\)=L; Calculate \\(P(P_2)\\) for all three models.\n\nExplain the differences"
  },
  {
    "objectID": "BayesNets.html#dibello-models-a-look-ahead",
    "href": "BayesNets.html#dibello-models-a-look-ahead",
    "title": "Bayesian Networks",
    "section": "DiBello Models: A look ahead",
    "text": "DiBello Models: A look ahead\nMap states of parents to points on IRT \\(\\theta\\) scale.\nUse Compensatory (average), Conjunctive (minimum) or Disjunctive (maximum) to combine effective thetas.\nUse IRT (logistic) model to map to probabilities."
  },
  {
    "objectID": "BayesNets.html#activity-3",
    "href": "BayesNets.html#activity-3",
    "title": "Bayesian Networks",
    "section": "Activity 3",
    "text": "Activity 3\n\nGo back to the Driver’s License Exam you built in Session I and add some numbers\nNow put in some observed outcomes\n\nHow did the probabilities change?\nIs that about what you expected?"
  },
  {
    "objectID": "BayesNets.html#aced-background",
    "href": "BayesNets.html#aced-background",
    "title": "Bayesian Networks",
    "section": "ACED Background",
    "text": "ACED Background\n\nACED (Adaptive Content with Evidence-based Diagnosis)\nVal Shute (PD), Aurora Graf, Jody Underwood, Eric Hansen, Peggy Redman, Russell Almond, Larry Casey, Waverly Hester, Steve Landau, Diego Zapata\nDomain: Middle School Math, Sequences\nProject Goals:\n\nAdaptive Task Selection\nDiagnostic Feedback\nAccessibility"
  },
  {
    "objectID": "BayesNets.html#aced-features",
    "href": "BayesNets.html#aced-features",
    "title": "Bayesian Networks",
    "section": "ACED Features",
    "text": "ACED Features\nValid Assessment . Based on evidence-centered design (ECD).\nAdaptive Sequencing . Tasks presented in line with an adaptive algorithm.\nDiagnostic Feedback . Feedback is immediate and addresses common errors and misconceptions.\nAligned . Assessments aligned with (a) state and national standards and (b) curricula in current textbooks."
  },
  {
    "objectID": "BayesNets.html#pm-em-algorithm-for-scoring",
    "href": "BayesNets.html#pm-em-algorithm-for-scoring",
    "title": "Bayesian Networks",
    "section": "PM-EM Algorithm for Scoring",
    "text": "PM-EM Algorithm for Scoring\n\nMaster Bayes net with just proficiency model(PM)\nDatabase of Bayes net fragments corresponding to evidence models (EMs), indexed by task ID\nTo score a task:\n\nFind EM fragment corresponding to task\nJoin EM fragment to PM\nEnter Evidence\nAbsorb evidence from EM fragment into network\nDetach EM fragment"
  },
  {
    "objectID": "BayesNets.html#an-example",
    "href": "BayesNets.html#an-example",
    "title": "Bayesian Networks",
    "section": "An Example",
    "text": "An Example\n\nFive proficiency variables\nThree tasks, with observables {X11}, {X21, X22 , X23}, {X31}."
  },
  {
    "objectID": "BayesNets.html#q-which-observables-depend-on-which-proficiency-variables",
    "href": "BayesNets.html#q-which-observables-depend-on-which-proficiency-variables",
    "title": "Bayesian Networks",
    "section": "Q: Which observables depend on which proficiency variables?",
    "text": "Q: Which observables depend on which proficiency variables?\nA: See the Q-matrix (Fischer, Tatsuoka).\n\n\n\n\nq1\nq2\nq3\nq4\nq5\nX23\n\n\n\n\nX11\n1\n0\n0\n0\n0\n–\n\n\nX21\n0\n1\n0\n0\n0\n1\n\n\nX22\n0\n1\n0\n1\n0\n1\n\n\nX23\n0\n0\n0\n0\n0\nN/A\n\n\nX31\n0\n1\n1\n1\n0\n–"
  },
  {
    "objectID": "BayesNets.html#proficiency-model-evidence-model-split",
    "href": "BayesNets.html#proficiency-model-evidence-model-split",
    "title": "Bayesian Networks",
    "section": "Proficiency Model / Evidence Model Split",
    "text": "Proficiency Model / Evidence Model Split\n\nFull Bayes net for proficiency model and observables for all tasks can be decomposed into fragments.\n\nProficiency model fragment(s) (PMFs) contain proficiency variables.\nAn evidence model fragment (EMF) for each task.\nEMF contains observables for that task and all proficiency variables that are parents of any of them.\n\nPresumes observables are conditionally independent between tasks, but can be dependent within tasks.\nAllows for adaptively selecting tasks, docking EMF to PMF, and updating PMF on the fly."
  },
  {
    "objectID": "BayesNets.html#on-the-way-to-pmf-and-emfs",
    "href": "BayesNets.html#on-the-way-to-pmf-and-emfs",
    "title": "Bayesian Networks",
    "section": "On the way to PMF and EMFs…",
    "text": "On the way to PMF and EMFs…\n\nProficiency variables\n\nObservables and proficiency variable parents for the tasks"
  },
  {
    "objectID": "BayesNets.html#marry-parents-drop-directions-and-triangulate-in-pmf-with-respect-to-all-tasks",
    "href": "BayesNets.html#marry-parents-drop-directions-and-triangulate-in-pmf-with-respect-to-all-tasks",
    "title": "Bayesian Networks",
    "section": "Marry parents, drop directions, and triangulate (in PMF, with respect to all tasks)",
    "text": "Marry parents, drop directions, and triangulate (in PMF, with respect to all tasks)\n\n\n$# Footprints of tasks in proficiency model (figure out from rows in Q-matrix)"
  },
  {
    "objectID": "BayesNets.html#result",
    "href": "BayesNets.html#result",
    "title": "Bayesian Networks",
    "section": "Result:",
    "text": "Result:\n\nEach EMF implies a join tree for Bayes net propagation.\n\nInitial distributions for proficiency variables are uniform.\n\nThe footprint of the PM in the EMF is a clique intersection between that EMF and the PMF.\nCan “dock” EMFs with PMF one-at-a-time, to …\n\nabsorb evidence from values of observables to that task as updated probabilities for proficiency variables, and\npredict responses in new tasks, to evaluate potential evidentiary value of administering it."
  },
  {
    "objectID": "BayesNets.html#docking-evidence-model-fragments",
    "href": "BayesNets.html#docking-evidence-model-fragments",
    "title": "Bayesian Networks",
    "section": "Docking evidence model fragments",
    "text": "Docking evidence model fragments"
  },
  {
    "objectID": "BayesNets.html#properties-of-woe",
    "href": "BayesNets.html#properties-of-woe",
    "title": "Bayesian Networks",
    "section": "Properties of WOE",
    "text": "Properties of WOE\n“Centibans” (log base 10, multiply by 100)\nPositive for evidence supporting hypothesis, negative for evidence refuting hypothesis\nMovement in tails of distribution as important as movement near center\nBayes theorem using log odds"
  },
  {
    "objectID": "BayesNets.html#conditional-weight-of-evidence",
    "href": "BayesNets.html#conditional-weight-of-evidence",
    "title": "Bayesian Networks",
    "section": "Conditional Weight of Evidence",
    "text": "Conditional Weight of Evidence\nCan define Conditional Weight of Evidence\nNice Additive properties\nOrder sensitive\nWOE Balance Sheet (Madigan, Mosurski & Almond, 1997)"
  },
  {
    "objectID": "BayesNets.html#evidence-balance-sheet",
    "href": "BayesNets.html#evidence-balance-sheet",
    "title": "Bayesian Networks",
    "section": "Evidence Balance Sheet",
    "text": "Evidence Balance Sheet\n63 tasks total\n1 Easy\n2 Medium\n3 Hard\na Item type\nb Isomorph\nP(Solve Geom Sequences)\n\n\n\nTask\nAcc\nH\nM\nL\n\n\n\n\nSolveGeometricProblems2a\n0\n0.16\n0.26\n0.58\n\n\nSolveGeometricProblems3a\n1\n0.35\n0.35\n0.30\n\n\nSolveGeometricProblems3b\n1\n0.64\n0.29\n0.07\n\n\nSolveGeometricProblems2b\n1\n0.83\n0.16\n0.01\n\n\nVisualExtendTable2a\n1\n0.89\n0.10\n0.01\n\n\nSolveGeometricProblems1a\n0\n0.78\n0.21\n0.01\n\n\nSolveGeometricProblems1b\n1\n0.82\n0.18\n0.00\n\n\nVisualExtendVerbalRule2a\n1\n0.85\n0.15\n0.00\n\n\nModelExtendTableGeometric3a\n1\n0.90\n0.10\n0.00\n\n\nExamplesGeometric2a\n0\n0.87\n0.13\n0.00\n\n\nVisualExplicitVerbalRule3a\n1\n0.91\n0.09\n0.00\n\n\nVerbalRuleModelGeometric3a\n1\n0.95\n0.05\n0.00\n\n\n\nWOE for H vs. M, L"
  },
  {
    "objectID": "BayesNets.html#expected-weight-of-evidence",
    "href": "BayesNets.html#expected-weight-of-evidence",
    "title": "Bayesian Networks",
    "section": "Expected Weight of Evidence",
    "text": "Expected Weight of Evidence\nWhen choosing next “test” (task/item) look at expected value of WOE where expectation is taken wrt P(E|H) .\nwhere represent the possible results."
  },
  {
    "objectID": "BayesNets.html#calculating-ewoe",
    "href": "BayesNets.html#calculating-ewoe",
    "title": "Bayesian Networks",
    "section": "Calculating EWOE",
    "text": "Calculating EWOE\nMadigan and Almond (1996)\nEnter any observed evidence into net\nInstantiate Hypothesis = True (may need to use virtual evidence if hypothesis is compound)\nCalculate for each candidate item\nInstantiate Hypothesis = False\nCalculate for each candidate item\n\n\n$# Related Measures\nValue of Information\nS is proficiency state\nd is decision\nu is utility"
  },
  {
    "objectID": "BayesNets.html#related-measures-2",
    "href": "BayesNets.html#related-measures-2",
    "title": "Bayesian Networks",
    "section": "Related Measures (2)",
    "text": "Related Measures (2)\nMutual Information\nExtends to non-binary hypothesis nodes\nKullback-Liebler distance between joint distribution and independence"
  },
  {
    "objectID": "BayesNets.html#task-selection-exercise-1",
    "href": "BayesNets.html#task-selection-exercise-1",
    "title": "Bayesian Networks",
    "section": "Task Selection Exercise 1",
    "text": "Task Selection Exercise 1\n\nUse ACEDMotif1.dne\n\nEasy, Medium, and Hard tasks for Common Ratio and Visual Geometric\n\nUse Hypothesis SolveGeometricProblems &gt; Medium\nCalculate EWOE for six observables\nAssume candidate gets first item right and repeat\n\nNext assume candidate gets first item wrong and repeat\nRepeat exercise using hypothesis SolveGeometricProblems &gt; Low\nUse Network ACEDMotif2.dne\nSelect the SolveGeometricProblems node\nRun the program Network&gt;Sensitivity to Findings\nThis will list the Mutual information for all nodes\nSelect the observable with the highest mutual information as the first task\nUse this to process a person who gets every task right\nUse this to process a person who gets every task wrong"
  },
  {
    "objectID": "BayesNets.html#aced-evaluation",
    "href": "BayesNets.html#aced-evaluation",
    "title": "Bayesian Networks",
    "section": "ACED Evaluation",
    "text": "ACED Evaluation\n\nMiddle School Students\nDid not normally study geometric series\nFour conditions:\n\nElaborated Feedback/Adaptive (E/A; n=71)\nSimple Feedback/Adaptive (S/A; n=75)\nElaborated Feedback/Linear (E/L; n=67)\nControl (no instruction; n=55)\n\nStudents given all 61 geometric items\nAlso given pretest/posttest (25 items each)"
  },
  {
    "objectID": "BayesNets.html#aced-scores",
    "href": "BayesNets.html#aced-scores",
    "title": "Bayesian Networks",
    "section": "ACED Scores",
    "text": "ACED Scores\n\n\n\nACED Marginal Distributions\n\n\n\nFor Each Proficiency Variable\n\nMarginal Distribution\nModal Classification\nEAP Score (High=1, Low=-1)"
  },
  {
    "objectID": "BayesNets.html#aced-reliability",
    "href": "BayesNets.html#aced-reliability",
    "title": "Bayesian Networks",
    "section": "ACED Reliability",
    "text": "ACED Reliability\n\n\n\nProficiency (EAP)\nReliability\n\n\n\n\nSolve Geometric Sequences (SGS)\n0.88\n\n\nFind Common Ratio\n0.90\n\n\nGenerate Examples\n0.92\n\n\nExtend Sequence\n0.86\n\n\nModel Sequence\n0.80\n\n\nUse Table\n0.82\n\n\nUse Pictures\n0.82\n\n\nInduce Rules\n0.78\n\n\nNumber Right\n0.88\n\n\n\nCalculated with Split Halves (ECD design)\nCorrelation of EAP score with posttest is 0.65 (close to reliability of posttest)\nEven with pretest forced into the equation, EAP score accounted for 17% unique variance\nReliability of modal classifications was worse"
  },
  {
    "objectID": "BayesNets.html#effect-of-adaptivity",
    "href": "BayesNets.html#effect-of-adaptivity",
    "title": "Bayesian Networks",
    "section": "Effect of Adaptivity",
    "text": "Effect of Adaptivity\n\n\n\nACED Validity by test length\n\n\nFor adaptive conditions, correlation with posttest seems to hit upper limit by 20 items\nStandard Error of Correlations is large\nJump in linear case related to sequence of items"
  },
  {
    "objectID": "BayesNets.html#effect-of-feedback",
    "href": "BayesNets.html#effect-of-feedback",
    "title": "Bayesian Networks",
    "section": "Effect of feedback",
    "text": "Effect of feedback\nE/A showed significant gains\nOthers did not\nLearning and assessment reliability!!!!!"
  },
  {
    "objectID": "BayesNets.html#acknowledgements",
    "href": "BayesNets.html#acknowledgements",
    "title": "Bayesian Networks",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nSpecial thanks to Val Shute for letting us used ACED data and models in this tutorial.\nACED development and data collection was sponsored by National Science Foundation Grant No. 0313202.\nComplete data available at: http://ecd.ralmond.net/ecdwiki/ACED/ACED"
  },
  {
    "objectID": "ECD.html#the-interplay-of-design-and-statistical-modeling",
    "href": "ECD.html#the-interplay-of-design-and-statistical-modeling",
    "title": "ECD Intro",
    "section": "The Interplay of Design and Statistical Modeling",
    "text": "The Interplay of Design and Statistical Modeling\nStatistical models must be selected/tailored according to the needs of the assessment\nSuch selection and adaptation is only meaningful in the larger context of the assessment design\nUnderstanding the discipline of assessment design is a necessary prerequisite for statistical modeling\nEvidence Centered Design is an assessment design framework with general applicability and utility"
  },
  {
    "objectID": "ECD.html#test-design-considerations",
    "href": "ECD.html#test-design-considerations",
    "title": "ECD Intro",
    "section": "Test Design Considerations",
    "text": "Test Design Considerations\n\nStakeholders\nRequirements\n\nPurpose of the test\nIntended population\n\nProspective Score Report\nEvidence-Centered Design\n\nClaims\nValidity\n\nSpecifications"
  },
  {
    "objectID": "ECD.html#evidence-centered-design",
    "href": "ECD.html#evidence-centered-design",
    "title": "ECD Intro",
    "section": "Evidence Centered Design",
    "text": "Evidence Centered Design\n\nEvidence Centered Design (ECD) provides a mechanism for\n\nCapturing and documenting information about the structure and strength of evidentiary relationships.\nCoordinating the work of test developers in authoring tasks and psychometricians in calibrating the measurement model.\nDocumenting the scientific information that provides the foundation for the assessment and its validity."
  },
  {
    "objectID": "ECD.html#the-central-question",
    "href": "ECD.html#the-central-question",
    "title": "ECD Intro",
    "section": "The Central Question",
    "text": "The Central Question\n\nEvidence-centered design centers around the questions:\n\n“What can we observe about an examinee’s performance which will provide evidence that the examinee has or does not have the knowledge, skills and abilities we wish to make claims about?”\n\n\n“How can we structure situations to be able to make those observations?”\n\n\nThis process results in the Conceptual Assessment Framework (CAF)"
  },
  {
    "objectID": "ECD.html#the-initial-frame",
    "href": "ECD.html#the-initial-frame",
    "title": "ECD Intro",
    "section": "The Initial Frame",
    "text": "The Initial Frame\n\nWhy are we measuring?\n\nWhat are the goals and the desires for use of this assessment?\nProspective Score Report\n\nWho are we measuring?\n\nWho would take the assessment?\nWho would view results and for what purpose?\n\nGoals of the assessment that represent the targets around which the rest of the design process is oriented"
  },
  {
    "objectID": "ECD.html#activity-1-drivers-license-exam",
    "href": "ECD.html#activity-1-drivers-license-exam",
    "title": "ECD Intro",
    "section": "Activity 1: Driver’s License Exam",
    "text": "Activity 1: Driver’s License Exam\nRedesign the driver’s licensure exam\nWrite down several claims you would like to make about people who receive a driver’s license\nGroup your claims into several proficiency variables related to the driver’s test\nDo the claims hold for high, medium or low values of those variables?\nUse Netica as a drawing tool and add your variables"
  },
  {
    "objectID": "ECD.html#activity-1-cont",
    "href": "ECD.html#activity-1-cont",
    "title": "ECD Intro",
    "section": "Activity 1 (cont)",
    "text": "Activity 1 (cont)\nList a bunch of activities that you may want prospective drivers to do in their exam\nWhat is environment of the task\nWhat are manipulable features of the task?\nPick one of the tasks you created and build an evidence model for it.\nWhat are some observable outcomes? their possible values?\nWhich proficiencies do they measure?\nThink a bit about putting this driver’s test together\nHow many tasks do we need of what types?\nHow much time will be spent in written tests? On the road? In simulators?\nHow do we verify the identity of applicants?"
  },
  {
    "objectID": "ECD.html#ecd---bayes-nets",
    "href": "ECD.html#ecd---bayes-nets",
    "title": "ECD Intro",
    "section": "ECD -> Bayes Nets",
    "text": "ECD -&gt; Bayes Nets\nRepresent Qualitative ECD argument with a graph (Domain Modeling) (Session I)\nTurn graphical structure into probability distribution over proficiency variables and observable outcomes (Bayes net; Session I)\nPerform inference (scoring) using that Bayes net (Session II)\nExpress probabilities in terms of unknown parameters – learn parameters (Session III)\nRefine model based on how well it fits data (Session IV)"
  },
  {
    "objectID": "ECD.html#cup-and-cap-notation",
    "href": "ECD.html#cup-and-cap-notation",
    "title": "ECD Intro",
    "section": "Cup and Cap notation",
    "text": "Cup and Cap notation\nIn probability theory, events are sets (sets of balls in the urn).\nLet \\(A\\) and \\(B\\) be two events\n\nUnion: Either \\(A\\) or \\(B\\) occurs \\[ A \\cup B \\qquad A \\vee B \\]\nIntersection: Both \\(A\\) and \\(B\\) occur \\[ A \\cap B \\qquad A \\wedge B \\]\n\nSometimes also just \\(\\Pr(A,B)\\)\n\nComplement: Not \\(A\\) \\[ \\neg A \\qquad \\overline{A} \\]\n\n\\(\\Pr(\\overline{A}) = 1- \\Pr(A)\\)$"
  },
  {
    "objectID": "ECD.html#independence",
    "href": "ECD.html#independence",
    "title": "ECD Intro",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\\[ \\Pr(B) = \\Pr(B|A) = \\Pr(B|\\overline{A}) \\] \\[ \\Pr(A) = \\Pr(A|B) = \\Pr(A|\\overline{B}) \\] \\[ \\Pr(A \\cap B) = \\Pr(A|B)\\Pr(B) = \\Pr(A)\\Pr(B) \\]\n\nKnowing \\(A\\) provides no information about \\(B\\) and vise versa."
  },
  {
    "objectID": "ECD.html#accident-proneness-feller-1968",
    "href": "ECD.html#accident-proneness-feller-1968",
    "title": "ECD Intro",
    "section": "Accident Proneness (Feller, 1968)",
    "text": "Accident Proneness (Feller, 1968)\n\nDriving Skill: 5/6 Normal, 1/6 Accident Prone\nProbability of an accident in a given year\n\n1/100 for Normal drivers\n1/10 for Accident prone drivers\n\nAccidents happen independently in each year\nWhat is the probability a randomly chosen driver will have an accident in Year 1?\nGiven a driver had an accident in Year 1, what is probability of accident in Year 2?"
  },
  {
    "objectID": "ECD.html#accident-proneness-year-1",
    "href": "ECD.html#accident-proneness-year-1",
    "title": "ECD Intro",
    "section": "Accident Proneness (Year 1)",
    "text": "Accident Proneness (Year 1)\nWhat is the probability a randomly chosen driver will have an accident in Year 1? Year 2?\n\\(\\Pr(Y_i)\\). – Prob of accident in a given year.\n\\[ \\Pr(A_i) = \\Pr(A_i|N)\\Pr(N) +\n\\Pr(A_i|\\overline{N})\\Pr(\\overline{N}) \\]\n\nDrivingSkill &lt;- c(N=5/6,A=1/6)\nAccLike &lt;- cbind(Yes=c(N=1/100,A=1/10),No=c(N=99/100,A=9/10))\nYear1 &lt;- sweep(AccLike,1,DrivingSkill,\"*\")\nYear1\n\n          Yes    No\nN 0.008333333 0.825\nA 0.016666667 0.150\n\nsum(Year1[,\"Yes\"])\n\n[1] 0.025"
  },
  {
    "objectID": "ECD.html#accident-proneness-year-ii",
    "href": "ECD.html#accident-proneness-year-ii",
    "title": "ECD Intro",
    "section": "Accident Proneness (Year II)",
    "text": "Accident Proneness (Year II)\nGiven a driver had an accident in Year 1, what is probability of accident in Year 2?\n\\[ \\begin{array}{rcl}\n\\Pr(A_1 \\cap A_2) &=& \\Pr(A_1 \\cap A_2|N)\\Pr(N) + \\Pr(A_1 \\cap\nA_2|\\overline{N}) \\Pr(\\overline{N}) \\\\\n  &=& \\Pr(A_1|N)\\Pr(A_2|N)\\Pr(N) + \\Pr(A_1|\\overline{N})\n\\Pr(A_2|\\overline{N}) \\Pr(\\overline{N})\n   \\end{array} \\]\n\nAcc2Like &lt;- AccLike\nAcc2Like[,\"Yes\"] &lt;- AccLike[,\"Yes\"]^2\nAcc2Like[,\"No\"] &lt;- 1 -Acc2Like[,\"Yes\"]\nYear12 &lt;- sweep(Acc2Like,1,DrivingSkill,\"*\")\nYear12\n\n           Yes      No\nN 8.333333e-05 0.83325\nA 1.666667e-03 0.16500\n\nsum(Year12[,\"Yes\"])\n\n[1] 0.00175"
  },
  {
    "objectID": "ECD.html#accident-prone-chain",
    "href": "ECD.html#accident-prone-chain",
    "title": "ECD Intro",
    "section": "Accident Prone (Chain)",
    "text": "Accident Prone (Chain)\n\\(\\Pr(Y_2 | Y_1)\\) – Accident in 2nd year given accident in first year.\n\nsum(Year12[,\"Yes\"])/sum(Year1[,\"Yes\"])\n\n[1] 0.07"
  },
  {
    "objectID": "ECD.html#explanation",
    "href": "ECD.html#explanation",
    "title": "ECD Intro",
    "section": "Explanation",
    "text": "Explanation\n\\(\\Pr(S=\\text{normal}|A_i)\\) – Probability in normal category given accident.\n\n\n\nAccident Proneness Graph"
  },
  {
    "objectID": "ECD.html#conditional-independence",
    "href": "ECD.html#conditional-independence",
    "title": "ECD Intro",
    "section": "Conditional Independence",
    "text": "Conditional Independence\n\nConditional Independence: $(Y_1,Y_2|S) = (Y_1|S) (Y_2|S) $\nYears are marginally dependent.\nSeparation in graph tells the story.\nInformation flows from from Year1 to Driving Skill to Year2"
  },
  {
    "objectID": "ECD.html#another-example",
    "href": "ECD.html#another-example",
    "title": "ECD Intro",
    "section": "Another Example",
    "text": "Another Example\n\n\n\nCatching Covid on a Train Ride"
  },
  {
    "objectID": "ECD.html#competing-explanations",
    "href": "ECD.html#competing-explanations",
    "title": "ECD Intro",
    "section": "Competing Explanations",
    "text": "Competing Explanations\n\n\n\nCompeting Explanations\n\n\n\nSkill1 and Skill2 are (a priori) independent in population\nTask X requires both skills (conjunctive model)\nAnswer the following questions:\n\nWhat is posterior of Skill2 after learning X=False, and Skill1=High?\nWhat is posterior of Skill1 after learning X=False, and Skill2=High?\nWhat is true of joint posterior of Skill1 and Skill2 after learning X=False?"
  },
  {
    "objectID": "ECD.html#d-separation-example",
    "href": "ECD.html#d-separation-example",
    "title": "ECD Intro",
    "section": "D-separation Example",
    "text": "D-separation Example\n\n\n\nD-separation example\n\n\n\\(B\\) and \\(C\\) are independent if \\(A\\) is known and all of \\(D\\), \\(E\\) or \\(F\\) are not known.\n\\(D\\) is independent of \\(F\\) if \\(E\\) is known."
  },
  {
    "objectID": "ECD.html#d-separation-exercise",
    "href": "ECD.html#d-separation-exercise",
    "title": "ECD Intro",
    "section": "D-Separation Exercise",
    "text": "D-Separation Exercise\n\n\n\nD-separation Exercise\n\n\n\nAre A and C independent if\n\nWe have observed no other variables?\n\nWhat could we condition on to make A and C independent?\n\nWe have observed F and H?\n\nWhat else could we condition on to make A and C independent?\n\nWe have observed G ?\n\nWhat else could we condition on to make A and C independent?"
  },
  {
    "objectID": "ECD.html#building-up-complex-networks-irt",
    "href": "ECD.html#building-up-complex-networks-irt",
    "title": "ECD Intro",
    "section": "Building Up Complex Networks: IRT",
    "text": "Building Up Complex Networks: IRT\n\n\n\nIRT Graphical Model\n\n\nFor example, in IRT, item responses are conditionally independent given \\(\\theta\\):\n\\[ p(X_1,\\ldots,X_J,\\theta) = p(\\theta) \\prod_{j=1}^{J} p(X_j|\\theta)\\]"
  },
  {
    "objectID": "ECD.html#bayes-net",
    "href": "ECD.html#bayes-net",
    "title": "ECD Intro",
    "section": "Bayes net",
    "text": "Bayes net\n\n\n\nA Bayesian Network\n\n\nOne factor for each node in graph\nThis factor is conditioned on parents in graph\n“Prior” nodes have no parents\n\\[p(A)p(B)p(C|A\\,B)p(D|C)p(E|C)p(F|D\\,E) = p(A\\,B\\,C\\,D\\,E\\,F)\\]\nDigraph must be acyclic"
  },
  {
    "objectID": "ECD.html#activity-2-build-a-bayes-net",
    "href": "ECD.html#activity-2-build-a-bayes-net",
    "title": "ECD Intro",
    "section": "Activity 2: Build a Bayes Net",
    "text": "Activity 2: Build a Bayes Net\nPick one of the tasks you created and build an a Bayes net in Netica:\nProficiency variables, their possible values\nObservable variables, their possible values\nConditional probabilities between Proficiency variables and Observable variables\nAdd your observables to the proficiency model you made in Netica"
  },
  {
    "objectID": "Heart.html",
    "href": "Heart.html",
    "title": "Heart Disease Example",
    "section": "",
    "text": "This example is based on a study by (janosi1989?) and recorded in the UCI Machine Learning Repository ((Murphy1992?)). The direct link is: https://archive.ics.uci.edu/dataset/45/heart+disease\nMadigan & Almond (1995) used it as an extended example."
  },
  {
    "objectID": "Heart.html#bayesian-network",
    "href": "Heart.html#bayesian-network",
    "title": "Heart Disease Example",
    "section": "Bayesian network",
    "text": "Bayesian network\n\nlibrary(PNetica)\nsess &lt;- NeticaSession()\nstartSession(sess)\n\nNetica 6.07 Linux (AFCl64), (C) 1992-2019 Norsys Software Corp.\n\nNetica is operating without a password; there are some limitations.\n\n\nA saved version of the network can be found Heart/Heart.dne\n\n\n\nHeart Disease Model"
  },
  {
    "objectID": "Heart.html#discrete-and-continuous-nodes",
    "href": "Heart.html#discrete-and-continuous-nodes",
    "title": "Heart Disease Example",
    "section": "Discrete and Continuous nodes",
    "text": "Discrete and Continuous nodes\nNote that some of the nodes have a number, plus or minus a second number.\nThis is the Expected value, plus or minus the standard deviation.\nThis can be done in one of two ways.\n\nAdd State levels to a discrete node.\n\nSee Colored-Flor and Health-State` (which represents number of blood vessels with high levels of blockage).\nEach state is assigned a number. (Note Netica state names must start with a letter).\n\nTake a continuous node and cut it into pieces\n\nSee Age, Rest-BP, Max-Heart-Rate, Chol and Old-Peak\nThe levels define the boundaries between categories.\nThere is one more boundary than there is categories (like the R function cut).\nInf and -Inf are legal cut points.\n\n\nNote: Netica deals with continuous nodes by discritizing them.\nIf all nodes in a Bayesian network are discrete, the propagation algorithms involve only sums not integrals.\nFor graphs with a mixture of continuous and discrete variables, the propagation algorithms may involve integrals that can’t be solved analytically."
  },
  {
    "objectID": "Heart.html#functional-dependencies",
    "href": "Heart.html#functional-dependencies",
    "title": "Heart Disease Example",
    "section": "Functional Dependencies",
    "text": "Functional Dependencies\nLook at the node Healty?\nThis is a reduction of the Health-State variable, collapsing states S1 through S4 into the single category No.\nThe conditional probability table only has 0 or 1 as entries.\nCan be represented with a table:\n\n\n\nHealth-State\nHealthy?\n\n\n\n\nHealthy\nYes\n\n\nS1\nNo\n\n\nS2\nNo\n\n\nS3\nNo\n\n\nS4\nNo\n\n\n\nThis is really useful for conditioning on Healthy? = No."
  },
  {
    "objectID": "Heart.html#inferences",
    "href": "Heart.html#inferences",
    "title": "Heart Disease Example",
    "section": "Inferences",
    "text": "Inferences\nStart by loading the network.\nNote, need to pass reference to NeticaSession to ReadNetworks.\nCalling NetworkAllNodes builds R objects which point at internal Netica objects.\nNeed to redo these steps every time R session is restarted.\n\nheart &lt;- ReadNetworks(\"Heart/Heart.dne\",sess)\nheart.nodes &lt;- NetworkAllNodes(heart)\n\nA NeticaBN or NeticaNode object is active if it points to an object inside of the Netica heap.\nSaving/restorting the R object will disconnect it from its corresponding Netica object.\n\n\n\n\n\n\nNote\n\n\n\n\n\nNeticaSession and NeticaNetwork objects wrap R environments which contain the networks and nodes respectively. So can access with $ operator.\n\nsess$nets$Heart\n\nNetica Network named  Heart \n  Network is currently active.\n  Nodes:  Age ca chol CP exang fbs ...\n    and  9 others.\n\nheart$nodes$Age\n\nContinuous Netica Node named  Age in network  Heart \n  Node is currently active.\n\n\n\n\n\n\nBeliefs\nThe basic function for querying the network is NodeBelief. This gives the marginal probability of the node.\nNote that the network needs to be compiled before doing inference.\n\nCompileNetwork(heart)\nhealth &lt;- NetworkFindNode(heart,\"health\")\nbaseline &lt;- NodeBeliefs(health)\nprint(baseline,digits=2)\n\nHealthy      S1      S2      S3      S4 \n  0.536   0.182   0.120   0.117   0.045 \n\n\nThis is the unconditional probability. To get the conditional probability, we need to add some evidence.\nAs we have mapped the states to numeric values we can also get the mean and variance.\n\nNodeExpectedValue(health)\n\n[1] 0.9545453\nattr(,\"std_dev\")\n[1] 1.237094\n\n\n\n\nFirst pass data\nJames Goodfellow is a 55 year old man who comes to the doctor complaining of typical anginal pain.\nThe NodeFinding function is used to set (or get) the value of a node. As Age is a continuous node, its finding can be entered directly with NodeValue\n\nNodeFinding(heart$nodes$Sex) &lt;- \"Male\"\nNodeFinding(heart$nodes$CP) &lt;- \"TypicalAnginal\"\nNodeValue(heart$nodes$Age) &lt;- 55\nNodeFinding(heart$nodes$Age)\n\n  between50and60 \n\"between50and60\" \n\n\nThe value of the beliefs for the target mode have now changed.\n\noffice &lt;- NodeBeliefs(health)\nprint(rbind(baseline=baseline,office=office),digits=2)\n\n         Healthy   S1   S2    S3    S4\nbaseline    0.54 0.18 0.12 0.117 0.045\noffice      0.49 0.29 0.11 0.039 0.061\n\n\n\n\nBlocked Evidence\nAssume that the nurse takes Mr. Goodfellows resting blood pressure and it is 130.\n\nNodeValue(heart$nodes$trestbps) &lt;- 130\nbp &lt;- NodeBeliefs(health)\nprint(rbind(baseline=baseline,office=office,bp=bp),digits=2)\n\n         Healthy   S1   S2    S3    S4\nbaseline    0.54 0.18 0.12 0.117 0.045\noffice      0.49 0.29 0.11 0.039 0.061\nbp          0.49 0.29 0.11 0.039 0.061\n\n\nWhy didn’t it change?\nLook closely at the graph. The path between Rest-BP and Health-State goes through Age which is known. Therefore, the resting blood pressure provides no additional information.\nThe function RetractNodeFinding clears it.\n\nRetractNodeFinding(heart$nodes$trestbps)\n\n\n\nVirutal Evidence\nSuppose that the doctor is not able to classify the Mr. Goodfellow’s complaint other than to say it is not Asymptomatic. The EnterNegativeFindings function allows eliminating one of the findings.\n\nRetractNodeFinding(heart$nodes$CP)\nbefore &lt;- NodeBeliefs(heart$nodes$CP)\nEnterNegativeFinding(heart$nodes$CP,\"Asymptomatic\")\nafter &lt;- NodeBeliefs(heart$nodes$CP)\nprint(cbind(before=before, after=after),digits=3)\n\n                before after\nTypicalAnginal  0.0827 0.171\nAtypicalAnginal 0.1509 0.311\nNonAnginal      0.2511 0.518\nAsymptomatic    0.5153 0.000\n\n\nThe probability is now spread out over the other categories.\nCan eliminate more than one categories.\n\nRetractNodeFinding(heart$nodes$CP)\nbefore &lt;- NodeBeliefs(heart$nodes$CP)\nEnterNegativeFinding(heart$nodes$CP,c(\"Asymptomatic\",\"NonAnginal\"))\nafter &lt;- NodeBeliefs(heart$nodes$CP)\nprint(cbind(before=before, after=after),digits=3)\n\n                before after\nTypicalAnginal  0.0827 0.354\nAtypicalAnginal 0.1509 0.646\nNonAnginal      0.2511 0.000\nAsymptomatic    0.5153 0.000\n\n\nFinally, if the evidence from a sensor is uncertain, a likelihood can be entered.\nSuppose that the technician doing the thalium stress test is uncertain about the result. They are pretty sure (80%) there is a defect, but they are uncertain as to whether it is fixed or reversible.\nThis can be expressed as a likelihood: c(.2,.4,.4) and can be set using NodeLikelihood\n\nNodeFinding(heart$nodes$CP) &lt;- \"TypicalAnginal\"  ## Return to previous state.\nNodeLikelihood(heart$nodes$thal) &lt;- c(.2,.4,.4)\nthalTest &lt;- NodeBeliefs(health)\nprint(rbind(baseline=baseline,office=office,thaliumStressTest=thalTest),digits=3)\n\n                  Healthy    S1    S2     S3     S4\nbaseline            0.536 0.182 0.120 0.1169 0.0455\noffice              0.494 0.291 0.114 0.0393 0.0610\nthaliumStressTest   0.419 0.322 0.137 0.0476 0.0738\n\n\n\n\nLikelihood\nThe first pass of the propagation algorithm calculates the probability of all of the observed (including vitrual observations) findings, that is the likelihood of the observations.\nThe function FindingsProbablity extracts this.\n\nFindingsProbability(heart)\n\n[1] 0.007482985\n\n\nFor contrast, lets retract the findings and move back to the baseline.\n\nRetractNetFindings(heart)\nFindingsProbability(heart)\n\n[1] 1"
  },
  {
    "objectID": "Heart.html#weight-of-evidence-balance-sheet",
    "href": "Heart.html#weight-of-evidence-balance-sheet",
    "title": "Heart Disease Example",
    "section": "Weight of Evidence balance sheet",
    "text": "Weight of Evidence balance sheet\nHow much does each observation contribute to the conclusion that James Goodfellow is healthy.\nOne measure is the weight of evidence (WOE; Good (1950)).\n\\[\nWOE(H:E) = \\log \\frac{P(H|E)}{P(\\neg H|E)} - \\log \\frac{P(H)}{P(\\neg H)} =\n\\log \\frac{P(E|H)}{P(E|\\neg H)}\n\\]\nNote that this can be calculated by simply differencing the probability of the target node before and after.\n\nConditional Weight of Evidence\nAs the value of the other nodes changes, the WOE will as well.\nThe conditional weight of evidence is defined by:\n\\[\nWOE(H:E_1|E_2) = \\log \\frac{P(H|E_1,E_2)}{P(\\neg H|E_1,E_2)} - \\log \\frac{P(H|E_1)}{P(\\neg H|E_1)} =\n\\log \\frac{P(E_2|H,E_1)}{P(E_2|\\neg H,E_1)}\n\\]\nNote that these add:\n\\[ WOE(H: E_1, E_2) = WOE(H: E_1) + WOE(H: E_2|E_1) \\]\nWe can use this to evaluate evidence as it come in\n\n\nExample\n\njgoodfellow &lt;- list(\n  Sex=\"Male\",\n  Age=55,\n  CP=\"TypicalAnginal\",\n  trestbps=130,\n  chol=255,\n  fbs=\"high\",\n  exang=\"No\",\n  restecg=\"Normal\")  \n\nBuild up a history\n\nRetractNetFindings(heart)\nhealthy &lt;- NetworkFindNode(heart,\"healthy\")\nbaseline &lt;- NodeBeliefs(healthy)\nhist &lt;- matrix(baseline,nrow=1,dimnames=list(c(\"*baseline*\"),names(baseline)))\nfor (i in 1:length(jgoodfellow)) {\n  nodename &lt;- names(jgoodfellow)[i]\n  nodeval &lt;- jgoodfellow[[i]]\n  node &lt;- NetworkFindNode(heart,nodename)\n  if (is.numeric(nodeval)) {\n    NodeValue(node) &lt;- nodeval\n  } else {\n    NodeFinding(node) &lt;- nodeval\n  }\n  event &lt;- paste(nodename,nodeval,sep=\"=\")\n  probs &lt;- NodeBeliefs(healthy)\n  hist &lt;- rbind(hist,probs)\n  rownames(hist)[i+1] &lt;- event\n}\nhist\n\n                        Yes        No\n*baseline*        0.5357143 0.4642856\nSex=Male          0.4700126 0.5299874\nAge=55            0.4041534 0.5958466\nCP=TypicalAnginal 0.4942228 0.5057772\ntrestbps=130      0.4942228 0.5057772\nchol=255          0.4056582 0.5943418\nfbs=high          0.3879469 0.6120530\nexang=No          0.3879469 0.6120530\nrestecg=Normal    0.5714988 0.4285012\n\n\nThe function woe calculates the joint WOE of all of the nodes.\n\nwoe(heart.nodes[names(jgoodfellow)],healthy,\"Yes\")\n\n[1] 6.291509\n\n\nThe function CPTtools::woeHist provides a history of the changes.\n\njgoodHist &lt;- woeHist(hist,\"Yes\",\"No\")\nprint(jgoodHist,digits=2)\n\n         Sex=Male            Age=55 CP=TypicalAnginal      trestbps=130 \n            -11.4             -11.6              15.9               0.0 \n         chol=255          fbs=high          exang=No    restecg=Normal \n            -15.6              -3.2               0.0              32.3 \n\n\n\n\nWeight of Evidence Balance Sheet\n(madigan1997?) provides a nice graphical summary. The function CPTtools::woeBal generates this.\n\nwoeBal(hist,\"Yes\",\"No\",title=\"Health status of James Goodfellow\")\n\n\n\n\n\n\nOrder sensitivity\n\nThe total weight of evidence does not change with the order in which it is entered.\nThe incremental weight of evidence is order sensitive.\n\nTo illustrate this, enter the blood pressure before entering the sex.\n\nRetractNetFindings(heart)\nhealthy &lt;- NetworkFindNode(heart,\"healthy\")\nbaseline &lt;- NodeBeliefs(healthy)\nhist1 &lt;- matrix(baseline,nrow=1,dimnames=list(c(\"*baseline*\"),names(baseline)))\nfor (i in c(4,1:3,5:length(jgoodfellow))) {\n  nodename &lt;- names(jgoodfellow)[i]\n  nodeval &lt;- jgoodfellow[[i]]\n  node &lt;- NetworkFindNode(heart,nodename)\n  if (is.numeric(nodeval)) {\n    NodeValue(node) &lt;- nodeval\n  } else {\n    NodeFinding(node) &lt;- nodeval\n  }\n  event &lt;- paste(nodename,nodeval,sep=\"=\")\n  probs &lt;- NodeBeliefs(healthy)\n  hist1 &lt;- rbind(hist1,probs)\n  rownames(hist1)[nrow(hist1)] &lt;- event\n}\njgoodHist1 &lt;- woeHist(hist1,\"Yes\",\"No\")\nprint(cbind(order0=jgoodHist,order1=jgoodHist1),digits=3)\n\n                  order0   order1\nSex=Male          -11.43   0.0178\nAge=55            -11.64 -11.4047\nCP=TypicalAnginal  15.86 -11.6867\ntrestbps=130        0.00  15.8552\nchol=255          -15.58 -15.5840\nfbs=high           -3.21  -3.2141\nexang=No            0.00   0.0000\nrestecg=Normal     32.31  32.3080"
  },
  {
    "objectID": "Heart.html#test-selection-and-value-of-information",
    "href": "Heart.html#test-selection-and-value-of-information",
    "title": "Heart Disease Example",
    "section": "Test Selection and Value of Information",
    "text": "Test Selection and Value of Information\nSuppose the cardiologist is interested in determining the health status of Mr. Goodfellow. There are a number of information measures that can be used to evaluate the tests (corresponding to nodes in the network).\n\nExpected Weight of Evidence\nGood & Card (1971) propose using the expected weight of evidence as a measure of potential information.\n\\[\nEWOE (H: E) = \\sum_{e \\in E} W(H:e) P(e|H).\n\\]\nNote this is the expectation wrt the conditional distribution given the hypothesis (using the marginal distribution will have expectation).\nMadigan & Almond (1995) offers a simple algorithm for calculating the EWOE for a number of nodes at once.\n\nEnter any known findings into the network.\nEnter a finding (or virtual evidence) that the Hypothesis is true.\n\n\nCalculate \\(P(E_i|H)\\) for all \\(E_i\\) (just reading NodeBeliefs(\\(E_i\\)))\n\n\nReplace this with a finding that the hypothesis is false\n\n\nCalculate \\(P(E_i|\\neg H)\\) for all \\(E_i\\)\n\n\nRectract the hypothesis finding\nCalcualte EWOE for all \\(E_i\\)\n\nThe function ewoe does this for us.\n\nenodes &lt;- heart.nodes[c(\"thal\", \"ca\", \"thalach\", \"slope\", \"oldpeak\")]\nprint(ewoe(enodes,healthy,\"Yes\"),digits=2)\n\n   thal      ca thalach   slope oldpeak \n  25.87   21.14    0.62   12.87   11.07 \n\n\n\n\nMutual Information\nEWOE only works with binary hypotheses. A similar measure which works with all states is the mutual information.\n\\[MI(X,Y) = \\sum_{x,y} Pr(x,y) \\log \\frac{\\Pr(x,y)}{Pr(x)Pr(y)}\\] To use it as test selection metric, we select a target node, and then calculate the mutual information with all of the potential evidence nodes.\nIn the Netica GUI, you can do this by selecting the target node, and then the menu item `Network &gt; Sensitivity to Findings …”\nIn the API, we call the MutualInfo function for each target.\n\nprint(MutualInfo(health,enodes),digits=3)\n\n   thal      ca thalach   slope oldpeak \n 0.2056  0.2039  0.0101  0.1151  0.1181 \n\n\n\n\nReduction in Variance\nAs the target node has numeric values, another meaningful measure is the reduction in variance. This is also reported in the Netica report, or can be calculated with the VarianceOfReal function.\n\nprint(VarianceOfReal(health,enodes))\n\n      thal         ca    thalach      slope    oldpeak \n0.24554364 0.27209573 0.01638281 0.15160286 0.17215686"
  },
  {
    "objectID": "Heart.html#learning-models-from-data",
    "href": "Heart.html#learning-models-from-data",
    "title": "Heart Disease Example",
    "section": "Learning Models from Data",
    "text": "Learning Models from Data\nBasic idea. Sufficient statistic for conditional probability table is contingency table with parent and child variable. We can calculate this from a data set.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.5     ✔ purrr   1.0.1\n✔ tibble  3.1.6     ✔ dplyr   1.0.8\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nRead in the Cleveland data set from (janosi1989?). The heart.features is information about the coding from the web site.\n\nheart.features &lt;- list(\n  Age=c(0,below50=50,between50and60=60,above60=100),\n  Sex=c(Male=1,Female=0),\n  CP=c(TypicalAnginal=1, AtypicalAnginal=2, NonAnginal=3, Asymptomatic=4),\n  trestbps=c(0,low=120,moderate=140,high=300),\n  chol=c(0,Normal=200, Moderate=250, High=300, VeryHigh=750),\n  fbs=c(high=1,normal=0),\n  restecg=c(Normal=0, STTabnormality=0, LeftVentricularHypertrophy=2),\n  thalach=c(0,Low=130,Moderate=160,High=500),\n  exang=c(Yes=1, No=0),\n  oldpeak=c(-10,Zero=1,Low=2,High=10),\n  slope=c(down=1, flat=2, up=3),\n  ca=c(Zero=0, One=1, Two=2, Three=3, Four=4),\n  thal=c(Normal=3, FixedDefect=6, ReversibleDefect=7),\n  health=c(Healthy=0, S1=1, S2=2, S3=3, S4=4)\n)\nheart.cont &lt;- sapply(heart.features,function(states) length(names(states)[1])==0L)\n\ncleveland &lt;- read_csv(\"Heart/processed.cleveland.csv\",\n                      col_names=names(heart.features),\n                      col_types=str_pad(\"\",\n                                       width=length(heart.features),\n                                       pad=\"n\"),\n                      na=\"?\")\n\nfor (var in names(heart.features)) {\n  states &lt;- heart.features[[var]]\n  if (names(states)[1]==\"\") {\n    ## Continuous node, use cut to create variable.\n    cleveland[[var]] &lt;- cut(cleveland[[var]],as.numeric(states),\n                            labels=names(states)[-1])\n  } else {\n    ## Discrete node, use factor.\n    cleveland[[var]] &lt;- factor(as.integer(cleveland[[var]]),\n                               levels=as.integer(states),labels=names(states))\n  }\n}\n\ncleveland\n\n# A tibble: 303 × 14\n   Age      Sex   CP    trestbps chol  fbs   restecg thalach exang oldpeak slope\n   &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;   &lt;fct&gt;\n 1 above60  Male  Typi… high     Mode… high  LeftVe… Modera… No    High    up   \n 2 above60  Male  Asym… high     High  norm… LeftVe… Low     Yes   Low     flat \n 3 above60  Male  Asym… low      Mode… norm… LeftVe… Low     Yes   High    flat \n 4 below50  Male  NonA… moderate Mode… norm… Normal  High    No    High    up   \n 5 below50  Fema… Atyp… moderate Mode… norm… LeftVe… High    No    Low     down \n 6 between… Male  Atyp… low      Mode… norm… Normal  High    No    Zero    down \n 7 above60  Fema… Asym… moderate High  norm… LeftVe… Modera… No    High    up   \n 8 between… Fema… Asym… low      Very… norm… Normal  High    Yes   Zero    down \n 9 above60  Male  Asym… moderate High  norm… LeftVe… Modera… No    Low     flat \n10 between… Male  Asym… moderate Mode… high  LeftVe… Modera… Yes   High    up   \n# … with 293 more rows, and 3 more variables: ca &lt;fct&gt;, thal &lt;fct&gt;,\n#   health &lt;fct&gt;\n\n\n\nBuild network structure.\nFor this example, I’ve saved out the network structure, both the node positions and the parents of each node. Read this in:\n\nheart.structure &lt;- dget(\"Heart/heartinfo.R\")\n\nMake a new network\n\nheart1 &lt;- GetNamedNetworks(\"Heart1\",sess)\nif (!is.null(heart1)) DeleteNetwork(heart1)\nheart1 &lt;- CreateNetwork(\"Heart1\",sess)\nNetworkTitle(heart1) &lt;- \"Heart example built from Cleveland data.\"\n\n\nfor ( nodename in names(heart.features)) {\n  states &lt;- heart.features[[nodename]]\n  if (all(names(states)!=\"\")) {\n    ## Discrete Node\n    newnode &lt;- NewDiscreteNode(heart1,nodename,names(states))\n    NodeLevels(newnode) &lt;- as.numeric(states)\n  } else {\n    newnode &lt;- NewContinuousNode(heart1,nodename)\n    NodeLevels(newnode) &lt;- as.numeric(states)\n    NodeStates(newnode) &lt;- names(states)[-1]\n  }\n  ## Fix properties\n  NodeTitle(newnode) &lt;- heart.structure[[nodename]]$title\n  NodeVisPos(newnode) &lt;- heart.structure[[nodename]]$pos\n}  \nheart1.nodes &lt;- NetworkAllNodes(heart1)\n\nThe easiest way of inputting the structure is to look at the list of parents for all nodes.\n\nfor (nodename in names(heart.structure)) {\n  parnames &lt;- heart.structure[[nodename]]$parents\n  if (length(parnames)==0L) next\n  node &lt;- heart1.nodes[[nodename]]\n  if (is.null(node)) next\n  parents &lt;- heart1.nodes[parnames]\n  NodeParents(node) &lt;- parents\n}\n\n\n\nNode Sets\nNode sets allow variables to be tagged according to how they are used.\n\nNodeSets(heart1$nodes$health) &lt;- \"Target\"\nNetworkNodesInSet(heart1,\"office\") &lt;-\n  heart1.nodes[c(\"Age\",\"Sex\",\"CP\",\"trestbps\",\"chol\",\"fbs\")]\nNetworkNodesInSet(heart1,\"lab\") &lt;-\n  heart1.nodes[c(\"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\")]\n\n\n\nBuilding CPTs from tables\n\nAge &lt;- heart1.nodes[[\"Age\"]]\nparnames &lt;- ParentNames(Age)\nAgeTab &lt;- table(cleveland[c(parnames,\"Age\")])\nAgeTab\n\n, , Age = below50\n\n          health\nthalach    Healthy S1 S2 S3 S4\n  Low            1  3  2  3  1\n  Moderate      16  7  3  4  0\n  High          48  4  0  1  1\n\n, , Age = between50and60\n\n          health\nthalach    Healthy S1 S2 S3 S4\n  Low            7  7  3 11  1\n  Moderate      26 12 12  4  3\n  High          31  8  2  2  1\n\n, , Age = above60\n\n          health\nthalach    Healthy S1 S2 S3 S4\n  Low            8  3  6  4  3\n  Moderate      22  8  7  4  2\n  High           5  3  1  2  1\n\n\nThere are basically two representations for CPTs, arrays CPTtools::CPA and data frames, CPTtools::CPF\n\nas.CPF(AgeTab)\n\n    thalach  health Age.below50 Age.between50and60 Age.above60\n1       Low Healthy           1                  7           8\n2  Moderate Healthy          16                 26          22\n3      High Healthy          48                 31           5\n4       Low      S1           3                  7           3\n5  Moderate      S1           7                 12           8\n6      High      S1           4                  8           3\n7       Low      S2           2                  3           6\n8  Moderate      S2           3                 12           7\n9      High      S2           0                  2           1\n10      Low      S3           3                 11           4\n11 Moderate      S3           4                  4           4\n12     High      S3           1                  2           2\n13      Low      S4           1                  1           3\n14 Moderate      S4           0                  3           2\n15     High      S4           1                  1           1\n\n\nAdd 1/(Number of states) to get rid of zeros and then normalize\n\nnormalize(as.CPF(AgeTab+1/NodeNumStates(Age)))\n\n    thalach  health Age.below50 Age.between50and60 Age.above60\n1       Low Healthy  0.07843137          0.4313725   0.4901961\n2  Moderate Healthy  0.25128205          0.4051282   0.3435897\n3      High Healthy  0.56862745          0.3686275   0.0627451\n4       Low      S1  0.23809524          0.5238095   0.2380952\n5  Moderate      S1  0.26190476          0.4404762   0.2976190\n6      High      S1  0.27083333          0.5208333   0.2083333\n7       Low      S2  0.19444444          0.2777778   0.5277778\n8  Moderate      S2  0.14492754          0.5362319   0.3188406\n9      High      S2  0.08333333          0.5833333   0.3333333\n10      Low      S3  0.17543860          0.5964912   0.2280702\n11 Moderate      S3  0.33333333          0.3333333   0.3333333\n12     High      S3  0.22222222          0.3888889   0.3888889\n13      Low      S4  0.22222222          0.2222222   0.5555556\n14 Moderate      S4  0.05555556          0.5555556   0.3888889\n15     High      S4  0.33333333          0.3333333   0.3333333\n\n\n\nAge[] &lt;- normalize(as.CPF(AgeTab+1/NodeNumStates(Age)))\nAge[]\n\n    thalach  health Age.below50 Age.between50and60 Age.above60\n1       Low Healthy  0.07843138          0.4313726   0.4901961\n2  Moderate Healthy  0.25128207          0.4051282   0.3435898\n3      High Healthy  0.56862748          0.3686275   0.0627451\n4       Low      S1  0.23809524          0.5238096   0.2380952\n5  Moderate      S1  0.26190478          0.4404762   0.2976190\n6      High      S1  0.27083334          0.5208333   0.2083333\n7       Low      S2  0.19444445          0.2777778   0.5277778\n8  Moderate      S2  0.14492753          0.5362319   0.3188406\n9      High      S2  0.08333334          0.5833333   0.3333333\n10      Low      S3  0.17543860          0.5964912   0.2280702\n11 Moderate      S3  0.33333334          0.3333333   0.3333333\n12     High      S3  0.22222222          0.3888889   0.3888889\n13      Low      S4  0.22222222          0.2222222   0.5555556\n14 Moderate      S4  0.05555556          0.5555556   0.3888889\n15     High      S4  0.33333334          0.3333333   0.3333333\n\n\nThe NodeProbs function uses the array format.\n\nNodeProbs(Age)\n\n, , Age = below50\n\n          health\nthalach       Healthy        S1         S2        S3         S4\n  Low      0.07843138 0.2380952 0.19444445 0.1754386 0.22222222\n  Moderate 0.25128207 0.2619048 0.14492753 0.3333333 0.05555556\n  High     0.56862748 0.2708333 0.08333334 0.2222222 0.33333334\n\n, , Age = between50and60\n\n          health\nthalach      Healthy        S1        S2        S3        S4\n  Low      0.4313726 0.5238096 0.2777778 0.5964912 0.2222222\n  Moderate 0.4051282 0.4404762 0.5362319 0.3333333 0.5555556\n  High     0.3686275 0.5208333 0.5833333 0.3888889 0.3333333\n\n, , Age = above60\n\n          health\nthalach      Healthy        S1        S2        S3        S4\n  Low      0.4901961 0.2380952 0.5277778 0.2280702 0.5555556\n  Moderate 0.3435898 0.2976190 0.3188406 0.3333333 0.3888889\n  High     0.0627451 0.2083333 0.3333333 0.3888889 0.3333333\n\nattr(,\"class\")\n[1] \"CPA\"   \"array\"\n\n\n\nfor (node in heart1.nodes) {\n  tab &lt;- table(cleveland[c(ParentNames(node),NodeName(node))])\n  node[] &lt;- normalize(as.CPF(tab+1/NodeNumStates(node)))\n}\n\n\n\nThe Healthy? node\nThis is a deterministic node and requires special treatment.\n\nhealthy &lt;- NewDiscreteNode(heart1,\"healthy\") # Default states are Yes, No\nNodeTitle(healthy) &lt;- \"Healthy?\"\nNodeParents(healthy) &lt;- list(heart1$nodes$health)\nNodeVisPos(healthy) &lt;- heart.structure[[\"healthy\"]]$pos\nhealthy[[\"Healthy\"]] &lt;- \"Yes\"\nhealthy[[c(\"S1\",\"S2\",\"S3\",\"S4\")]] &lt;- \"No\"\nhealthy[[]]\n\n   health healthy\n1 Healthy     Yes\n2      S1      No\n3      S2      No\n4      S3      No\n5      S4      No\n\n\nCompiling the network will give a warning if any node lacks a CPT.\n\nCompileNetwork(heart1)\n\nWrite it out.\n\ntmpfile &lt;- tempfile(\"Heart\",fileext=\".dne\")  ## DNE is ascii representations; NETA is binary\nWriteNetworks(heart1,tmpfile)"
  },
  {
    "objectID": "Heart.html#working-with-case-files",
    "href": "Heart.html#working-with-case-files",
    "title": "Heart Disease Example",
    "section": "Working with Case Files",
    "text": "Working with Case Files\nNetica supports case files, which are essentially CSV files. These are easier ways to manage large numbers of cases.\nThe function CaseFileStream() generates a new file stream. The functions ReadFindings() and WriteFindings() read and write to the case stream.\nAs we want the CaseStream to stay open while we are reading, explicitly call OpenCaseStream and CloseCaseStream.\nNote that good programming style requires that the CaseStream be closed when you are done with it. To ensure this, use WithOpenCaseStream(stream,{...}).\n\ncasefile &lt;- tempfile(fileext=\".cas\")\nfilestream &lt;- CaseFileStream(casefile,session=sess)\nWithOpenCaseStream(filestream, {\n  ## Do some stuff\n})\n## Or equivalently use the withr::defer construct\n{\n  OpenCaseStream(filestream)\n  withr::defer(CloseCaseStream(filestream))\n}\n\nNote that the CaseStream is opened in the Netica environment, not the R environment.\n\nGenerating random cases\nStart by generating random cases.\nNote that Netica uses a different random number sequence than R. To make a reproducible sequence, we need to create a NeticaRNG object and give it a seed. Use the FreeNeticaRNG method to free it when you are done (or use WithNeticaRNG to free it at the end of the computation.\nYou can also associate the RNG with a network using NetworkSetRNG().\n\nCompileNetwork(heart)\nRetractNetFindings(heart)\nrnodes &lt;- NetworkAllNodes(heart)\ncasefile &lt;- tempfile(\"heartrandom\",fileext=\".cas\")\nfilestream &lt;- CaseFileStream(casefile,session=sess)\nrng &lt;- NewNeticaRNG(12345,session=sess)\nN &lt;- 10L\nWithRNG(rng,\n  WithOpenCaseStream(filestream,\n    for (n in 1L:N) {\n      GenerateRandomCase(rnodes,rng=rng)\n      WriteFindings(rnodes,filestream,n)\n      RetractNetFindings(heart)\n    }))\nranCases &lt;- read.CaseFile(casefile,session=sess)\nranCases\n\n   IDnum  health             thal                    restecg    fbs    ca\n1      1 Healthy           Normal                     Normal normal   One\n2      2      S1 ReversibleDefect                     Normal normal  Four\n3      3 Healthy           Normal LeftVentricularHypertrophy normal  Zero\n4      4 Healthy      FixedDefect                     Normal normal Three\n5      5 Healthy ReversibleDefect                     Normal normal  Zero\n6      6 Healthy           Normal                     Normal normal  Zero\n7      7      S1           Normal LeftVentricularHypertrophy normal  Zero\n8      8 Healthy ReversibleDefect                     Normal normal   One\n9      9 Healthy           Normal LeftVentricularHypertrophy normal  Zero\n10    10 Healthy           Normal LeftVentricularHypertrophy normal  Zero\n    thalach slope              CP healthy   oldpeak      Age    Sex    chol\n1  336.8150  down    Asymptomatic     Yes  1.797370 83.09490   Male 336.272\n2  447.8580  down    Asymptomatic      No  1.620750 24.84530   Male 375.803\n3  167.0890    up      NonAnginal     Yes  2.078560  5.79728 Female 344.551\n4   14.9075  down AtypicalAnginal     Yes  1.460710 74.15340   Male 346.944\n5  273.5120  flat      NonAnginal     Yes  0.716837 29.03160   Male 423.564\n6  385.7460  down      NonAnginal     Yes  1.785150 43.65560   Male 448.651\n7  167.2420  down    Asymptomatic      No  2.658360 28.84410   Male 479.814\n8  462.3440    up AtypicalAnginal     Yes -8.350040 30.89620   Male 485.311\n9  485.7980  down      NonAnginal     Yes  1.343940 10.48630 Female 448.566\n10 391.5800  flat      NonAnginal     Yes -6.699450  1.43075   Male 320.405\n   exang trestbps\n1    Yes 141.8620\n2    Yes 136.2370\n3    Yes 132.9230\n4     No 129.0380\n5     No 134.2890\n6     No 135.1820\n7     No 139.9000\n8     No 122.9250\n9     No  27.2462\n10    No 122.2600\n\n\nThe function read.CaseFile reads the generated cases and sets them up as a data frame.\n\n\nConverting data into case file format\nNote that continuous variables are left at their numeric values. Need to reread cleveland data and not do the conversion this time.\n\ncleveland1 &lt;- read_csv(\"Heart/processed.cleveland.csv\",\n                      col_names=names(heart.features),\n                      col_types=str_pad(\"\",\n                                        width=length(heart.features),\n                                        pad=\"n\"),\n                      na=\"?\")\n\nfor (var in names(heart.features)) {\n  states &lt;- heart.features[[var]]\n  if (names(states)[1]==\"\") {\n    ## Continuous node, leave as is create variable.\n  } else {\n    ## Discrete node, use factor.\n    cleveland1[[var]] &lt;- factor(as.integer(cleveland1[[var]]),\n               levels=as.integer(states),labels=names(states))\n  }\n}\nwrite.CaseFile(cleveland1,\"Heart/cleveland.cas\",session=sess)\n\nWhile we are at it, lets convert the Swiss and Hungarian data to case files.\n\nswiss &lt;- read_csv(\"Heart/processed.switzerland.csv\",\n                      col_names=names(heart.features),\n                      col_types=str_pad(\"\",\n                                        width=length(heart.features),\n                                        pad=\"n\"),\n                      na=\"?\")\nhungary &lt;- read_csv(\"Heart/processed.hungarian.csv\",\n                      col_names=names(heart.features),\n                      col_types=str_pad(\"\",\n                                        width=length(heart.features),\n                                        pad=\"n\"),\n                      na=\"?\")\nfor (var in names(heart.features)) {\n  states &lt;- heart.features[[var]]\n  if (names(states)[1]==\"\") {\n    ## Continuous node, leave as is create variable.\n  } else {\n    ## Discrete node, use factor.\n    swiss[[var]] &lt;- factor(as.integer(swiss[[var]]),\n               levels=as.integer(states),labels=names(states))\n    hungary[[var]] &lt;- factor(as.integer(hungary[[var]]),\n               levels=as.integer(states),labels=names(states))\n  }\n}\nwrite.CaseFile(swiss,\"Heart/switzerland.cas\",session=sess)\nwrite.CaseFile(hungary,\"Heart/hungary.cas\",session=sess)\n\n\n\nBulk Scoring and Testing\nBulk Scoring can be done by looping through the case file.\n\nswissStream &lt;- CaseFileStream(\"Heart/switzerland.cas\",sess)\ntestnodes &lt;- c(NetworkNodesInSet(heart,\"office\"),\n               NetworkNodesInSet(heart,\"lab\"))\nWithOpenCaseStream(swissStream, {\n  for (i in 1L:10L) {\n    RetractNetFindings(heart)\n    swissStream &lt;- \n      ReadFindings(testnodes,\n                  swissStream,\n                  ifelse(i==1L,\"FIRST\",\"NEXT\"))\n    cat(\"Case \", i,\"\\n\")\n    print(NodeBeliefs(health),digits=3)\n    if (is.na(getCaseStreamPos(swissStream))) break\n  }\n})\n\nCase  1 \nHealthy      S1      S2      S3      S4 \n 0.6670  0.1447  0.0245  0.0319  0.1319 \nCase  2 \nHealthy      S1      S2      S3      S4 \n 0.3438  0.1901  0.0833  0.2095  0.1733 \nCase  3 \nHealthy      S1      S2      S3      S4 \n 0.3438  0.1901  0.0833  0.2095  0.1733 \nCase  4 \nHealthy      S1      S2      S3      S4 \n 0.3438  0.1901  0.0833  0.2095  0.1733 \nCase  5 \nHealthy      S1      S2      S3      S4 \n 0.5600  0.1681  0.0502  0.1226  0.0992 \nCase  6 \nHealthy      S1      S2      S3      S4 \n 0.5648  0.1808  0.0436  0.1094  0.1014 \nCase  7 \nHealthy      S1      S2      S3      S4 \n 0.8201  0.0730  0.0186  0.0483  0.0400 \nCase  8 \nHealthy      S1      S2      S3      S4 \n 0.8224  0.0781  0.0161  0.0428  0.0406 \nCase  9 \nHealthy      S1      S2      S3      S4 \n 0.3438  0.1901  0.0833  0.2095  0.1733 \nCase  10 \nHealthy      S1      S2      S3      S4 \n 0.3438  0.1901  0.0833  0.2095  0.1733 \n\n\nObviously, what we want to do is compare this to the actual readings. The NetworkTester allows us to do that.\n\nRetractNetFindings(heart)\nswiss.test &lt;- \n  testNetwork(list(health),\n              OpenCaseStream(swissStream))\nsummary(swiss.test)\n\n       ErrorRate  LogLoss QuadraticLoss       kappa         QWK lambda\nhealth 0.8292683 1.827684      0.912821 -0.06385144 -0.03701352  -0.36\n       LinearLambda\nhealth   -0.3445783\n\n\n\n\nLearning CPTs\nWith a case stream we can learn all of the CPTs at once.\n\nlnodes &lt;- c(NetworkNodesInSet(heart1,\"office\"),\n            NetworkNodesInSet(heart1,\"lab\"))\ninvisible(sapply(lnodes,DeleteNodeTable))\nLearnCases(\"Heart/cleveland.cas\",lnodes)\n\nLook at the new values.\nNode experience is how many responses were used in that row of the table.\n\nheart1$nodes$trestbps[]\n\n             Age trestbps.low trestbps.moderate trestbps.high\n1        below50    0.3222222         0.5444444     0.1333333\n2 between50and60    0.1640625         0.5000000     0.3359375\n3        above60    0.1382979         0.3723404     0.4893617\n\nNodeExperience(heart1$nodes$trestbps)\n\nAge\n       below50 between50and60        above60 \n            90            128             94 \n\n\nLearnCases uses the same counting algorithm shown above (building the CPT), so it doesn’t handle missing data.\nLearnCPTs does. There are three methods. I recommend EM\n\ninvisible(sapply(lnodes,DeleteNodeTable))\nLearnCPTs(\"Heart/cleveland.cas\",lnodes, method=\"EM\")\n\nNULL\n\nheart1$nodes$trestbps[]\n\n             Age trestbps.low trestbps.moderate trestbps.high\n1        below50    0.3218391         0.5517241     0.1264368\n2 between50and60    0.1600000         0.5039999     0.3360000\n3        above60    0.1318682         0.3736264     0.4945054\n\nNodeExperience(heart1$nodes$trestbps)\n\nAge\n       below50 between50and60        above60 \n      87.00003      125.00003       91.00003"
  },
  {
    "objectID": "Heart.html#learning-structure-from-data",
    "href": "Heart.html#learning-structure-from-data",
    "title": "Heart Disease Example",
    "section": "Learning Structure from Data",
    "text": "Learning Structure from Data\n\n\n\n\n\n\nFixing the HeartNet to properly use continuous nodes.\n\n\n\n\n\n(This was some code I used to fix and issue with the initial version of the test network which had the continuous nodes coded as discrete.)\n\nheart.info &lt;- lapply(heart.nodes, function (nd) {\n  list(parents = ParentNames(nd),\n       cpt = NodeProbs(nd),\n       pos= NodeVisPos(nd),\n       title=NodeTitle(nd))\n})\n\nNumber of states of Age has changed, need to fix some CPTs.\n\nfixme &lt;- c(\"Age\",\"trestbps\")\nfor (nodename in fixme) {\n  xtab &lt;- as.array(table(cleveland[c(heart.info[[nodename]]$parents,nodename)]))\n  class(xtab) &lt;- c(\"CPA\",\"array\")\n  probs &lt;- normalize(xtab+1/last(dim(xtab)))\n  heart.info[[nodename]]$cpt &lt;- probs\n}\n\n\nfor ( nodename in names(heart.features)) {\n  states &lt;- heart.features[[nodename]]\n  if (all(names(states)!=\"\")) next\n  oldnode &lt;- heart.nodes[[nodename]]\n  ## Save old data\n  title &lt;- NodeTitle(oldnode)\n  descrip &lt;- NodeDescription(oldnode)\n  vispos &lt;- NodeVisPos(oldnode)\n  ## Replace node\n  DeleteNodes(oldnode)\n  newnode &lt;- NewContinuousNode(heart,nodename)\n  ## Fix properties\n  NodeTitle(newnode) &lt;- title\n  NodeDescription(newnode) &lt;- descrip\n  NodeVisPos(newnode) &lt;- vispos\n  ## Fix the states\n  NodeLevels(newnode) &lt;- as.numeric(states)\n  NodeStates(newnode) &lt;- names(states)[-1]\n  ## Fix the list of nodes.\n  heart.nodes[[nodename]] &lt;- newnode\n}\n\n\n## Fix up any set of broken parent relationships.\nfor (nodename in names(heart.info)) {\n  cat(nodename,\"\\n\")\n  node &lt;- heart.nodes[[nodename]]\n  parents &lt;- heart.nodes[heart.info[[nodename]]$parents]\n  NodeParents(node) &lt;- parents\n  NodeProbs(node) &lt;- heart.info[[nodename]]$cpt\n}\n\n\nCompileNetwork(heart)\nWriteNetworks(heart,\"Heart/Heart.dne\")\n\n\nNodeStates(heart$nodes$CP)[4] &lt;- \"Asymptomatic\""
  },
  {
    "objectID": "AltIndex.html",
    "href": "AltIndex.html",
    "title": "Medical Index",
    "section": "",
    "text": "Reworked ECD – Patient Status Model – Evidence Model (Test outcomes) – Assembly Model\nBayes Net Basics\n\n\nNodes and edges\nLocal independence\nCompeting Explanations.\n\n\nSimple Examples\n\n\nL&S example (Lung Cancer)\nCompensatory/Conjunction\nHeart Disease (diagnosis)\n\n\nWeight of Evidence\n\n\nEvidence Spreadsheet\nTest Selection\n\n\nInfluence Diagrams\n\n\nDecisions\nUtility\nPlans\n\n\nDynamic Models\n\n\nDBN\nPOMDP\n\n\nFitting Models to Data\n\n\nFixed Structure\n\nEM appraoches\nMCMC approaches\n\nLearning Structure"
  },
  {
    "objectID": "Session3.html",
    "href": "Session3.html",
    "title": "RNetica",
    "section": "",
    "text": "Bayesian Networks in Educational Assessment\nTutorial\nSession III: __ __ Bayes Net with R\nDuanli Yan, Diego Zapata, ETS\nRussell Almond, FSU\n2021 NCME Tutorial: Bayesian Networks in Educational Assessment\nSESSION __ __ TOPIC __ __ PRESENTERS\nSession 1 : Evidence Centered Design Diego Zapata Bayesian Networks\nSession 2 : Bayes Net Applications Duanli Yan & ACED: ECD in Action Russell Almond\nSession 3 : Bayes Nets with R Russell Almond & Duanli Yan\nSession 4 : Refining Bayes Nets with Duanli Yan & Data Russell Almond\n\nRNetica\n\n\nQuick Start Guide\nScoring A Student\nRNetica Quick Start\n\n\nDownloading\n\nhttp://pluto.coe.fsu.edu/RNetica/\nFour Packages:\n\nRNetica – R to Netica link\nCPTtools – Design patterns for CPTs\nPeanut/PNetica -- Object-Oriented Parameterized Network\n\nSource & binary version (Win 64, Mac OS X)\n\nBinary versions include Netica.dll/libNetica.so\n\nIn RStudio select “Package Archive” rather than CRAN\n\nSource version need to download from http://www.norsys.com/ first\n\nSee INSTALLATION\n\n\n\nRNetica Quick Start\n\n\nLicense\n\nR – GPL-3 (Free and open source)\nRNetica – Artistic (Free and open source)\nNetica.dll/libNetica.so – Commercial (open API, but not open source)\n\nFree Student/Demo version\n\nLimited number of nodes\nLimited usage (education, evaluation of Netica)\n\nPaid version (see http://www.norsys.com/ for price information)\n\nNeed to purchase API not GUI version of Netica\nMay want both (use GUI to visualize networks build in RNetica)\n\n\nCPTtools – Artistic (Free and open source), does not depend on Netica\n\nRNetica Quick Start\n\n\nInstalling the License Key\n\nWhen you purchase a license, Norsys will send you a license key. Something that looks like: “+Course/FloridaSU/Ex15-05-30,120,310/XXXXX” (Where I’ve obscured the last 5 security digits)\nTo install the license key, start R in your project directory and type:\n\n\nNeticaLicenseKey &lt;- “+Course/FloridaSU/Ex15-05-30,120,310/XXXXX”\n\n\nq(“yes”)\n\n\nRestart R and type\n\n\nlibrary(RNetica)\n\n\nIf license key is not installed, then you will get the limited/student mode. Most of these examples will run\n\nRNetica Quick Start\n\n\nThe R heap and the Netica heap\nR and Netica have two different workspaces (memory heaps)\nR workspace is saved and restored automatically when you quick and restart R.\nNetica heap must be reconnected manually.\nRNetica Quick Start\n\n\nActive and Inactive pointers\nWhen RNetica creates/finds a Netica object it creates a corresponding R object\nIf the R object is active then it points to the Netica object, and the Netica object points back at it\nIf the pointer gets broken (saving & restarting R, deleting the network/node) then the R object becomes inactive.\nThe function is.active(nodeOrNet) test to see if the node/net is active\nRNetica Quick Start\n\n\nMini-ACED Proficiency model\nSubset of ACED network (Shute, Hansen & Almond (2008); http://ecd.ralmond.net/ecdwiki/ACED )\nProficiency Model subset:\n\nRNetica Quick Start\n\n\nMini-ACED EM Fragments\nAll ACED tasks were scored correct/incorrect\nEach evidence model is represented by a fragment consisting of observables with stub  edges indicating where it should be adjoined with the network.\n\n\nCommon Ratio Easy\nModel Extend Table Hard\nRNetica Quick Start\n\n\nTask to EM map\nNeed a table to tell us which EM to use with which task\n\n\n\nTask ID\nEM Filename\nX\nY\n\n\n\n\ntCommonRatio1b\nCommonRatioEasyEM\n108\n414\n\n\ntCommonRatio2a\nCommonRatioMedEM\n108\n534\n\n\ntCommonRatio2b\nCommonRatioMedEM\n108\n654\n\n\ntCommonRatio3a\nCommonRatioHardEM\n108\n774\n\n\ntCommonRatio3b\nCommonRatioHardEM\n108\n894\n\n\ntExamplesGeometric1a\nExamplesEasyEM\n342\n294\n\n\ntExamplesGeometric1b\nExamplesEasyEM\n342\n414\n\n\n\n\n\n\n\n\n\nRNetica Quick Start\n\n\nScoring Script\nFollow along using the script found in ScoringScript.R in the miniACED folder.\nDon’t forget to setwd() to the miniACED folder (as it needs to find its networks).\nDon’t forget to set the license key before issuing library(RNetica) command.\nRNetica Quick Start\n\n\nReloading Nets and Nodes\n## Scoring Script\n## Preliminaries\nlibrary(RNetica)\nlibrary(CPTtools)\n## Read in network – Do this every time R is restarted\nprofModel &lt;- ReadNetworks(“miniACEDPnet.dne”)\n## If  profModels  already exists could also use\n## Reconnect nodes – Do this every time R is restarted\nallNodes &lt;- NetworkAllNodes(profModel)\nsgp &lt;- allNodes$SolveGeometricProblems\nprofNodes &lt;- NetworkNodesInSet(profModel,“Proficiencies”)\nRNetica Quick Start\n\n\nAside 1: Node Sets\n\nNetica defines a node set functionality which\n\nAdds a collection of labels (sets) to each node\nDefines a collection of nodes with that label\n\nNetica GUI really only offers the opportunity to color nodes by set\nRNetica can loop over node sets (lists of nodes)\n## Node Sets\nNetworkNodeSets(profModel)\nNetworkNodesInSet(profModel,“pnodes”)\nNodeSets(sgp)\n## These are all settable\nNodeSets(sgp) &lt;- c(NodeSets(sgp),“HighLevel”)\nNodeSets(sgp)\n\nRNetica Quick Start\n\n\nAside 2: RNetica Functions\n## Querying Nodes\nNodeStates(sgp) #List states\nNodeParents(sgp) #List parents\nNodeLevels(sgp) #List numeric values associated with states\nNodeProbs(sgp) # Conditional Probability Table (as array)\nsgp[] # Conditional Probability Table (as data frame)\n## These are all settable (can be used on RHS of &lt;-) for \n## model construction\n## Inference\nCompileNetwork(profModel) #Lightning bolt on GUI\n## Must do this before inference\n## Recompiling an already compiled network is harmless\nRNetica Quick Start\n\n\nAside 2: Inference\n## Enter Evidence by setting values for these functions\nNodeValue(sgp) #View or set the value\nNodeLikelihood(sgp) #Virtual evidence\n## Query beliefs\nNodeBeliefs(sgp) #Current probability (given entered evidence)\nNodeExpectedValue(sgp) #If node has values, EAP\n## These aren’t settable\n## Retract Evidence\nRetractNodeFinding(profNodes$ExamplesGeometric)\nRetractNetFindings(profModel)\nRNetica Quick Start\n\n\nAside 2: Example\n## Enter Evidence\nNodeValue(profNodes$CommonRatio) &lt;- “Medium”\n## Enter Evidence “Not Low” (“High or Medium”)\nNodeLikelihood(profNodes$ExamplesGeometric) &lt;- c(1,1,0)\nNodeBeliefs(sgp) #Current probability (given entered evidence)\nNodeExpectedValue(sgp) #If node has values, EAP\n## Retract Evidence\nRetractNetFindings(profModel)\n## Many more examples\nhelp(RNetica)\nRNetica Quick Start\n\n\nBack to work\nLoad the evidence model table\nRow names are task IDs\nEM column contains evidence model name\nEM filename has suffix “.dne” attached.\n## Read in task-&gt;evidence model mapping\nEMtable &lt;- read.csv(“MiniACEDEMTable.csv”,row.names=1,\nas.is=2) #Keep EM names as strings\nhead(EMtable)\nRNetica Quick Start\n\n\nA student walks into the test center …\n\nStudent gives the name “Fred”\nStudent is the right grade/age for ACED (8th or 9th grader, pre-algebra)\nBayes net has three states\n\nFred logs into ACED\nFred attempts the task tCommonRatio1a and gets it right\nFred attempts the task tCommonRatio2a and gets it wrong\n\n\nRNetica Quick Start\n\n\nStart a new student\n## Copy the master proficiency model\n## to make student model\nFred.SM &lt;- CopyNetworks(profModel,“Fred”)\nFred.SMvars &lt;- NetworkAllNodes(Fred.SM)\nCompileNetwork(Fred.SM)\n## Setup score history\nprior &lt;- NodeBeliefs(Fred.SMvars$SolveGeometricProblems)\nFred.History &lt;- matrix(prior,1,3)\nrow.names(Fred.History) &lt;- “*Baseline*”\ncolnames(Fred.History) &lt;- names(prior)\nFred.History\nRNetica Quick Start\n\n\nScore 1st Task\n### Fred does a task\nt.name &lt;- “tCommonRatio1a”\nt.isCorrect &lt;- “Yes”\n## Adjoin SM and EM\nEMnet &lt;- ReadNetworks(paste(EMtable[t.name,“EM”],“dne”,sep=“.”))\nobs &lt;- AdjoinNetwork(Fred.SM,EMnet)\nNetworkAllNodes(Fred.SM)\n## Fred.SM is now the Motif for the current task.\nCompileNetwork(Fred.SM)\n## Enter finding\nNodeFinding(obs$isCorrect) &lt;- t.isCorrect\nRNetica Quick Start\n\n\nStats and Cleanup for 1st task\n## Calculate statistics of interest\npost &lt;- NodeBeliefs(Fred.SMvars$SolveGeometricProblems)\nFred.History &lt;- rbind(Fred.History,new=post)\nrownames(Fred.History)[nrow(Fred.History)] &lt;- paste(t.name,t.isCorrect,sep=“=”)\nFred.History\n## Cleanup and Observable no longer needed, so absorb it:\nDeleteNetwork(EMnet) ## Delete EM\n## AbsorbNodes(obs)\n## Currently, there is a  Netica  bug with Absorb Nodes, we will leave\n## this node in place as that is mostly harmless.\nRNetica Quick Start\n\n\n2nd Task\n### Fred does another task\nt.name &lt;- “tCommonRatio2a”\nt.isCorrect &lt;- “No”\nEMnet &lt;- ReadNetworks(paste(EMtable[t.name,“EM”],“dne”,sep=“.”))\nobs &lt;- AdjoinNetwork(Fred.SM,EMnet)\nNetworkAllNodes(Fred.SM)\n## Fred.SM is now the Motif for the current task.\nCompileNetwork(Fred.SM)\nNodeFinding(obs[[1]]) &lt;- t.isCorrect\npost &lt;- NodeBeliefs(Fred.SMvars$SolveGeometricProblems)\nFred.History &lt;- rbind(Fred.History,new=post)\nrownames(Fred.History)[nrow(Fred.History)] &lt;-\npaste(t.name,t.isCorrect,sep=“=”)\nFred.History\n# # Cleanup: Delete EM and Absorb observables\nDeleteNetwork(EMnet) ## Delete EM\n## AbsorbNodes(obs)\nRNetica Quick Start\n\n\nSave and Restore\n## Fred logs out\nWriteNetworks(Fred.SM,“FredSM.dne”)\nDeleteNetwork(Fred.SM)\nis.active(Fred.SM)\n## No longer active in  Netica  space\n## Fred logs back in\nFred.SM &lt;- ReadNetworks(“FredSM.dne”)\nis.active(Fred.SM)\nRNetica Quick Start\n\n\nGetting Serious\n\nACED field test has 230 students attempt all 63 tasks.\nFile miniACED-Geometric contains 30 task subset\n\nThere may be data registration issues here, don’t publish using these data before checking with me for an update\n\nEach row is one student Record\nLets score the first student\n\nAnd build a score history\n\n\nRNetica Quick Start\n\n\nSetup for mini-ACED\nminiACED.data &lt;- read.csv(“miniACED-Geometric.csv”,row.names=1)\nhead(miniACED.data)\nnames(miniACED.data)\n## Mark columns of table corresponding to tasks\nfirst.task &lt;- 9\nlast.task &lt;- ncol(miniACED.data)\n## Code key for numeric values\nt.vals &lt;- c(“No”,“Yes”)\nRNetica Quick Start\n\n\nSetup new Student\n## Pick a student, we might normally iterate over this.\nStudent.row &lt;- 1\n## Setup for student in sample\n## Create Student Model from Proficiency Model\nStudent.SM &lt;- CopyNetworks(profModel,“Student”)\nStudent.SMvars &lt;- NetworkAllNodes(Student.SM)\nCompileNetwork(Student.SM)\n## Initialize history list\nprior &lt;- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\nStudent.History &lt;- matrix(prior,1,3)\nrow.names(Student.History) &lt;- “*Baseline*”\ncolnames(Student.History) &lt;- names(prior)\nRNetica Quick Start\n\n\nLoop Part 1: Add Evidence\n## Now loop over tasks\nfor (itask in first.task:last.task) {\n ## Look up the EM for the task, and adjoin it.\ntid &lt;- names(miniACED.data)[itask]\nEMnet &lt;- ReadNetworks(paste(EMtable[tid,“EM”],“dne”,sep=“.”))\nobs &lt;- AdjoinNetwork(Student.SM,EMnet)\nCompileNetwork(Student.SM)\n## Add the evidence\nt.val &lt;- t.vals[miniACED.data[Student.row,itask]] #Decode integer\nNodeFinding(obs[[1]]) &lt;- t.val\nRNetica Quick Start\n\n\nLoop Part 2: Capture Statistics\n## Update the history\npost &lt;- NodeBeliefs(Student.SMvars$SolveGeometricProblems)\nStudent.History &lt;- rbind(Student.History,new=post)\nrownames(Student.History)[nrow(Student.History)] &lt;- paste(tid,t.val,sep=“=”)\n## Cleanup, Delete EM and  Absob  Observables\nDeleteNetwork(EMnet)\n##  AbsorbNodes ( obs ) # Still broken\n}\nRNetica Quick Start\n\n\nWeight of Evidence\nGood (1985)\nH is binary hypothesis, e.g., Proficiency &gt; Medium\nE is evidence for hypothesis\nWeight of Evidence (WOE) is"
  },
  {
    "objectID": "Session3b.html",
    "href": "Session3b.html",
    "title": "Discrete Partial Credit Model: A generic framework for building CPTs",
    "section": "",
    "text": "Bayesian Networks in Educational Assessment\nTutorial\nSession III:  Bayes Net with R\nDuanli Yan, Diego Zapata, ETS\nRussell Almond, FSU\n2021 NCME Tutorial: Bayesian Networks in Educational Assessment\nSESSION __ __ TOPIC __ __ PRESENTERS\nSession 1 : Evidence Centered Design Diego Zapata Bayesian Networks\nSession 2 : Bayes Net Applications Duanli Yan & ACED: ECD in Action Russell Almond\nSession 3 : Bayes Nets with R Russell Almond & Duanli Yan\nSession 4 : Refining Bayes Nets with Duanli Yan & Data Russell Almond\n\nDiscrete Partial Credit Model: A generic framework for building CPTs\n\n\nRussell Almond\n\n\nConditional Probability Tables\n\nFocus on a child variable\nChild has zero or more parent variables in graph\nFor each configuration of parent variables, need conditional probability of each child variable.\n\nUnconditional probability in the case of no parents\n\nIf there are N parents, each with M states and the child variable has K states, then the number of unconstrained entries in the table is\nM N (K-1)\n\n\n\nProblems\nToo many parameters to comfortably elicit\nCertain cases might be rare in population (Very High on Skill 1 and Very Low on Skill 2 )\nWant to capture intuition of experts on how skills interact to generate performace.\n\n\nReduced Parameter Models\n\nNoisy-and and Noisy-Or models\n\nNIDA, DINA and Fusion model (Junker & Sijtsma)\nAssume binary responses\n\nDiscrete IRT models\nDiBello—Samejima models\n\nBased on “effective theta” and graded response model\nCompensatory, Conjunctive, Disjunctive and Inhibitor relationships\n\nCPTtools framework\n\nEffective theta mapping\nSelectable combination rule\nSelectable link function (graded response, normal, generalized partial credit)\n\nFor all of these model types, number of parameters grows linearly with number of parents\n\n\n\nNoisy-And (Or)\n\nAll input skills needed to solve problem\nBypass parameter for Skill j , q j\nSlip probability (overall), q 0\nProbability of correct outcome\nNIDA/DINA\n\n\n\nNoisy Min (Max)\n\nIf skills have more than two levels\n\nUse a cut point to make skill binary (e.g., reading skill must be greater than X)\nUse a Noisy-min model\n\nProbability of success is determined by the weakest skill\n\n\nNoisy-And/Min common in ed. measurement, Noisy-Or/Max common in diagnosis\nNumber of parameters is linear in number of parents/states\nVariants of propagation algorithm take advantage of extra Noisy-Or/And independence conditions\n\n\n\nDiscrete IRT (2PL) model\n\n\nDiBello–Samejima Models\nSingle parent version\nMap each level of parent state to “effective theta” on IRT (N(0,1)) scale,\nNow plug into Samejima graded response model to get probability of outcome\nUses standard IRT parameters, “difficulty” and “discrimination”\nDiBello--Normal model uses regression model rather than graded response\n\n\n\nThe “Effective q” Method (1):\u000bSamejima’s Model\nSamejima’s (1969) psychometric model for graded responses:\n\n\n\nThe “Effective q” Method (2):\u000b Conditional Probabilities for Three q ’s\n_ q _ X =1 (Poor) X =2 (Okay) X =3 (Good)\nLow= -1.8 .70 .25 .05\nMed= -.4 .35 .40 .25\nHigh= 1.0 .10 .40 .50\n\n\nVarious Combination Rules\n\nFor Multiple Parents, assign each parent j  an effective theta at each level k , .\nCombine Using a Combination Rule (Structure Function)\nPossible Structure Functions:\n\nCompensatory = average\nConjunctive = min\nDisjunctive = max\nInhibitor; e.g. level k * on :\n\nwhere is some low value.\n\n\n\n\n\n\n\nExample (Biomass)\nDKMendel is the student-model variable that determines probabilities of response to the several observable variables in the Mode of Inheritance chart.\nContext is a parent that induces conditional dependence among these observations, for reasons other than the DKMendel (e.g., did not understand what was required in task).\nLike an IRT testlet effect.\n\n\nEffective Thetas for \u000bCompensatory Relationship\nequally spaced normal quantiles\n\n\n\n\n\nEffective Theta to CPT\nIntroduce new parameter d inc _ _ as spread between difficulties in Samejima model\nb i,Full _ = b_ j _ + d_ inc /2 b j,Partial _ = b_ j _ - d_ inc /2\nConditional probability table for _ d_ inc _ _ = 1 is then…\n\n\n\nCPTtools framework\n\nBuilding a CPT requires three steps:\nMap each parent state into a effective theta for that parent\nCombine the parent effective thetas to an effective theta for each row of the CPT using one (or more) combination rules\n\nCombination rules generally take one or more (often one for each parent variable) discrimination parameters which weight the parent variable contributions (log alphas)\nCombination rules generally take one or more difficulty parameters (often one for each state of the child variable) which shift the average probability of a correct response (betas)\n\nMap the effect theta for each row into a conditional probability of seeing each state using a link function\n\nLink functions can take a scaling parameter. (link scale)\n\n\n\n\nParent level effective thetas\nEffective theta scale is a logit scale corresponds to mean 0 SD 1 in a “standard” population.\nWant the effective theta values to be equally spaced on this scale\nWant the marginal distribution implied by the effective thetas to be uniform (unit of the combination operator)\nWhat the effective theta transformation to be effectively invertible (this is reason to add the 1.7 to the IRT equation).\n\n\nEqually spaced quantiles of the normal distribution"
  },
  {
    "objectID": "Building.html",
    "href": "Building.html",
    "title": "Building Networks Models with Expert Opinion",
    "section": "",
    "text": "First, the Peanut libraries need to be loaded, along with an implementation. As PNetica is the currently available implementation, that will be used in this example. The Resources section shows how to install this.\nAfter some experience using various graphical tools, -Almond (2010) found that the easiest way to work with experts in eliciting a Bayesian network structure is to use a spreadsheet. The spreadsheet for the miniACED example is available through Google Sheets\nNote that the format here is a bit of a work in progress. I am looking for ways to simplify the presentation, in particular, collapsing over lines, so look for htat in the future.\nAs the example network data is stored in a Google sheet, we will also need the googlesheets4 library. The knitr package provides table formatting.\nWith Google sheets, Google needs to verify that your computer is authorized to read the sheet. Fortunately, the example sheet is readable by the public. So we will call gs4_deauth() to make sure that Google uses the public interface. (See the googlesheets4 documentation for more information about OAuth support.) The sheet ID is the long string of digits and numbers after spreadsheets/d/ in the URI for the sheet."
  },
  {
    "objectID": "Building.html#preliminaries",
    "href": "Building.html#preliminaries",
    "title": "Building Networks Models with Expert Opinion",
    "section": "",
    "text": "First, the Peanut libraries need to be loaded, along with an implementation. As PNetica is the currently available implementation, that will be used in this example. The Resources section shows how to install this.\nAfter some experience using various graphical tools, -Almond (2010) found that the easiest way to work with experts in eliciting a Bayesian network structure is to use a spreadsheet. The spreadsheet for the miniACED example is available through Google Sheets\nNote that the format here is a bit of a work in progress. I am looking for ways to simplify the presentation, in particular, collapsing over lines, so look for htat in the future.\nAs the example network data is stored in a Google sheet, we will also need the googlesheets4 library. The knitr package provides table formatting.\nWith Google sheets, Google needs to verify that your computer is authorized to read the sheet. Fortunately, the example sheet is readable by the public. So we will call gs4_deauth() to make sure that Google uses the public interface. (See the googlesheets4 documentation for more information about OAuth support.) The sheet ID is the long string of digits and numbers after spreadsheets/d/ in the URI for the sheet."
  },
  {
    "objectID": "Building.html#defining-networks",
    "href": "Building.html#defining-networks",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Defining Networks",
    "text": "Defining Networks\nNote that this sheet has five tabs which the analyst must complete to specify the model.\n\nNets—describes the competency and evidence models.\nNodes—describes the nodes and their states.\nQ—describes the distribution of variables representing observable indicators that appear in the evidence models.\nOmega—describes the relationship and distribution of the competency model variables.\nStatistics—describes what values will be written out.\n\nNote that there is a extra tab Dropdowns. This is used as part of the network validation, and doesn’t need to be filled out by the analyst."
  },
  {
    "objectID": "Building.html#nets",
    "href": "Building.html#nets",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Nets",
    "text": "Nets\nThe first spreadsheet is the network manifest; which basically lists all of the nets.\n\nknitr::kable(netman)\n\n\n\nTable 1: Nets Tab\n\n\n\n\n\n\n\n\n\nName\nTitle\nHub\nPathname\nDescription\n\n\n\n\nminiACEDPM\nACED subset\nminiACEDPM\nminiACEDPM.dne\nA subset of the full ACED model\n\n\nCommonRatioEasyEM\nCommon Ratio (Easy variant)\nminiACEDPM\nCommonRatioEasyEM.dne\nAn easy task using the common ratio skill\n\n\nCommonRatioMedEM\nCommon Ratio (Medium variant)\nminiACEDPM\nCommonRatioMedEM.dne\nA moderate task using the common ratio skill\n\n\nCommonRatioHardEM\nCommon Ratio (Hard variant)\nminiACEDPM\nCommonRatioHardEM.dne\nA hard task using the common ratio skill\n\n\nExamplesEasyEM\nExamples (Easy variant)\nminiACEDPM\nExamplesEasyEM.dne\nAn easy task using the examples skill\n\n\nExamplesMedEM\nExamples (Medium variant)\nminiACEDPM\nExamplesMedEM.dne\nA moderate task using the examples skill\n\n\nExamplesHardEM\nExamples (Hard variant)\nminiACEDPM\nExamplesHardEM.dne\nA hard task using the examples skill\n\n\nExtendEasyEM\nExtend (Easy variant)\nminiACEDPM\nExtendEasyEM.dne\nAn easy task using the Extend skill\n\n\nExtendMedEM\nExtend (Medium variant)\nminiACEDPM\nExtendMedEM.dne\nA moderate task using the Extend skill\n\n\nExtendHardEM\nExtend (Hard variant)\nminiACEDPM\nExtendHardEM.dne\nA hard task using the Extend skill\n\n\nTableExtendEasyEM\nTableExtend (Easy variant)\nminiACEDPM\nTableExtendEasyEM.dne\nAn easy task using both the table and extend skills.\n\n\nTableExtendMedEM\nTableExtend (Medium variant)\nminiACEDPM\nTableExtendMedEM.dne\nA moderate task using both the table and extend skills.\n\n\nTableExtendHardEM\nTableExtend (Hard variant)\nminiACEDPM\nTableExtendHardEM.dne\nA hard task using both the table and extend skills.\n\n\nModelTableExtendEasyEM\nModelTableExtend (Easy variant)\nminiACEDPM\nModelTableExtendEasyEM.dne\nAn easy task using the model, table, and extend skills.\n\n\nModelTableExtendMedEM\nModelTableExtend (Medium variant)\nminiACEDPM\nModelTableExtendMedEM.dne\nA moderate task using the model, table, and extend skills.\n\n\nModelTableExtendHardEM\nModelTableExtend (Hard variant)\nminiACEDPM\nModelTableExtendHardEM.dne\nA hard task using the model, table, and extend skills.\n\n\n\n\n\n\nThis table is the easiest to understand. Each row represents a Bayes net model, either a comptency model or an evidence model. The columns have the following meaning.\n\nName — the identifier of the network. Note that projects using Netica will need to follow the Netica syntax for identifiers, which is the identifier, must start with a letter and may contain only letters, numbers or underscores (_). It is also limited to 32 characters.\nTitle — A longer version of the name, with no character restrictions.\nHub — For evidence models, this is the name of the corresponding competency model; for competency models, leave if blank.\nPathname — The name of the file for storing the network. This defaults to the name plus the .dne extension, which is Netica’s extension for the text representation of a Bayes net.\nDescription — A place for user comments.\n\nThe Peanut package works by creating a Warehouse for the networks. The idea is that if the network has already been created, then the Net Warehouse will go ahead and build it using the specifications in the manifest; which is the netman file.\nNote here the use of the PNetica package and the RNetica::Session object, sess. This is one of two places in which the explicit dependence on Netica is required. (The other is the creation of the Node Warehouse in the next session). The rest of the commands use function calls in the Peanut or CPTtools frameworks, which are agnostic as to the Bayes net engine used (although so far, only the PNetica implementation exists)."
  },
  {
    "objectID": "Building.html#nodes",
    "href": "Building.html#nodes",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Nodes",
    "text": "Nodes\nThe next spreadsheet defines the nodes. There are two things which are important to note about the nodes. The first is that each network defines its own namespace. Thus, the node isCorrect in the CommonRatioEasyEM and in the ExamplesGeometricEasyEM are different. Thus, the variable is identified by the first two columns in the spreadsheet.\nThe second thing of note is that there are some information which is specific to the possible states of the node. Thus, if the node has 3 possible states, then there are three rows for each node.\n\n\n\n\n\n\nNote\n\n\n\nThis should change in a future version. The idea is to instead use a a vector notation, i.e., [state1,state2,state3], to express multiple states.\n\n\n\nknitr::kable(nodeman)\n\n\n\nTable 2: Nodes Tab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nNodeName\nModelHub\nNodeTitle\nNodeDescription\nNodeLabels\nContinuous\nNstates\nStateName\nStateTitle\nStateDescription\nStateValue\nLowerBound\nUpperBound\n\n\n\n\nminiACEDPM\nSolveGeometricProblems\nNA\nSolve Geometric Problems\nCombination of all Geometric Problem Skills\nProficiencies,pnodes,Highlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nSolveGeometricProblems\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nSolveGeometricProblems\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nminiACEDPM\nCommonRatio\nNA\nCommon Ratio\nSolve problems involving finding the common ratio\nProficiencies,pnodes,Lowlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nCommonRatio\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nCommonRatio\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nminiACEDPM\nExamplesGeometric\nNA\nExamples of Geometric Sequeces\nSolve problems involving examples of Geometric sequences\nProficiencies,pnodes,Lowlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nExamplesGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nExamplesGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nminiACEDPM\nExtendGeometric\nNA\nExtending Geometric Sequeces\nSolve problems involving extending Geometric sequences\nProficiencies,pnodes,Lowlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nExtendGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nExtendGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nminiACEDPM\nTableGeometric\nNA\nExtending Geometric Sequeces in Tables\nSolve problems involving extending Geometric sequences in tabular form\nProficiencies,pnodes,Lowlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nTableGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nTableGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nminiACEDPM\nModelGeometric\nNA\nExtending Geometric Sequeces in Tables using Models\nSolve problems involving extending Geometric sequences in tabular form with models\nProficiencies,pnodes,Lowlevel\nFALSE\n3\nHigh\nNA\nNA\n0.9674\nNA\nNA\n\n\nminiACEDPM\nModelGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nMedium\nNA\nNA\n0.0000\nNA\nNA\n\n\nminiACEDPM\nModelGeometric\nNA\nNA\nNA\nNA\nNA\nNA\nLow\nNA\nNA\n-0.9674\nNA\nNA\n\n\nCommonRatioEasyEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nCommonRatioEasyEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nCommonRatioMedEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nCommonRatioMedEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nCommonRatioHardEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nCommonRatioHardEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesEasyEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesEasyEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesMedEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesMedEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesHardEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExamplesHardEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExtendEasyEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExtendEasyEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExtendMedEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExtendMedEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nExtendHardEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nExtendHardEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendEasyEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendEasyEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendMedEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendMedEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendHardEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nTableExtendHardEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendEasyEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendEasyEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendMedEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendMedEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendHardEM\nisCorrect\nminiACEDPM\nCorrectly Solved?\nWas the task correctly solved\nEvidence,pnodes,onodes\nFALSE\n2\nYes\nNA\nNA\nNA\nNA\nNA\n\n\nModelTableExtendHardEM\nisCorrect\nminiACEDPM\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nNode Primary Key\nThe following columns are present in the spreadsheet:\n\nModel — The name of the network, this should appear in the Name column of the Nets sheet.\nNodeName — The name of the node. This should correspond to the Netica naming rules.\n\nNote that the (Model, NodeName) ordered pair is the primary key for the table.\n\n\nNode-level properties\nThe following values are assigned at the node level.\n\nModelHub — The name of the proficiency/competency model. (This information is redundant with that in the Nets sheet.\nNodeTitle — A longer name for the node which does not need to conform to variable naming rules.\nNodeDescription — Documentation for the node.\nContinuous — A logical value. If FALSE, then the node has a number of discrete values. If TRUE, then the node is a discritized version of an underlying continuous value.\nNstates — The number of states of the node. If the node is discrete (Continuous=FALSE), then this is the number of possible values. If the node is continuous, then this is the number of states the node will be partitioned into.\n\nNote that most of the state level value must be vectors of the same length as Nstates for that row.\n\n\nState-level descriptors\n\nStateName — An identifier for the state. Must follow variable rules.\nStateTitle — A longer descriptor which doesn’t need to follow varaible rules.\nStateDescription — Documentation for the state.\nStateValue — This is used in both the CPT construction algorithms and in calculating expected values and varainces for nodes. The best practice here is to map these values to quantiles of the normal distribution (Almond et al. (2015)). Note that for three levels these are approximately 1, 0 and -1.\nLowerBound, UpperBound. These are lists of upper and lower bounds for an underlying continuous variable. Note that -Inf and Inf are acceptable values. Note also that the upper bound for one state must match the lower bound for the next. ::: {.callout-warning title=“Depricated” collapse=“true”} This will be replaced in a future version with a new field Cuts which will use the cutpoints. :::\nCuts — Only used for continuous nodes. These are the upper and lower cuts for the underlying continuous variable. There should be one more value in the vector than there are states in the node. The continuous variable is chopped into intervals with the given cut points (including the minimum and maximum). Note that -Inf and Inf are acceptable values.\n\n\n\nNode Warehouse\nPeanut uses the same Warehouse metaphor to build the nodes. The Nodes spreadsheet is the instruction for building the node (if it is not already built).\nThis is the last place in which we explicitly reference Netica. The WarehouseSupply method on the Net or Node Warehouse returns an object of the abstract type Peanut::Pnet or Peanut::Pnode, which then supports all of the other operations.\nThe two warehouses give us the basic definitions of the networks and the nodes. The next step is to describe the relationships among the nodes and the conditional probability tables that are defined.\nAlmond (2010) divides this into two pieces: the \\(Q\\)-matrix which describes the relationship between the competency nodes and the observable outcomes(Section 6), and the \\(\\Omega\\) matrix which describes the relationship among the competency variables (Section Section 5). ?@sec-growth)."
  },
  {
    "objectID": "Building.html#sec-Omega",
    "href": "Building.html#sec-Omega",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Omega",
    "text": "Omega\nThe Omega (\\(\\Omega\\)) matrix is named for the inverse of the covariance matrix. Is is based on the observation that zero in the inverse covariance matrix leads to a conditional independence assumption (and hence lack of edge) in the graphical model (dempster1972?; whittaker1990?). However, rather than specify the covariance matrix, the approach used in Peanut is to specify the joint distribution of the variable in the competency (proficiency) model through a series of regressions.\n\nOmega Matrix Structure\nThe first part of the model is specifying which variables should be predictors (inputs) to the others. In the \\(\\Omega\\)-matrix (Figure 1), the rows represent the variables in their roles as dependent (output) variables and the columns the role as input variables. A check mark indicates that the column variable is a parent of the row variable (an input in its regression).\n\n\n\nFigure 1: Omega Matrix Structure\n\n\nThis defines the graphical structure. An edge is placed between the nodes corresponding to the edges. Figure Figure 2 shows this graph.\n\n\n\n\n\n\n\n\nEASD\n\n  \n\nSGP\n\n Solve Geometric Problems   \n\nCR\n\n Common Ratio   \n\nSGP-&gt;CR\n\n    \n\nExamG\n\n Example Geometric   \n\nSGP-&gt;ExamG\n\n    \n\nExtG\n\n Extend Geometric   \n\nSGP-&gt;ExtG\n\n    \n\nTabEG\n\n Table   Extend Geometric   \n\nSGP-&gt;TabEG\n\n    \n\nModTEG\n\n Model Table   Extend Geometric  \n\n\nFigure 2: Graph of EASD Net\n\n\n\n\nNote that the nodes are ordered (in both rows and columns) so that the parents of a given node always fall earlier in the sequence than the child. With this topological sorting, all of the checked boxes (selected edges) should be in the lower triangle; this will ensure that the model graph has no directed cycles.\n\n\nAncestral Nodes (without parents)\nThe regression model with no parents is just a normal distribution. The parameters specify the mean and variance of the model.\n\nSGP &lt;- PnetFindNode(CM1,\"SolveGeometricProblems\")\ngadget1 &lt;- MakeRegressionGadget(SGP)\nshiny::shinyApp(gadget1$ui,gadget1$server, options(height=2000))\n\nAn online version of this gadget can be seen at https://pluto.coe.fsu.edu/rdemos/Peanut/miniACED_SGP.Rmd.\nThe distribution is discretized, but adjusting mean parameter controls which bars the distribution is concentrated in, and how concentrated it is.\n\n\nCompetency Variables with one parent.\nHere again the normalLink is chosen because the parent variable states will be mapped to quantiles of the normal distribution.\nRather expressing the regression in terms of the residual standard deviation, it is easier to think of it in terms of \\(R^2\\). In the final model, the coefficients are scaled so that final model has the appropriate \\(R^2\\).\nIn particular, the residual standard deviation is set to\n\\[ \\sqrt{ \\left ( \\frac{1}{K}\\sum_{k \\in \\text{parents}} a_k^2 \\right\n) \\left ( \\frac{1}{R^2} - 1 \\right ) } .\\]\nHere, \\(K\\) is the number of parents and \\(k\\) is an index that ranges over the parent variables. The \\(a_k\\) are the discriminations (regression weights) input through the system and \\(R^2\\) is the selected \\(R^2\\) value. The \\(1/K\\) scale factor is set so that if all of the parent variables have an SD of 1, and all of the coefficients are 1, the child variable has a marginal SD of 1 as well.\nFinally, the intercept column represents a shift in the mean of the distribution relative to the parents. Positive values mean that more people in the population have this skill relative to the parents, and negative values mean fewer.\n\nCR &lt;- PnetFindNode(CM1,\"CommonRatio\")\ngadget2 &lt;- MakeRegressionGadget(CR)\nshiny::shinyApp(gadget2$ui,gadget2$server, options(height=2000))\n\nThe resulting gadget is in: https://pluto.coe.fsu.edu/rdemos/Peanut/miniACED_CR.Rmd.\n\n\nColumns in the Omega Matrix\nIn this model, SolveGeometricProblems is the ancestral node. The others all have just the one parent. Therefore\n\n\n\nFigure 3: Omega Matrix Parameters\n\n\nOverall, the columns in the \\(\\Omega\\)-matrix are as follows:\n\nNode – The name of the nodes in the competency model. The order is important, generally, the parents of a given node should come before the child nodes in the sequence.\nNodeNames – The names of the nodes appear also in the columns, in the same order as the first column. The cells here provide check boxes for the edges. Note that the diagonal is not used.\nLink – The link function used (see next section). Note that currently the normalLink is the only available choice for the \\(\\Omega\\)-matrix.\nRules – The combination function used (see next section). The choice of Compensatory gives the discrete regression model.\nR2 – The \\(R^2\\) is the measure of how much of the variance of the child variable can be explained by the parents.\nA.NodeName – There is one column here for each node. This gives the slope/discrimination value for that node.\nIntercept – The intercept for the regression; positive is the child skill is more prevalent in the population than the parent skills, and negative if it is less prevalent. Note this is a negative difficulty parameter.\nPriorWeight – Used in Bayesian parameter updating, the prior parameter values are assigned the equivalent weight as this many observations."
  },
  {
    "objectID": "Building.html#sec-Q",
    "href": "Building.html#sec-Q",
    "title": "Building Networks Models with Expert Opinion",
    "section": "\\(Q\\)-matrix",
    "text": "\\(Q\\)-matrix\nThe two warehouses give us the basic definitions of the networks and the nodes. The \\(\\Omega\\)-matrix has the distributions for the competency model variables. The next step is to describe the relationships between the competency variables and the observable outcomes in the evidence models. Note that the same variable name, e.g., isCorrect can be reused across many evidence models, so the full name of the variable has both the model and variable name.\n\nRepresenting Structure\nThe \\(Q\\)-matrix is the place to start. Tatsuoka (1983) popularized the use of a matrix \\(Q\\) to represent the cognitive structure of an assessment, where the rows represent items (or in the case of more complex assessment observable variables) and the columns represent attributes. Tatsuoka used the term attribute to emphasize the duality between the cognitive skill possessed by the subjects and the items which demand that skill. Tatsuoka’s rule space model regarded the attributes as binary, but the notation works equally well if the cognitive variables are ordinal or scale.\nThere is a simple relationship between the \\(Q\\)-matrix and the Bayesian network. If an element \\(q_{jk}=1\\), then there is an edge between Competency Variable \\(k\\) and Observable Variable \\(j\\) in the corresponding evidence model. For every observable variable, the parents can be determined by looking across the row to find all cell entries with a one.\n\n\n\nFigure 4: Q-matrix in the Spreadhseet\n\n\nNote that the first two columns are Model and Node. The isCorrect nodes in the models CommonRatioEasyEM, ExamplesGeometricMedEM and ModelTableExtendGeometricHardEM are all different variables. In particular, they have different entries in the \\(Q\\)-matrix. In ACED, the observable names reflect the targeted competencies, but that was merely a convention used by the design team.\nNote also that in the spreadsheet the cells of the \\(Q\\)-matrix is represented with check boxes. This emphasizes the idea that checking the box is showing that skill referenced in the column is needed for the observable node referenced in the row.\nThe Bayes net structure (for the evidence models) follows directly from the \\(Q\\)-matrix. The parents for the observable variables are the nodes (in the competency model) that correspond to the checked columns in the \\(Q\\)-matrix.\n\n\nDefining Conditional Probability Tables.\nIn addition to the graphical structure, a conditional probability table for each node, given its parents must be specified. Lou DiBello pressed some models from item response theory (IRT) into service to provide a language for describing relationships using the familiar psychometric parameters “difficulty” and “discrimination” (Almond et al., 2015).\n\nThe DiBello Framework\nThe DiBello method has three steps:\n\nMap the states of the parent variable onto a unit normal \\(\\theta\\) scale. Let \\(\\theta_{k,s_{k,m}}\\) be the numeric value associated with the Parent Variable \\(S_k\\) being in State \\(s_{k,m}\\). These are represented by the StateValue column in the “Nodes” sheet.\nCombine the parent \\(\\theta\\) values into a single variable using combination Rules. This yields a single set of effective \\(\\theta\\) values for each combination of the parent variables. Let \\(\\eta_{j,c}(\\theta_1, \\ldots, \\theta_K)\\) be the function which calculates this effective theta for observable \\(Y_j\\) associated with the transition between State \\(c-1\\) and State \\(c\\) for the child variable. (In the simple cases below, \\(\\eta_{j,c}(\\cdot)\\) has the same functional form for all \\(c\\), but possibly different values of the parameters.) This is specified through the Rules column in the “Q” matrix spreadsheet.\nChange the effective \\(\\theta\\)s into conditional probabilities using a Link function. Let \\(g_j(\\eta_{j,1},\\ldots,\\eta_{j,C})\\) be the link function. This is specified through the Link column in the “Q” matrix spreadsheet.\n\nThe setup is similar in many ways to generalized linear models, where \\(\\eta(\\cdot)\\) takes the place of the linear predictor. (The method allows non-linear functions, but favors monotonic combination functions.)\nThe first step was already done in the “Nodes” sheet, as the NodeValues column provides the values. Note that the chosen values of \\([1, 0, -1]\\) are close to the midpoints (with respect to the normal distribution) of three equal probability intervals, which would be \\([.97, 0, -.97]\\).\nFor the last step, the partialCredit link function is chosen (see the last column of Figure 4). This is the most flexible of the currently available link functions, as it allows for a different probability to transition from NoCredit to Partial credit, and Partial credit to Full credit. Note that in general, the link function needs to address the transitions between states and not the states themselves.1 In this case, all of the observables are binary, so there is no difference between the partialCredit and gradedResponse link functions.\nFinally, the combination rule is chosen in consultation with the task designers and subject matter experts. The Compensatory rule states that all skills are necessary for solving the problem so that performance will be dominated by the weakest skill.2\n\n\n\nMultiple-A Rules\nIn the compensatory model, there needs to be one slope parameter for each parent variable, and a difficulty (negative interncept) parameter. The first is named “A._VariableName_”, and the latter is named “B”. Note that only the cells corresponding to the 1’s in the Q-matrix columns.\n\n\n\nQ-matrix A-parameters\n\n\nNote that there is another set of combination rules (the OffsetConjuctive and OffsetDisjunctive) rules in which there is a different difficulty (demand) parameter for each parent and a single slope (\\(A\\)). This is the Multiple-B style. These columns are currently hidden.\nNote also, that if the observable values has more than two states, there will in general be one fewer numbers in each column than the number of states. (The old way of representing this is with multiple rows in the Q-matrix; the revised method will include multiple entries in each cell.\n\n\nA graphical example\n\nEM &lt;- WarehouseSupply(Nethouse,\"TableExtendGadget\")\nisC &lt;- PnetFindNode(EM,\"isCorrect\")\ngadget3 &lt;- MakeDPCGadget(isC)\nshiny::shinyApp(gadget3$ui,gadget3$server)\n\nThe gadget can be seen at https://pluto.coe.fsu.edu/rdemos/Peanut/miniACED_TE.Rmd.\n\n\nColumns in the Extended Q-matrix\nThe first two columns in the \\(Q\\)-matrix sheet identify the variable.\n\nModel, Node – The name of the model and node. Note that two variables with the same name (Node column) but different models are distinct.\nNStates – Number of States the node has. This should agree with the Nodes sheet.\nStates – The names of all but one of the states. In general, probabilities are built for the transition between states. States are assume to be in decreasing order: (High, Medium, Low), so in the \\(Q\\)-matrix the state High refers to the transition from Medium to High and Medium refers to the transition from Low to Medium. It is assumed that all responses are at least Low, so no parameters are needed for the last state.\nLink – The name of the link function. Currently supported are partialCredit and gradedResponse. There is limited support for normalLink as well.\nLinkScale – An additional scale parameter (e.g., residual standard deviation) for the link function. Currently only the normalLink uses this. If supplied, it should be a positive number.\n\\(Q\\)-matrix proper (NodeName) – Next, there are columns in the \\(Q\\)-matrix corresponding to the variables in the competency model. There is a check in the corresponding box if the observable referenced in the row depends on the competency variable in the column.\nRules – This is the name of the combination function that is used. Note that under the partialCredit link function, this can be either a single value or a distinct value for each state transition. (If a single value is given it is replicated out to the number of sites minus 1).\n\nThere are two types of rules, which require different sets of parameters. Multi-A Rules (Compensatory and all Conjuctive and Disjunctive3) have one discrimination (\\(A_p\\)) parameter for each parent variable, and a single difficulty/demand (\\(B\\)) parameter. Multi-B Rules (OffsetConjunctive and OffsetDisjunctive) have one demand parameter (\\(B_p\\)) for each parent variable, and a single discrimination ($A%). Therefore different columns in the spreadsheet are used depending on the choice of rules.\n\nMulti-A Table Columns.\nThese are regression-like models. There is one slope (\\(A_p\\)) for each parent variable, and a single intercept (\\(B\\)).\n\nA.NodeName – is used to give the weight for the corresponding parent variable. This should be left blank if the column is not a parent variable (not checked in the \\(Q\\)-matrix).\nB – Difficulty/demand. The B column without a variable name attached is used to specify the overall difficulty of the item.\n\nIn each case there can be a single value, or a list of values corresponding to the state transitions.\nThere are some additional rules depending on the link function.\n\npartialCredit – no restrictions on the parameters. Some discriminations can be 0 or missing to indicate that a particular parent variable has no role in that transition.\ngradedResponse – This model requires parameter restrictions to prevent negative probabilities. In particular, there must be one difficulty (\\(B\\)) parameter for each transition and they must be non-increasing. Although it is possible to have multiple discrimination parameters, restricting the discrimination parameters to be the same for every level ensures there will be no negative probabilities.\nnormalLink – This link function requires a single set of parameters for all state transitions. (It also requres the LinkScale).\n\n\n\nMulti-B Parameters\nThe OffsetConjunctive and OffsetDisjunctive models use min() and max() respectively to collapse from the \\(p\\) dimensions of the parents to the single dimension of the child. There is one offset parameter (\\(B_p\\)), a difficulty or a demand, for each parent. There is only a single discrimination (\\(A\\)).\n\nA – Discrimination (slope). The A column without a variable name attached is used to specify the overall discrimination of the item.\nB._NodeName_ – There is a demand parameter for each relevant (box checked in the \\(Q\\)-matrix) competency variable.\n\nIn each case there can be a single value, or a list of values corresponding to the state transitions.\nThere are some additional rules depending on the link function.\n\npartialCredit – no restrictions on the parameters. Some discriminations can be infinite or missing to indicate that a particular parent variable has no role in that transition.\ngradedResponse – Each difficulty parameter should be a non-increasing series. Again, is safest to use a common discrimination parameter for all transitions.\nnormalLink – Again, a single set of parameters for all transitions is used here.\n\nFinally, the last column in the spreadsheet for both multi-\\(A\\) and multi-\\(B\\) rules is the PriorWeight. This has the same interpretation as it does in the \\(\\Omega\\)-matrix."
  },
  {
    "objectID": "Building.html#statistics",
    "href": "Building.html#statistics",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Statistics",
    "text": "Statistics\nAfter student specific evidence, the Bayesian network contains the posterior distribution over the possible skill states for the student based on both the prior (population) model and the observed evidence. Rather than look at the full posterior distribution, usually the Bayesian networks outputs certain statistics of the posterior distribution. Most of these are focused on a single node.\nThe currently supported statistics are:\n\nPnodeMargin – This returns the marginal posterior probability for the node. It is a vector (simplex) over the states of the node.\nPnodeMode – The state of the node that has the highest marginal probability. (Value is a character scalar).\nPnodeMedian – (Assumes states are ordered). The state of the node which is associated with cumulative probability of .5. (Values is a character scalar).\n\nThe next two statistics assume that real values have been assigned to the states, making it a random variable.\n\nPnodeEAP – The expected a posterior value or posterior mean for the random variable’s marginal distribution.\nPnodeSD – The standard deviation of the marginal distribution of the random variable.\n\n\n\n\nStatistics Table"
  },
  {
    "objectID": "Building.html#putting-it-all-together",
    "href": "Building.html#putting-it-all-together",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Putting it all together",
    "text": "Putting it all together\nFirst download and clean all the sheets.\n\nnetman &lt;- read_sheet(sheetID,\"Nets\")\n\n✔ Reading from \"MiniACED\".\n\n\n✔ Range ''Nets''.\n\nnodeman &lt;- read_sheet(sheetID,\"Nodes\")\n\n✔ Reading from \"MiniACED\".\n\n\n✔ Range ''Nodes''.\n\nomega &lt;- read_sheet(sheetID,\"Omega\")\n\n✔ Reading from \"MiniACED\".\n\n\n✔ Range ''Omega''.\n\nQQ &lt;- read_sheet(sheetID,\"Q\")\n\n✔ Reading from \"MiniACED\".\n\n\n✔ Range ''Q''.\n\nstats &lt;- read_sheet(sheetID,\"Statistics\")\n\n✔ Reading from \"MiniACED\".\n\n\n✔ Range ''Statistics''.\n\n\nNext, build the Network and Node warehouse. Note that for PNetica this requires a reference to the NeticaSession object. Note also, that the network warehouse has a reference to a directory where the network file will be stored.\n\nNethouse &lt;- PNetica::BNWarehouse(manifest=netman,session=sess,\n                                 address=file.path(\"miniACED1\"),\n                                 key=\"Name\")\nNodehouse &lt;- PNetica::NNWarehouse(manifest=nodeman,\n                                  key=c(\"Model\",\"NodeName\"),\n                                  session=sess)\n\nNext build the competency model. First create a blank network from the Network arehouse, then, use the Omega2Pnet function.\n\nCM &lt;- WarehouseSupply(Nethouse,\"miniACEDPM\")\nOmega2Pnet(omega,CM,Nodehouse,override=TRUE)\n\nINFO [2023-06-09 18:32:50] Proficiency variables are\n\n[1] \"SolveGeometricProblems\" \"CommonRatio\"            \"ExamplesGeometric\"     \n[4] \"ExtendGeometric\"        \"TableGeometric\"         \"ModelGeometric\"        \n\n\nWarning: Setting row names on a tibble is deprecated.\nSetting row names on a tibble is deprecated.\n\n\nINFO [2023-06-09 18:32:50] Building list of nodes.\nINFO [2023-06-09 18:32:50] Processing links.\nWARN [2023-06-09 18:32:50] While processing links for node: CommonRatio\nWARN [2023-06-09 18:32:50] Node has parents: \n\n                         \n\"SolveGeometricProblems\" \nWARN [2023-06-09 18:32:50] But Omega matrix has parents: \n\ncharacter(0)\nWARN [2023-06-09 18:32:50] Changing node CommonRatio to match Omega matrix.\nWARN [2023-06-09 18:32:50] While processing links for node: ExamplesGeometric\nWARN [2023-06-09 18:32:50] Node has parents: \n\n                         \n\"SolveGeometricProblems\" \nWARN [2023-06-09 18:32:50] But Omega matrix has parents: \n\ncharacter(0)\nWARN [2023-06-09 18:32:50] Changing node ExamplesGeometric to match Omega matrix.\nWARN [2023-06-09 18:32:50] While processing links for node: ExtendGeometric\nWARN [2023-06-09 18:32:50] Node has parents: \n\n                         \n\"SolveGeometricProblems\" \nWARN [2023-06-09 18:32:50] But Omega matrix has parents: \n\ncharacter(0)\nWARN [2023-06-09 18:32:50] Changing node ExtendGeometric to match Omega matrix.\nWARN [2023-06-09 18:32:50] While processing links for node: TableGeometric\nWARN [2023-06-09 18:32:50] Node has parents: \n\n                         \n\"SolveGeometricProblems\" \nWARN [2023-06-09 18:32:50] But Omega matrix has parents: \n\ncharacter(0)\nWARN [2023-06-09 18:32:50] Changing node TableGeometric to match Omega matrix.\nWARN [2023-06-09 18:32:50] While processing links for node: ModelGeometric\nWARN [2023-06-09 18:32:50] Node has parents: \n\n                         \n\"SolveGeometricProblems\" \nWARN [2023-06-09 18:32:50] But Omega matrix has parents: \n\ncharacter(0)\nWARN [2023-06-09 18:32:50] Changing node ModelGeometric to match Omega matrix.\nINFO [2023-06-09 18:32:50] Processing CPTs.\nWARN [2023-06-09 18:32:50] Alpha vector for node SolveGeometricProblems is empty.\nWARN [2023-06-09 18:32:50] Alpha vector for node CommonRatio is empty.\nWARN [2023-06-09 18:32:50] Alpha vector for node ExamplesGeometric is empty.\nWARN [2023-06-09 18:32:50] Alpha vector for node ExtendGeometric is empty.\nWARN [2023-06-09 18:32:50] Alpha vector for node TableGeometric is empty.\nWARN [2023-06-09 18:32:50] Alpha vector for node ModelGeometric is empty.\n\n\nNetica Network named  miniACEDPM \n  Network is currently active.\n  Nodes :  CommonRatio ExamplesGeometric ExtendGeometric ModelGeometric SolveGeometricProblems TableGeometric .\n\n\nOnce the competency model is built, build the evidence models.\n\nQmat2Pnet(QQ,Nethouse,Nodehouse,override=TRUE)\n\nINFO [2023-06-09 18:32:50] Proficiency variables:\n\n[1] \"SolveGeometricProblems\" \"CommonRatio\"            \"ExamplesGeometric\"     \n[4] \"ExtendGeometric\"        \"TableGeometric\"         \"ModelGeometric\"        \nINFO [2023-06-09 18:32:50] Processing net CommonRatioEasyEM\nINFO [2023-06-09 18:32:50] Processing node isCorrect in net CommonRatioEasyEM\nINFO [2023-06-09 18:32:50] Processing net CommonRatioMedEM\nINFO [2023-06-09 18:32:50] Processing node isCorrect in net CommonRatioMedEM\nINFO [2023-06-09 18:32:50] Processing net CommonRatioHardEM\nINFO [2023-06-09 18:32:50] Processing node isCorrect in net CommonRatioHardEM\nINFO [2023-06-09 18:32:50] Processing net ExamplesEasyEM\nINFO [2023-06-09 18:32:50] Processing node isCorrect in net ExamplesEasyEM\nINFO [2023-06-09 18:32:50] Processing net ExamplesMedEM\nINFO [2023-06-09 18:32:50] Processing node isCorrect in net ExamplesMedEM\nINFO [2023-06-09 18:32:50] Processing net ExamplesHardEM\nINFO [2023-06-09 18:32:50] Processing node isCorrect in net ExamplesHardEM\nINFO [2023-06-09 18:32:50] Processing net ExtendEasyEM\nINFO [2023-06-09 18:32:50] Processing node isCorrect in net ExtendEasyEM\nINFO [2023-06-09 18:32:50] Processing net ExtendMedEM\nINFO [2023-06-09 18:32:50] Processing node isCorrect in net ExtendMedEM\nINFO [2023-06-09 18:32:50] Processing net ExtendHardEM\nINFO [2023-06-09 18:32:50] Processing node isCorrect in net ExtendHardEM\nINFO [2023-06-09 18:32:50] Processing net TableExtendEasyEM\nINFO [2023-06-09 18:32:50] Processing node isCorrect in net TableExtendEasyEM\nINFO [2023-06-09 18:32:51] Processing net TableExtendMedEM\nINFO [2023-06-09 18:32:51] Processing node isCorrect in net TableExtendMedEM\nINFO [2023-06-09 18:32:51] Processing net TableExtendHardEM\nINFO [2023-06-09 18:32:51] Processing node isCorrect in net TableExtendHardEM\nINFO [2023-06-09 18:32:51] Processing net ModelTableExtendEasyEM\nINFO [2023-06-09 18:32:51] Processing node isCorrect in net ModelTableExtendEasyEM\nINFO [2023-06-09 18:32:51] Processing net ModelTableExtendMedEM\nINFO [2023-06-09 18:32:51] Processing node isCorrect in net ModelTableExtendMedEM\nINFO [2023-06-09 18:32:51] Processing net ModelTableExtendHardEM\nINFO [2023-06-09 18:32:51] Processing node isCorrect in net ModelTableExtendHardEM\n\n\nNext write out the nets. The Warehouse takes care of the mechanics here.\n\nfor (name in netman$Name) {\n  if (nchar(name)&gt;0L) {\n    net &lt;- WarehouseSave(Nethouse,name)\n  }\n}\n\nWrite out the manifest. In this case, there are six variants of each task type: 1a and 1b are easy, 2a and 2b are medium and 3a and 3b are hard. So this can be used to build up the manifest file which maps tasks to evidence models.\n\n#: eval: false\nEMs &lt;- c(\"CommonRatio\",\"ExamplesGeometric\",\"ExtendGeometric\",\n         \"TableExtendGeometric\",\"ModelTableExtend\")\nEMTable &lt;- data.frame(\n    Task=paste(\"t\",rep(EMs,each=6),\n               rep(c(\"1a\",\"1b\",\"2a\",\"2b\",\"3a\",\"3b\"),5),sep=\"\"),\n    EM=paste(rep(EMs,each=6),rep(c(\"Easy\",\"Easy\",\"Med\",\"Med\",\"Hard\",\"Hard\"),5),\n             \"EM\",sep=\"\"),\n    X=rep(c(108,342,588,1134,858),each=6),\n    Y=rep(c(282,402,522,642,762,882),5))\nwrite.csv(EMTable,file.path(\"miniACED1\",\"EMTable.csv\"))\nwrite.csv(stats,file.path(\"miniACED1\",\"Statistics.csv\"))"
  },
  {
    "objectID": "Building.html#footnotes",
    "href": "Building.html#footnotes",
    "title": "Building Networks Models with Expert Opinion",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLink functions are represented as actual functions in the CPTtools package (CPTtools?). The currently supported link functions are partialCredit, gradedResponse and normalLink. The system is extendable.↩︎\nCombination functions are also implemented as functions in the CPTtools package. The currently recommended functions are Compensatory, OffsetConjunctive and OffsetDisjunctive.↩︎\nThe Conjunctive and Disjunctive rules are deprecated in favor of the OffsetConjuctive and OffsetDisjunctive rules, differing demands makes more sense than differening discrimination when max() and min() are used for dimension reduction.↩︎"
  },
  {
    "objectID": "Session4.html",
    "href": "Session4.html",
    "title": "Posterior Distribution",
    "section": "",
    "text": "Bayesian Networks in Educational Assessment\nSession IV: __ __ Refining __ __ Bayes Net with Data\nEstimating Parameters with MCMC\nRussell Almond, FSU\nDuanli Yan, Diego Zapata, ETS\n2021 NCME Tutorial: Bayesian Networks in Educational Assessment\nSESSION __ __ TOPIC __ __ PRESENTERS\nSession 1 : Evidence Centered Design Diego Zapata Bayesian Networks\nSession 2 : Bayes Net Applications Duanli Yan & ACED: ECD in Action Russell Almond\nSession 3 : Bayes Nets with R Russell Almond & Duanli Yan\nSession 4 : Refining Bayes Nets with Duanli Yan & Data Russell Almond\nBayesian Inference: Expanding Our Context\n\nPosterior Distribution\nPosterior distribution for unknowns given knowns _ _ is\nInference about examinee latent variables ( θ ) given observables ( x )\nExample: ACED Bayes Net Fragment for Common Ratio\nθ _ _ = Common Ratio \nx _ _ = Observables from tasks that measure Common Ratio \n\n\nBayes Net Fragment\nθ _ _ = Common Ratio \nx s _ _ = Observables from tasks that measure Common Ratio \n\n\nProbability Distribution for the Latent Variable\nθ _ _ = Common Ratio \n θ   _ ~ _  Categorical(  λ  )\nACED Example\n2 Levels of   θ   (Low, High)\n λ   = (  λ  1 ,   λ  2 ) contains probabilities for Low and High \n\n\n\n\nθ (Common Ratio)\n\n\n\n\n\n\n1\n2\n\n\nProb.\nλ1\nλ2\n\n\n\n\n\nProbability Distribution for the Observables\nx s _ _ = Observables from tasks that measure Common Ratio \n(  x   j   |   θ   =   c  ) ~ Bernoulli(  π   cj  )\nACED Example\n π   cj   _ _   is the probability of correct response on task   j   given   θ   =   c \n\n\n\n\np(xj\nθ)\n\n\n\n\nθ\n0\n1\n\n\n1\n1 – π1j\nπ1j\n\n\n2\n1 – π2j\nπ2j\n\n\n\n\n\nBayesian Inference\n\n\n\n\nθ (Common Ratio)\n\n\n\n\n\n\n1\n2\n\n\nProb.\nλ1\nλ2\n\n\n\n\n\n\n\np(xj\nθ)\n\n\n\n\nθ\n0\n1\n\n\n1\n1 – π1j\nπ1j\n\n\n2\n1 – π2j\nπ2j\n\n\n\nIf the λ s and π s are unknown, they become subject to posterior inference too\n\n\n\n\np(xj\nθ)\n\n\n\n\nθ\n0\n1\n\n\n1\n1 – π1j\nπ1j\n\n\n2\n1 – π2j\nπ2j\n\n\n\nA convenient choice for prior distribution is the beta distribution\nACED Example: π 1 j ~ Beta(1, 1) π 2 j ~ Beta(1, 1)\nFor first task, constrain ( π 21 &gt; π 11) to resolve indeterminacy in the latent variable and avoid label switching\n\n\n\n\nθ (Common Ratio)\n\n\n\n\n\n\n1 (Low)\n2 (High)\n\n\nProb.\nλ1\nλ2\n\n\n\nA convenient choice for the prior distribution is the Dirichlet distribution\nwhich generalizes the Beta distribution to the case of multiple categories\nACED Example: λ = ( λ 1, λ 2) ~ Dirichlet(1, 1)\nλ ~  Dirichlet(αλ)\n\n\nModel Summary\n θ   i   _ ~ _  Categorical( λ )\nλ   ~  Dirichlet ( 1, 1 ) \n(  x   ij   |   θ   i   =   c  ) ~ Bernoulli(  π   cj  )\n π  11   ~ Beta( 1, 1 )\n π  21   ~ Beta( 1, 1 )   I  (  π  21  &gt;   π  11 )\n π   cj    ~ Beta( 1, 1 ) for others obs.\n\n\nJAGS Code\nfor ( i  in 1:n){\n for(j in 1:J){\n x[ i,j ] ~  dbern (pi[theta[ i ],j]) \n }\n}\n(  x   ij   |   θ   i   =   c  ) ~ Bernoulli(  π   cj  )\n\n\n\n\np(xj\nθ)\n\n\n\n\nθ\n0\n1\n\n\n1\n1 – π1j\nπ1j\n\n\n2\n1 – π2j\nπ2j\n\n\n\nReferencing the table for   π   j  s  in terms of   θ   = 1 or 2\n π  11   ~ Beta( 1, 1 )\npi[1,1] ~  dbeta (1,1) \npi[2,1] ~  dbeta (1,1) T(pi[1,1], )\nfor(c in 1:C){\n for(j in 2:J){\n pi[ c,j ] ~  dbeta (1,1)\n }\n}\n π  21   ~ Beta( 1, 1 )   I  (  π  21  &gt;   π  11 )\n π   cj    ~ Beta( 1, 1 ) for remaining observables\nfor ( i  in 1:n){\n theta[ i ] ~  dcat (lambda[]) \n}\n θ   i   _ ~ _  Categorical( λ )\nlambda[1:C] ~  ddirch ( alpha_lambda [])\nfor(c in 1:C){\n  alpha_lambda [c] &lt;- 1\n}\nλ   ~  Dirichlet ( 1, 1 ) \nMarkov Chain Monte Carlo\n\n\nEstimation in Bayesian Modeling\n\nOur “answer” is a posterior distribution\n\nAll parameters treated as random, not fixed\n\nContrasts with frequentist approaches to inference, estimation\n\nParameters are fixed, so estimation comes to finding the single best value\n“Best” here in terms of a criterion (ML, LS, etc.)\n\nPeak of a mountain vs. mapping the entire terrain of peaks, valleys, and plateaus (of a landscape)\n\n\n\nWhat’s In a Name?\n\nMarkov chain Monte Carlo\nConstruct a sampling algorithm to simulate  or draw from  the posterior.\nCollect many such draws, which serve to empirically approximate the posterior distribution, and can be used to empirical approximate summary statistics.\nMonte Carlo Principle:\n\nAnything we want to know about a random variable θ _ _ can be learned by sampling many times from f ( θ ), the density of θ .\n-- Jackman (2009)\n\n\nMarkov chain  Monte Carlo\nValues really generated as a sequence or chain\nt  denotes the step in the chain\nθ (0), θ (1), θ (2),…, θ ( t ),…, θ ( T )\nAlso thought of as a time indicator\nMarkov  chain _ _ Monte Carlo\nFollows the Markov property…\n\n\nThe Markov Property\n\nCurrent state depends on previous position\n\nExamples: weather, checkers, baseball counts & scoring\n\nNext state conditionally independent of past, given the present\n\nAkin to a full mediation model\n\np ( θ ( t +1) | θ ( t ), _ _ θ ( t -1), _ _ θ ( t -2) ,…) = p ( θ ( t +1) | θ ( t ))\n\n\n\nVisualizing the Chain: Trace Plot\n\n\n\nMarkov Chain Monte Carlo\n\nMarkov chains are sequences of numbers  that have the Markov property\n\nDraws in cycle t+ 1 depend on values from cycle t , but given those not on previous cycles (Markov property)\n\nUnder certain assumptions Markov chains reach stationarity\nThe collection of values converges to a distribution, referred to as a stationary distribution\n\nMemoryless: It will “forget” where it starts\nStart anywhere, will reach stationarity if regularity conditions hold\nFor Bayes, set it up so that this is the posterior distribution\n\nUpon convergence, samples from the chain approximate the stationary (posterior) distribution\n\nAssessing Convergence\n\n\nDiagnosing Convergence\n\nWith MCMC, convergence to a distribution , not a point\nML:\n\nConvergence is when we’ve reached the highest point in the likelihood,\nThe highest peak of the mountain\n\nMCMC:\n\nConvergence when we’re sampling values from the correct distribution,\nWe are mapping the entire terrain accurately\n\n\nA properly constructed Markov chain is guaranteed to converge to the stationary (posterior) distribution…eventually\nUpon convergence, it will sample over the full support of the stationary (posterior) distribution…over an ∞ number of draws\nIn a finite chain, no guarantee that the chain has converged or is sampling through the full support of the stationary (posterior) distribution\nMany ways to diagnose convergence\nWhole software packages dedicated to just assessing convergence of chains (e.g., R packages ‘coda’ and ‘boa’)\n\n\nGelman & Rubin’s (1992) \u000bPotential Scale Reduction Factor (PSRF)\nRun multiple  chains from dispersed starting points\nSuggest convergence when the chains come together\nIf they all go to the same place, it’s probably the stationary distribution\nAn analysis of variance type argument\nPSRF  or R =\nIf there is substantial between-chain variance, will be &gt;&gt; 1\nRun multiple  chains from dispersed starting points\nSuggest convergence when the chains come together\nOperationalized in terms of partitioning variability\nRun multiple chains for 2 T  iterations, discard first half\nExamine between and within chain variability\nVarious versions, modifications suggested over time\n\n\nPotential Scale Reduction Factor (PSRF)\nFor any θ , for any chain c the within-chain variance is\nFor all chains, the pooled within-chain variance is\nThe between-chain variance is\nThe estimated variance is\nThe potential scale reduction factor is\nIf close to 1 (e.g., &lt; 1.1) for all parameters, can conclude convergence\nExamine it over “time”, look for , stability of B and W\nIf close to 1 (e.g., &lt; 1.2, or &lt; 1.1) can conclude convergence\n\n\nAssessing Convergence: No Guarantees\n\nMultiple chains coming together does not guarantee they have converged\n\n\n\nmultiple chains come together does not guarantee they have converged\n\n\n\n\n\nMultiple chains coming together does not guarantee they have converged\n\n\n\n\nAssessing Convergence\n\nRecommend running multiple chains far apart and determine when they reach the same “place”\n\nPSRF criterion an approximation to this\nAkin to starting ML from different start values and seeing if they reach the same maximum\nHere, convergence to a distribution, not a point\n\nA chain hasn’t converged until all  parameters converged\n\nBrooks & Gelman multivariate PSRF\n\n\nSerial Dependence\n\n\nSerial Dependence\nSerial dependence between draws due to the dependent nature of the draws (i.e., the Markov structure)\np ( θ ( t +1) | θ ( t ), _ _ θ ( t -1), _ _ θ ( t -2) ,…) = p ( θ ( t +1) | θ ( t ))\nHowever there is a marginal  dependence across multiple lags\nCan examine the autocorrelation across different lags\n\n\nAutocorrelation\n\n\nThinning\nCan “thin” the chain by dropping certain iterations\nThin = 1  keep every iteration\nThin = 2  keep every other iteration (1, 3, 5,…)\nThin = 5  keep every 5th iteration (1, 6, 11,…)\nThin = 10  keep every 10th iteration (1, 11, 21,…)\nThin = 100  keep every 100th iteration (1, 101, 201,…)\n\nCan “thin” the chain by dropping certain iterations\nThin = 1  keep every iteration\nThin = 2  keep every other iteration (1, 3, 5,…)\nThin = 5  keep every 5th iteration (1, 6, 11,…)\nThin = 10  keep every 10th iteration (1, 11, 21,…)\nThin = 100  keep every 100th iteration (1, 101, 201,…)\nThinning does not  provide a better portrait of the posterior\n\nA loss of information\n\nMay want to keep, and account for time-series dependence\nUseful when data storage, other computations an issue\n\nI want 1000 iterations, rather have 1000 approximately independent iterations\n\nDependence within  chains, but none between  chains\n\n\n\nMixing\nWe don’t want the sampler to get “stuck” in some region of the posterior , or ignore a certain area of the posterior\nMixing refers to the chain “moving” throughout the support of the distribution in a reasonable way\nrelatively poor mixing\nrelatively good mixing\n\nMixing ≠ convergence, but better mixing usually leads to faster convergence\nMixing ≠ autocorrelation, but better mixing usually goes with lower autocorrelation (and cross-correlations between parameters)\nWith better mixing, then for a given number of MCMC iterations, get more information about the posterior\n\nIdeal scenario is independent draws from the posterior\n\nWith worse mixing, need more iterations to (a) achieve convergence and (b) achieve a desired level of precision for the summary statistics of the posterior\n\nChains may mix differently at different times\nOften indicative of an adaptive MCMC algorithm\nrelatively poor mixing\nrelatively good mixing\n\nSlow mixing can also be caused by high dependence between parameters\n\nExample: multicollinearity\n\nReparameterizing the model can improve mixing\n\nExample: centering predictors in regression\n\n\nStopping the Chain(s)\n\n\nWhen to Stop The Chain(s)\n\nDiscard the iterations prior to convergence as burn-in\nHow many more iterations to run?\n\nAs many as you want \nAs many as time provides\n\nAutocorrelaion complicates things\nSoftware may provide the “MC error”\n\nEstimate of the sampling variability of the sample mean\nSample here is the sample of iterations\nAccounts for the dependence between iterations\nGuideline is to go at least until MC error is less than 5% of the posterior standard deviation\n\nEffective sample size\n\nApproximation of how many independent samples we have\n\n\nSteps in MCMC in Practice\n\n\nSteps in MCMC (1)\n\nSetup MCMC using any of a number of algorithms\n\nProgram yourself (have fun )\nUse existing software (BUGS, JAGS)\n\nDiagnose convergence\n\nMonitor trace plots, PSRF criteria\n\nDiscard iterations prior to convergence as burn-in\n\nSoftware may indicate a minimum number of iterations needed\nA lower bound\n\n\n\n\nAdapting MCMC  Automatic Discard\nrelatively poor mixing during adaptive phase\nrelatively good mixing\nafter adaptive phase\n\n\nSteps in MCMC (2)\n\nRun the chain for a desired number of iterations\n\nUnderstanding serial dependence/autocorrelation\nUnderstanding mixing\n\nSummarize results\n\nMonte Carlo principle\nDensities\nSummary statistics\n\n\n\n\nModel Summary\n θ   i   _ ~ _  Categorical( λ )\nλ   ~  Dirichlet ( 1, 1 ) \n(  x   ij   |   θ   i   =   c  ) ~ Bernoulli(  π   cj  )\n π  11   ~ Beta( 1, 1 )\n π  21   ~ Beta( 1, 1 )   I  (  π  21  &gt;   π  11 )\n π   cj    ~ Beta( 1, 1 ) for others obs.\nACED Example\nSee ‘ACED Analysis.R’ for Running the analysis in R\nSee Following Slides for Select Results\n\n\nConvergence Assessment (1)\n\n\n\n\nPosterior Summary (1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\nSD\nNaive SE\nTime-series SE\n0.025\n0.25\n0.5\n0.75\n0.975\nMedian\n95% HPD lower\n95% HPD Upper\n\n\n\n\nlambda[1]\n0.51\n0.04\n0\n0\n0.42\n0.48\n0.51\n0.54\n0.6\n0.51\n0.43\n0.6\n\n\nlambda[2]\n0.49\n0.04\n0\n0\n0.4\n0.46\n0.49\n0.52\n0.58\n0.49\n0.4\n0.57\n\n\npi[1,1]\n0.13\n0.04\n0\n0\n0.06\n0.1\n0.13\n0.16\n0.23\n0.13\n0.05\n0.22\n\n\npi[2,1]\n0.84\n0.04\n0\n0\n0.75\n0.81\n0.84\n0.87\n0.91\n0.84\n0.75\n0.92\n\n\npi[1,2]\n0.22\n0.05\n0\n0\n0.12\n0.18\n0.22\n0.26\n0.33\n0.22\n0.12\n0.33\n\n\npi[2,2]\n0.98\n0.02\n0\n0\n0.93\n0.97\n0.99\n0.99\n1\n0.99\n0.94\n1\n\n\npi[1,3]\n0.02\n0.01\n0\n0\n0\n0.01\n0.02\n0.03\n0.06\n0.02\n0\n0.05\n\n\npi[2,3]\n0.19\n0.04\n0\n0\n0.12\n0.17\n0.19\n0.22\n0.28\n0.19\n0.12\n0.27\n\n\npi[1,4]\n0.03\n0.02\n0\n0\n0.01\n0.02\n0.03\n0.04\n0.07\n0.03\n0\n0.06\n\n\npi[2,4]\n0.23\n0.05\n0\n0\n0.15\n0.2\n0.23\n0.26\n0.33\n0.23\n0.15\n0.33\n\n\npi[1,5]\n0.15\n0.04\n0\n0\n0.08\n0.12\n0.15\n0.17\n0.22\n0.15\n0.08\n0.22\n\n\npi[2,5]\n0.64\n0.05\n0\n0\n0.53\n0.6\n0.64\n0.67\n0.74\n0.64\n0.53\n0.74\n\n\npi[1,6]\n0.17\n0.04\n0\n0\n0.1\n0.14\n0.17\n0.2\n0.25\n0.17\n0.1\n0.25\n\n\npi[2,6]\n0.82\n0.05\n0\n0\n0.72\n0.79\n0.82\n0.86\n0.92\n0.82\n0.73\n0.92\n\n\ntheta[1]\n2\n0.06\n0\n0\n2\n2\n2\n2\n2\n2\n2\n2\n\n\ntheta[2]\n1\n0.02\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\ntheta[3]\n1\n0.01\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\ntheta[4]\n1.97\n0.17\n0\n0\n1\n2\n2\n2\n2\n2\n2\n2\n\n\ntheta[5]\n1.17\n0.38\n0\n0.01\n1\n1\n1\n1\n2\n1\n1\n2\n\n\ntheta[6]\n1\n0.01\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\ntheta[7]\n1.01\n0.07\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\nSummary and Conclusion\n\n\nSummary\n\nDependence on initial values is “forgotten” after a sufficiently long run of the chain (memoryless)\nConvergence to a distribution\n\nRecommend monitoring multiple chains\nPSRF as approximation\n\nLet the chain “burn-in”\n\nDiscard draws prior to convergence\nRetain the remaining draws as draws from the posterior\n\nDependence across draws induce autocorrelations\n\nCan thin if desired\n\nDependence across draws within and between parameters can slow mixing\n\nReparameterizing may help\n\n\n\n\nWise Words of Caution\n\nBeware: MCMC sampling can be dangerous!\n-- Spiegelhalter, Thomas, Best, & Lunn (2007)\n(WinBUGS User Manual)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PeanutTutorial",
    "section": "",
    "text": "This website contains the downloads and handouts for the tutorial Bayesian Networks in Educational Assessment. using the Peanut Bayesian network toolkit.\nYou can access this tutorial in two ways:\nThe latter allows you to reproduce the calculations on your own computer using R Studio. See the Resources page for more information about the software you need to download.\nThe complete tutorial has the following parts:\nAn older version of the resources with powerpoint slides is available at https://pluto.coe.fsu.edu/BNinEA/NCMETutorial"
  },
  {
    "objectID": "index.html#powerpoint-slides",
    "href": "index.html#powerpoint-slides",
    "title": "PeanutTutorial",
    "section": "Powerpoint Slides",
    "text": "Powerpoint Slides\nThese are from the ISCA slides which are a general stat (as opposed to psychometric) audience.\nSession 1 Session 2 Session 3 Session 4"
  },
  {
    "objectID": "graphTheory.html",
    "href": "graphTheory.html",
    "title": "Graph theory details",
    "section": "",
    "text": "This is a more technical introduction to graph theory"
  },
  {
    "objectID": "graphTheory.html#simple-graphs",
    "href": "graphTheory.html#simple-graphs",
    "title": "Graph theory details",
    "section": "Simple Graphs",
    "text": "Simple Graphs\nIn a simple graph all of the edges are unordered pairs.\n\n\n\nA simple Graph\n\n\n\nSimple Graph Terms\nNodes are neighbors if connected by an edge.\nThe set of all neighbors of a node \\(A_i\\) is called the neighborhood, \\({\\rm N}(A_i|\\cal G)\\).\nLet \\({\\bf C}\\) be a set of nodes.\nIf all nodes in \\({\\bf C}\\) are neighbors, \\({\\bf C}\\) is complete.\nA maximal complete set is called a clique."
  },
  {
    "objectID": "graphTheory.html#directed-graphs",
    "href": "graphTheory.html#directed-graphs",
    "title": "Graph theory details",
    "section": "Directed Graphs",
    "text": "Directed Graphs\nA directed graph is simple graph whose edges are ordered pairs.\n\n\n\nA directed graph\n\n\n\nDirected Graph Terms\nThe nodes \\(\\{A^* | (A^*,A) \\in {\\cal E}\\}\\) are the parents of \\(A\\); \\(\\pi(A|{\\cal G})\\).\nThe nodes \\(\\{A_* | (A,A_*) \\in {\\cal E} \\}\\) are the children of \\(A\\)."
  },
  {
    "objectID": "graphTheory.html#separation",
    "href": "graphTheory.html#separation",
    "title": "Graph theory details",
    "section": "Separation",
    "text": "Separation\nLet \\(A_0, A_1, \\ldots, A_n\\) be a series of nodes such that \\(A_i\\) and \\(A_{i+1}\\) are neighbors. Such a series is called a path (simple graph) or chain (directed graph).\nIn a directed graph, to make a path \\((A_i,A_{i+1})\\) must be an edge. (i.e., must travel in edge direction).\nTwo nodes are connected if there exists a path or chain between them.\nA graph is connected if all its nodes are connected.\nA path whose first and last node are the same is a cycle.\nA Tree is an acyclic graph.\n\n\n\nA Tree contains no cycles\n\n\nAcyclic directed graphs (directed graphs which contain no directed cycles) play a special role in the construction of models. (Note that these graphs may contain directed cycles). Such graphs are often called by the euphonious misnomer DAG.\n \nA chord is a shortcut through a cycle. (Dotted edge in Figure 5).\nA simple graph that has no simple chordless cycles of length greater than three is called triangulated.\nIf a graph is not triangulated, additional edges can be filled in until it is triangulated.\n[triangulation]Filling-in edges for triangulationhsize=2.75truein vsize=2.75truein/Without the dotted edge, this graph is not triangulated. Adding the dotted edge makes the graph triangulated.\nA.3.1 Separation and D-separation\nThe idea of separation in the graph is important for reading conditional independence statements from the graph. However, we use a different definition for separation in directed and undirected graphs.\n.\n. Let \\({\\bf X\\/}\\), \\({\\bf Y\\/}\\), and \\({\\bf Z\\/}\\) be sets of nodes in a graph, \\(\\cal G\\). \\({\\bf Z\\/}\\) separates \\({\\bf X\\/}\\) and \\({\\bf Y\\/}\\), if for every \\(A_x\\) in \\({\\bf X\\/}\\) and for every \\(A_y\\) in \\({\\bf Y\\/}\\), all paths from \\(A_x\\) to \\(A_y\\) in \\(\\cal G\\) contain at least one node of \\({\\bf Z\\/}\\).\nAnother equivalent way to think about the separation is that deletion of the nodes \\({\\bf Z\\/}\\) from graph disconnects the nodes of \\({\\bf X\\/}\\) from the nodes of \\({\\bf Y\\/}\\).\n.\n. (Pearl[1988]) Let \\({\\bf X\\/}\\), \\({\\bf Y\\/}\\), and \\({\\bf Z\\/}\\) be sets of nodes in a directed graph, \\(\\cal G\\). \\({\\bf Z\\/}\\) d-separates \\({\\bf X\\/}\\) and \\({\\bf Y\\/}\\), if for every \\(A_x\\) in \\({\\bf X\\/}\\) and for every \\(A_y\\) in \\({\\bf Y\\/}\\), there is no chains from \\(A_x\\) to \\(A_y\\) in \\(\\cal G\\) along which the following conditions hold: (1) every node with converging arrows is in \\({\\bf Z\\/}\\) or has a descendent in \\({\\bf Z\\/}\\) and (2) every other node is outside \\({\\bf Z\\/}\\).\nIntuition: Assume we “know” values for variables in \\({\\bf Z\\/}\\).\n\nKnowing common (direct) ancestors separates \\(A_x\\) and \\(A_y\\).\nKnowing intermediate steps along a path separates \\(A_x\\) and \\(A_y\\).\nKnowing common descendents joins \\(A_x\\) and \\(A_y\\), even if they would have been separated if the common descendent was unknown.\n\n[d-separation]Separation and D-Separationhsize=6truein vsize=2truein/(Pearl [1988]). Here, \\(\\{E\\}\\) d-separate \\(D\\) and \\(F\\) (intermediate step in chain). \\(\\{A\\}\\) d-separates \\(B\\) and \\(C\\) (common ancestor), but \\(\\{A,F\\}\\) does not (common descendents must not be included).\nB. Independence and Conditional Independence\n. Let \\(A\\) and \\(B\\) be any two events such that \\({\\rm P\\/}(B)\\not=0\\). Define the conditional probability of \\(A\\) given \\(B\\) (written \\({\\rm P\\/}(A|B)\\)) as follows: \\[{\\rm P\\/}(A|B) = {{\\rm P\\/}(A \\cap B) \\over {\\rm P\\/}(B)} \\ .\\eqno (1)\\]\nWe write \\(A\\indep B\\).\n. Let \\(A_1, \\ldots, A_n\\) be a partition and let \\(B\\) be another event. Then \\[{\\rm P\\/}(B) = \\sum_{i=1}^n {\\rm P\\/}(B|A_i){\\rm P\\/}(A_i)\\\n.\\eqno (2)\\]\n. Let \\(A_1, \\ldots, A_n\\) be a partition and \\(B\\) be an event, such that \\({\\rm P\\/}(B) &gt;0\\) and \\({\\rm P\\/}(A_i) &gt;0\\) for all \\(i\\). Then: \\[{\\rm P\\/}(A_i|B) = {{{\\rm P\\/}(B|A_i){\\rm P\\/}(A_i)}\\over\n        {\\displaystyle \\sum_{i=1}^n {\\rm P\\/}(B|A_i){\\rm P\\/}(A_i)}}\n\\ .\\eqno (3)\\]\nB.1 Marginal Independence\n. Let \\(A\\) and \\(B\\) be two events. Then we say \\(A\\) and \\(B\\) are independent if and only if \\[{\\rm P\\/}(A \\cap B) = {\\rm P\\/}(A) \\cdot {\\rm P\\/}(B)\\ .\\eqno (4)\\]\nThis is also called Marginal Independence to distinguish it from Conditional Independence.\nIf additionally \\({\\rm P\\/}(A)&gt;0\\) and \\({\\rm P\\/}(B)&gt;0\\), then the following lemma shows how to interpret independence in terms of conditional probability. Note that Pearl [1988] takes this as the definition.\nAlternative Definition of Independence. If \\(A\\) and \\(B\\) are two events such that \\({\\rm P\\/}(A)&gt;0\\) and \\({\\rm P\\/}(B)&gt;0\\). Then the following three statements are equivalent: \\(A\\) and \\(B\\) are independent (Equation 4), \\[{\\rm P\\/}(A|B) = {\\rm P\\/}(A) = {\\rm P\\/}(A|\\overline B) \\ ,\\eqno (5a)\\] and \\[{\\rm P\\/}(B|A) = {\\rm P\\/}(B) = {\\rm P\\/}(B|\\overline A) \\ .\\eqno (5b)\\]\n. Let \\(A_1, \\ldots, A_n\\) be a set of \\(n\\) events. These events are mutually independent if \\({\\rm P\\/}(A_1 \\cap \\cdots \\cap A_n) = \\prod_{i=1}^n {\\rm P\\/}(A_i)\\) and any smaller subset of those events is mutually independent.\nNote that pairwise independence does not imply mutual independence.\nB.2 Conditional Independence\n. Let \\(A\\), \\(B\\) and \\(C\\) be three events. Then we say \\(A\\) and \\(B\\) are conditionally independent given \\(C\\) if and only if \\[{\\rm P\\/}(A \\cap B | C) = {\\rm P\\/}(A|C) \\cdot {\\rm P\\/}(B|C)\\ .\\eqno(6)\\]\nWe write \\(A\\indep B \\mathrel| C\\).\nConditional independence does not imply marginal independence nor visa versa (Simpson’s Paradox).\nB.3 Common Parameter Dependence\n[This is a small illustrative example we cut from the talk.]\nAccident Proneness (Feller [1968]). Imagine a population with two types of individuals: \\(N\\), normal, and \\(\\overline N\\), accident prone. And suppose that 5/6 of these people are normal, so that if we randomly select a person from this population the probability that the chosen person is normal is \\(P(N) = 5/6\\).\nLet \\(A_i\\) be the event that an individual has an accident in year \\(i\\). For each individual \\(A_i\\) is independent of \\(A_j\\) whenever \\(i \\ne j\\). Thus for each individual, whether or not that person has an accident follows a Bernoulli process. The accident probability, however, is different for the two classes of individuals. \\[P(A_i|N) = .01    \\qquad\\qquad    P(A_i|\\overline N) = .1\\] The chance of a randomly chosen individual having an accident in a given year follows from the Law of Total Probability. \\[\\eqalign{\nP(A_i)  & = P(A_i|N)P(N) + P(A_i|\\overline N)P(\\overline N) \\cr\n    & = {.05\\over 6} + {.1\\over 6} = {1.5\\over 6} = .025\\ .\\cr}\\] The probability that a randomly chosen individual has an accident in both the first and second year follows from the Law of Total Probability and the fact that \\(A_1\\) and \\(A_2\\) are independent for a given individual \\[\\eqalign{\nP(A_1 \\cap A_2)\n    &= P(A_1 \\cap A_2 |N)P(N) + P(A_1 \\cap A_2|\\overline N)P(\\overline N)   \\cr\n    &= P(A_1|N)P(A_2|N)P(N)   + P(A_1|\\overline N)P(A_2|\\overline N)P(\\overline N)\\cr\n    &= .01\\times .01 \\times {5\\over 6} + .1\\times .1 \\times{1\\over 6}\\cr\n    & =  {.0005\\over 6} + {.01/6} = {.0105\\over 6} = .00175 \\ .\\cr}\\] Note that: \\[P(A_2|A_1) = {P(A_1\\cap A_2)\\over P(A_2)} =  {.00175\\over .025} =\n.07\\ .\\] Therefore \\(A_1\\) and \\(A_2\\) are not (unconditionally) independent!\n[spurious]Graph for Feller’s Accident Proneness examplehsize=5truein vsize=2truein/\nThe explanation for this phenomenon lies with the interpretation of probability as a state of information. When we learn that the individual in question has had an accident during the first year, that provides information about whether or not he is accident prone which in turn provides information about what will happen during the next year. In general, whenever the parameter is unknown, information about one sample value provides information about the others through the parameter. This is the essence of common parameter dependence.\n[uirt]Unidimensional IRT as a graphical modelhsize=4truein vsize=2truein/\nA very common example from educational testing is Unidimensional IRT. Here the latent trait \\(\\theta\\) accounts for all of the dependence between the observations. This is sometimes called the “naïve Bayes” model, although in practice the assessment can be engineered to make this model fit pretty well.\nB.4 Competing Explanations\nConditioning on common descendents induces dependencies. This is the Competing Explanation phenomenon.\n[compete]Variables \\(\\theta_1\\) and \\(\\theta_2\\) are conditionally dependent given \\(X\\)hsize=4.5truein vsize=1.5truein/Note that even though \\(\\theta_1\\) and \\(\\theta_2\\) are marginally independent, if \\(X\\) is known they become dependent.\nConjunctive Skills Model. Suppose \\(\\theta_1\\) and \\(\\theta_2\\) represent two skills (, reading and writing) and \\(X\\) represents performance on a task which requires both (, document based writing task). Poor performance on the task could be a sign of lack of either of the skills. Suppose we learned (from an earlier reading test) that the reading skills of the examinee were high; we would then conclude that there was a deficiency in writing. Thus, observing the performance on the task induces a dependency in our knowledge about the skill variables.\nThis is intuition behind D-separation.\nB.5 I-Maps and D-Maps\nIdeally, the separation properties of the model graph should show all of the conditional independence relationships in the graphical model.\nI-map (Independence Map) Separation in graph implies conditional independence.\nD-map (Dependence Map) Conditional independence implies separation in graph.\nDAGs are good for making maximal D-maps. Undirected Graphs are good for making minimal I-maps.\nPearl [1988] develops these ideas.\nC. Factorization and Hypergraphs\nC.1 Products of Potentials\nKey idea: Here we are using the law of total probability to break a big probability distribution up into many small factors. Each of those small factors should be related to things going on in the graph.\n\\({\\rm P\\/}(A,B,C,D,E,F)={\\rm P\\/}(A){\\rm P\\/}(B){\\rm P\\/}(C|A,B){\\rm P\\/}(D|C){\\rm P\\/}(E|C){\\rm P\\/}(F|E,D)\\)\nThe probability function \\({\\rm P\\/}_{\\cal G}\\) is called the total probability and is defined by: \\[{\\rm P\\/}_{\\cal G} =\n\\prod_{{\\bf C\\/} \\in {\\cal C}} \\Phi_{\\bf C\\/}  \\ .\n\\eqno (7)\\]\n\\(\\Phi_{\\bf C\\/}\\) is a potential and represents either a probability or conditional probability.\nC.2 Three Graphical Representations\n[abcdefDigraph]Directed Graph representing \\({\\rm P\\/}(A){\\rm P\\/}(B){\\rm P\\/}(C|A,B){\\rm P\\/}(D|C){\\rm P\\/}(E|C){\\rm P\\/}(F|E,D)\\) hsize=6truein vsize=1.25truein/\nIn directed graph, factors correspond to nodes and their parents.\n[abcdefHypergraph]Hypergraph representing \\({\\rm P\\/}(A){\\rm P\\/}(B){\\rm P\\/}(C|A,B){\\rm P\\/}(D|C){\\rm P\\/}(E|C){\\rm P\\/}(F|E,D)\\) hsize=6truein vsize=1.25truein/\nHyperedges (edges with arbitrary number of nodes) represent factors of the joint probability distribution are drawn with square boxes.\nTentacles link hyperedges to nodes.\nNote here factors in the graphical model are represented by icons in the graph. Shenoy and Shafer [1990] call this representation “Valuation based system.”\nWe make an undirected 2-section by connecting nodes in the same hyperedge.\n[abcdefGraph]Simple graph representing \\({\\rm P\\/}(A){\\rm P\\/}(B){\\rm P\\/}(C|A,B){\\rm P\\/}(D|C){\\rm P\\/}(E|C){\\rm P\\/}(F|E,D)\\)hsize=6truein vsize=1.25truein/\nIn simple graph, potentials correspond to cliques of graph.\nThe moral graph for a DAG is the 2-section of its directed hypergraph. (I-map for this distribution.)\nC.3. Gibb–Markov Equivalence Theorem\nMoussouris [1974] discusses this problem under the name Gibbs–Markov equivalence.\n“Markov” means separation in the graph implies conditional independence. (I-map)\n“Gibbs” means can be factored according to Equation (7).\n. The model hypergraph is an I-map of its graphical model, or equivalently, a graphical model is Markov with respect to its model hypergraph.\nC.4. Causality\nBuilding directed graphical models is easier than building undirected graphs. The consistency constraint is that each variable must appear as the child in exactly one factor, and the (directed) graph must be acyclic.\nThe direction of arrows means the “direction of conditioning.” Can be in either “causal” or “diagnostic” direction.\n[causal]Directed Graph running in “causal” direction: \\({\\rm P\\/}({\\rm Skill}){\\rm P\\/}({\\rm Performance}|{\\rm Skill})\\) hsize=6truein vsize=1.0truein/\n[diagnostic]Directed Graph running in “diagnostic” direction: \\({\\rm P\\/}({\\rm Performance}){\\rm P\\/}({\\rm Skill}|{\\rm Performance})\\) hsize=6truein vsize=1.0truein/\nCan use Bayes theorem to translate between the two (called arc reversal in influence diagram literature.\nGraphs running in the “causal” direction are generally simpler.\nCausality, however, is not necessary. Weaker notions like “tendency to cause”, “influence” or “knowledge dependence” are sufficient. The word “causal” is easy to misinterpret by the lay public.\nBUT\nIf we have a causal theory, we can use it to help build efficient models.\nFor example, Cognative Theory about factors which go into performance on an assessment task can be used to build a model of that performance (Mislevy [1994]).\n“All models are false, but some models are useful.”\nD. Related Models\nD.1 Influence Diagrams\nRelated to the concept of causal models are influence diagrams (Howard and Matheson [1981], Shachter [1986], and Oliver and Smith [1990]). Influence diagrams use both probabilities and utilities which represent preferences among outcomes. Influence diagrams also use two classes of nodes, one to represent random variables and one to represent decisions (under the control of the decision maker). The “solution” to an influence diagram is a strategy for making the decisions involved in the problem to maximize the expected utility.\n[ValueOfInformation]Influence diagram for skill training decisionhsize=6truein vsize=3truein/An influence diagram which shows factors revolving around decision on whether or not to send a candidate for traiing in a particular skill.\nSquare boxes are decision variables Round boxes are chance nodes Hexagonal boxes are utilities (costs are negative utilities).\nArrows into decision nodes represent information available at time of decision.\nAn influence diagram will all chance nodes is called a relevance diagram and is a Bayesian Network.\nC.2 Structural Equation Models\n[sem]Grahical ModelStructural Equation Modelhsize=2.5in vsize=1.5in/\nMarginal Independence rather than Conditional independence\nCyclical Relationships allowed\nMixed Directed and Undirected edges\nTypically used to describe overall relationships rather than to make predictions for single individuals\nD. Graph Queries\nWhat kinds of questions might we want to ask our model?\n\nMarginal Belief “What is the probability that the learner has skill \\(S_1\\)?” \\[{\\rm P\\/}(S_1)\\]\n\nTerm “Margin” comes from contingency tables, this is the “Margin” of the table associated with \\(S_1\\).\n\nConditional Belief “What is the probability that the learner has skill \\(S_1\\) given we have made observations on tasks \\(X_1,\\ldots, X_n\\)?” \\[{\\rm P\\/}(S_1|{\\bf X})\\]\nHypothetical Belief \"Given that we have made observations on tasks \\(X_1,\\ldots, X_n\\), if we observed a performance on task \\(Y\\), how would our beliefs change?\" \\[{\\rm P\\/}(S_1|{\\bf X},Y)\\over{\\rm P\\/}(S_1|{\\bf X})\\]\n\nLet \\(p_Y = {\\rm P\\/}(Y|S_1)\\), that is the probability of seeing a good performance on task Y given that the learner has skill \\(S_1\\).\n\nSensitivity Analysis “How do our conclusions about the learner change as \\(p_Y\\) changes?” \\[{\\rm P\\/}(S_1|{\\bf X},Y) = f(p_Y)\\]\n\nSuppose further that we say \\(p_Y \\sim {\\rm Gamma\\/}(\\cdots)\\).\n\nUncertainty Analysis “What is the expected value of our beliefs about \\(S_1\\), the variance?” \\[E\\left[{\\rm P\\/}(S_1|{\\bf X},Y)\\right]  \\qquad Var\\left({\\rm P\\/}(S_1|{\\bf X},Y)\\right)\\]"
  }
]